{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Data Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "li-Dc8YVp0nC",
        "ZRctfCeYt8fo",
        "AjGE1Jvw4icb",
        "gKpB4tvFWRyD",
        "pERzQ1Ej3l2a",
        "aOzzYD-_W9zr",
        "yWJytehVs0Ni"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li-Dc8YVp0nC"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Install the necessary modules that are going to be used in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxIsG3p7pi6E"
      },
      "source": [
        "%%capture\n",
        "!pip install -U scikit-learn tensorflow-addons tf-models-official\n",
        "# !pip install --upgrade tfds-nightly\n",
        "!pip install dropbox\n",
        "!pip install nlpaug transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXPkv3OHp6_8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfpd\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "import dropbox\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import applications as apps\n",
        "from tensorflow.keras import layers, optimizers, metrics, utils\n",
        "\n",
        "from IPython.display import display\n",
        "from os import path\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from functools import reduce\n",
        "from shutil import make_archive, unpack_archive, rmtree\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpvUOuC3j27n"
      },
      "source": [
        "# try:\n",
        "#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "# except ValueError:\n",
        "#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9858QyylqFUr"
      },
      "source": [
        "# Dataset creation and visualization\n",
        "\n",
        "We use the *ag_news_subset* to demonstrate the sample efficiency improvements for text datasets. Tensorflow makes it available through *tensorflow_datasets* package. Details about the dataset are available at: [AG News Dataset](https://www.tensorflow.org/datasets/catalog/ag_news_subset)\n",
        "\n",
        "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n",
        "\n",
        "| Name     | Id       |\n",
        "| -------- | :------: |\n",
        "| World    | 0        |\n",
        "| Sports   | 1        |\n",
        "| Business | 2        |\n",
        "| Sci/Tech | 3        |\n",
        "\n",
        "In this section, we will create the training, validation and the test splits for the dataset. The dataset elements are formatted as a tuple *(text, class_label)*. We expose the following objects:\n",
        "* train_ds (Training dataset)\n",
        "* test_ds (Test dataset)\n",
        "* ds_info (Dataset metadata object)\n",
        "\n",
        "* [Colab Tutorial](https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/tensorflow/fine_tuning_bert.ipynb#scrollTo=idxyhmrCQcw5)\n",
        "* [Kaggle Tutorial](https://www.kaggle.com/au1206/fine-tuning-bert-text-classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afYO3pEW1Bwk"
      },
      "source": [
        "DATASET_SPLITS = ['train[:60%]', 'train[60%:]', 'test']\n",
        "NUM_CLASSES = 2 #ds_info.features['label'].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fgfA0i1L5T3"
      },
      "source": [
        "%%capture\n",
        "(train_ds, val_ds, test_ds), ds_info = tfds.load(\n",
        "  name='imdb_reviews', \n",
        "  split=DATASET_SPLITS,\n",
        "  as_supervised=True,\n",
        "  with_info=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "410cBfHgODOU",
        "outputId": "0d182376-55ca-4359-cd2c-0f08f96e815e"
      },
      "source": [
        "next(train_ds.as_numpy_iterator()), ds_info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
              "  0),\n",
              " tfds.core.DatasetInfo(\n",
              "     name='imdb_reviews',\n",
              "     version=1.0.0,\n",
              "     description='Large Movie Review Dataset.\n",
              " This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "     homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "     features=FeaturesDict({\n",
              "         'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "         'text': Text(shape=(), dtype=tf.string),\n",
              "     }),\n",
              "     total_num_examples=100000,\n",
              "     splits={\n",
              "         'test': 25000,\n",
              "         'train': 25000,\n",
              "         'unsupervised': 50000,\n",
              "     },\n",
              "     supervised_keys=('text', 'label'),\n",
              "     citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "       author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "       title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "       booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "       month     = {June},\n",
              "       year      = {2011},\n",
              "       address   = {Portland, Oregon, USA},\n",
              "       publisher = {Association for Computational Linguistics},\n",
              "       pages     = {142--150},\n",
              "       url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "     }\"\"\",\n",
              "     redistribution_info=,\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRctfCeYt8fo"
      },
      "source": [
        "## Apply preliminary transformations to the datasets.\n",
        "\n",
        "These are the transformations that apply to all the splits of the dataset like data cleaning or formatting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Q1XCmprVi0"
      },
      "source": [
        "# !pip install -U gensim\n",
        "# !pip install spacy[cuda111]\n",
        "# import spacy\n",
        "# spacy.prefer_gpu()\n",
        "# !nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ETx6uqAqMar"
      },
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# info = api.info()  # show info about available models/datasets\n",
        "# model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
        "# model.get_vector('hello')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMIb6fPTwxL5",
        "outputId": "e5228cea-5b70-40d4-964b-80bcde936278"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051302 sha256=40544cf782ce8508437120a5e166b55a489d94365426616d1ecf62f073a396ad\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7krwubyc/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeS8HXoCwSrO"
      },
      "source": [
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOB7JO_h57P2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yiw645k9FoN",
        "outputId": "4a907198-9257-4fde-b14a-98200fbe090a"
      },
      "source": [
        "np.zeros(shape=(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SXSWLW-yfGM"
      },
      "source": [
        "MAX_SEQ_LEN = 500\n",
        "WORD2VEC_LEN = 300\n",
        "\n",
        "def vectorizepyfn(text):\n",
        "  text = text.numpy().decode('utf-8')\n",
        "  doc = nlp(text)\n",
        "  text_vectors = np.array(list(map(lambda word: word.vector, doc))[:MAX_SEQ_LEN])\n",
        "  num_text_vectors = len(text_vectors)\n",
        "  padding = np.zeros(shape=(MAX_SEQ_LEN - num_text_vectors, WORD2VEC_LEN)) if num_text_vectors < MAX_SEQ_LEN else None\n",
        "  vector = np.concatenate((text_vectors, padding)) if padding is not None else np.array(text_vectors)\n",
        "\n",
        "  return vector\n",
        "\n",
        "def vectorizefn(text, label):\n",
        "  vector = tf.py_function(vectorizepyfn, inp=[text], Tout=tf.float32)\n",
        "  vector.set_shape((None, WORD2VEC_LEN))\n",
        "  return vector, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F-SOhiwxypx"
      },
      "source": [
        "def mapchainfn(ds, funcs=[], cache=False):\n",
        "  result_ds = reduce(lambda y,x: y.map(x), funcs, ds)\n",
        "  return result_ds.cache() if cache else result_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7cxvNdsYdbr"
      },
      "source": [
        "train_prep_ds = mapchainfn(train_ds.take(500))\n",
        "val_prep_ds = mapchainfn(val_ds)\n",
        "test_prep_ds = mapchainfn(test_ds)\n",
        "# itr = train_prep_ds.map(vectorizefn)\n",
        "# next(itr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjGE1Jvw4icb"
      },
      "source": [
        "## Dataset visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TvEiDRV4mcv",
        "outputId": "96fd3e4a-f0e5-4636-a5de-d7365ae83df9"
      },
      "source": [
        "def ds_show(ds, info=None, count=10, title='dataset'):\n",
        "  df = pd.DataFrame(ds.take(count), columns=info.supervised_keys if info else [])\n",
        "  return df\n",
        "  #return display(tfds.as_dataframe(ds.take(count), info))\n",
        "\n",
        "print(ds_show(train_ds, ds_info))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text                                label\n",
            "0  tf.Tensor(b\"This was an absolutely terrible mo...  tf.Tensor(0, shape=(), dtype=int64)\n",
            "1  tf.Tensor(b'I have been known to fall asleep d...  tf.Tensor(0, shape=(), dtype=int64)\n",
            "2  tf.Tensor(b'Mann photographs the Alberta Rocky...  tf.Tensor(0, shape=(), dtype=int64)\n",
            "3  tf.Tensor(b'This is the kind of film for a sno...  tf.Tensor(1, shape=(), dtype=int64)\n",
            "4  tf.Tensor(b'As others have mentioned, all the ...  tf.Tensor(1, shape=(), dtype=int64)\n",
            "5  tf.Tensor(b\"This is a film which should be see...  tf.Tensor(1, shape=(), dtype=int64)\n",
            "6  tf.Tensor(b'Okay, you have:<br /><br />Penelop...  tf.Tensor(0, shape=(), dtype=int64)\n",
            "7  tf.Tensor(b'The film is based on a genuine 195...  tf.Tensor(0, shape=(), dtype=int64)\n",
            "8  tf.Tensor(b'I really love the sexy action and ...  tf.Tensor(0, shape=(), dtype=int64)\n",
            "9  tf.Tensor(b'Sure, this one isn\\'t really a blo...  tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpB4tvFWRyD"
      },
      "source": [
        "## Metrics visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgN7eA8sWUZx"
      },
      "source": [
        "def plot_metric(history, metric='accuracy', val=True):\n",
        "  val_metric = 'val_' + metric\n",
        "\n",
        "  # Plot training and validation metrics\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history[val_metric]) if val else None\n",
        "\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'] if val else ['train'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0LpS0YeMJBR"
      },
      "source": [
        "# Initialize the datastore\n",
        "\n",
        "We store our run data in a dropbox store. Key Variables:\n",
        "\n",
        "* **PROJECT**: Location of the store.\n",
        "* **UPLOAD_FREQUENCY**: Checkpoint upload frequency.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgoI4_BZ6fNB"
      },
      "source": [
        "PROJECT = 'sample_efficiency_text_word2vec'\n",
        "CHKPT_DIR = Path('chkpt')\n",
        "CHKPT_TMPL = Path(CHKPT_DIR, 'epoch-{epoch:d}-val_accuracy-{val_accuracy:.4f}/weights')\n",
        "HISTORY_DIR = Path('hist')\n",
        "INITIAL_WEIGHTS = Path('initial_weights/weights')\n",
        "UPLOAD_FREQUENCY = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai0h5KYeMRUL"
      },
      "source": [
        "DBX = dropbox.Dropbox('')\n",
        "CHUNK_SIZE = 20 * 1024 * 1024\n",
        "\n",
        "def dbx_upload_large_file(local_path, remote_path, mode=None):\n",
        "  size = local_path.stat().st_size\n",
        "  with open(local_path, \"rb\") as f, tqdm(total=size, desc=\"Uploaded\") as pbar:\n",
        "    session_start = DBX.files_upload_session_start(f.read(CHUNK_SIZE))\n",
        "    pbar.update(CHUNK_SIZE)\n",
        "    \n",
        "    cursor = dropbox.files.UploadSessionCursor(session_id=session_start.session_id, offset=f.tell())\n",
        "    while f.tell() < size:\n",
        "      if (size - f.tell()) <= CHUNK_SIZE:\n",
        "        commit = dropbox.files.CommitInfo(path=remote_path, mode=mode)\n",
        "        DBX.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)\n",
        "      else:\n",
        "        DBX.files_upload_session_append_v2(f.read(CHUNK_SIZE), cursor)\n",
        "        cursor.offset = f.tell()\n",
        "      \n",
        "      pbar.update(CHUNK_SIZE)\n",
        "\n",
        "def dbx_download(local_path, remote_path):\n",
        "  DBX.files_download_to_file(local_path, remote_path)\n",
        "  return local_path\n",
        "\n",
        "def dbx_upload(local_path, remote_path, overwrite=False):\n",
        "  mode = dropbox.files.WriteMode.overwrite if overwrite else None\n",
        "  \n",
        "  if local_path.stat().st_size <= CHUNK_SIZE:\n",
        "    # Upload Small File\n",
        "    with open(local_path, \"rb\") as f: DBX.files_upload(f.read(), remote_path, mode=mode)\n",
        "  else:\n",
        "    dbx_upload_large_file(local_path, remote_path, mode)\n",
        "    \n",
        "  return remote_path\n",
        "\n",
        "def dbx_latest_filter(item):\n",
        "  return item.server_modified\n",
        "\n",
        "def dbx_latest_history(experiment):\n",
        "  hist_path = Path('/', PROJECT, experiment, HISTORY_DIR)\n",
        "  info = DBX.files_list_folder(str(hist_path))\n",
        "  latest = max(info.entries, key=dbx_latest_filter) # Latest entry\n",
        "  local_path = HISTORY_DIR / latest.name if latest else None # Local path\n",
        "  remote_path = latest.path_display if latest else None\n",
        "\n",
        "  return str(local_path), remote_path\n",
        "\n",
        "def dbx_latest_chkpt(experiment):\n",
        "  chkpt_path = path.join('/', PROJECT, experiment, CHKPT_DIR)\n",
        "  info = DBX.files_list_folder(chkpt_path)\n",
        "  latest = max(info.entries, key=dbx_latest_filter) # Latest entry\n",
        "  local_path = path.join(CHKPT_DIR, latest.name) if latest else None # Local path\n",
        "  remote_path = latest.path_display if latest else None\n",
        "\n",
        "  return local_path, remote_path\n",
        "\n",
        "def latest_local_chkpt():\n",
        "  checkpoints = Path(CHKPT_DIR).glob('**/*.index')\n",
        "  latest = max(checkpoints, key=path.getctime) if checkpoints else None\n",
        "  return latest.parent\n",
        "\n",
        "def latest_local_hist():\n",
        "  histories = HISTORY_DIR.glob('*.hist')\n",
        "  latest = max(histories, key=path.getctime) if histories else None\n",
        "  return latest\n",
        "\n",
        "def dbx_exp_download(experiment, filepath):\n",
        "  remote_path = path.join('/', PROJECT, experiment, filepath)\n",
        "  dbx_download(filepath, remote_path)\n",
        "  return filepath\n",
        "\n",
        "def dbx_exp_upload(experiment, filepath):\n",
        "  remote_path = path.join('/', PROJECT, experiment, filepath)\n",
        "  dbx_upload(filepath, remote_path)\n",
        "  return filepath\n",
        "\n",
        "def dbx_download_chkpt(experiment):\n",
        "  info = dbx_latest_chkpt(experiment)\n",
        "  return dbx_download(*info)\n",
        "\n",
        "def dbx_upload_chkpt(experiment, overwrite=False):\n",
        "  checkpoint = latest_local_chkpt()\n",
        "  zip_path = Path(make_archive(str(checkpoint), 'zip', str(checkpoint))) # Create temporary archive\n",
        "  \n",
        "  zip_name = zippath(checkpoint) # Upload file name\n",
        "  remote_path = Path('/', PROJECT, experiment, zip_name) # Construct remote path\n",
        "  dbx_upload(zip_path, str(remote_path), overwrite) # Upload file\n",
        "\n",
        "  Path(zip_path).unlink() # Clean up temporary archive\n",
        "\n",
        "  return remote_path\n",
        "\n",
        "def dbx_upload_history(experiment):\n",
        "  hist_name = latest_local_hist()\n",
        "  remote_path = Path('/', PROJECT, experiment, hist_name)\n",
        "  return dbx_upload(hist_name, str(remote_path))\n",
        "\n",
        "def dbx_download_history(experiment):\n",
        "  info = dbx_latest_history(experiment)\n",
        "  return dbx_download(*info)\n",
        "\n",
        "def zippath(p):\n",
        "  return p.parent / f'{p.name}.zip'\n",
        "\n",
        "def dbx_download_initial_weights():\n",
        "  weight_zip = zippath(INITIAL_WEIGHTS.parent)\n",
        "  remote_path = path.join('/', PROJECT, weight_zip.name)\n",
        "\n",
        "  zip_path = dbx_download(weight_zip.name, remote_path) # Download archive\n",
        "  unpack_archive(zip_path, str(INITIAL_WEIGHTS.parent)) # Unzip contents\n",
        "  Path(zip_path).unlink() # Clean up the archive\n",
        "\n",
        "def dbx_upload_initial_weights(overwrite=False):\n",
        "  weight_dir = INITIAL_WEIGHTS.parent.name\n",
        "  zip_path = Path(make_archive(weight_dir, 'zip', weight_dir))\n",
        "\n",
        "  remote_path = path.join('/', PROJECT, zip_path.name)\n",
        "  return dbx_upload(zip_path, remote_path, overwrite)\n",
        "\n",
        "Path(CHKPT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "HISTORY_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhH341wfMTfJ"
      },
      "source": [
        "def dbx_history(experiment):\n",
        "  hist_file = dbx_download_history(experiment)\n",
        "  hist = pickle.load(open(hist_file, \"rb\"))\n",
        "  \n",
        "  return hist\n",
        "\n",
        "def history_tmpl():\n",
        "  snapshot_time = int(time())\n",
        "  return 'history-{0}.hist'.format(snapshot_time)\n",
        "\n",
        "def save_history(history):\n",
        "  hist_name = history_tmpl()\n",
        "  hist_path = HISTORY_DIR / hist_name\n",
        "\n",
        "  pickle.dump(history.history, open(hist_path, \"wb\"))\n",
        "  return str(hist_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pERzQ1Ej3l2a"
      },
      "source": [
        "# Model architecture\n",
        "\n",
        "We use a double stacked bidirectional LSTM model to classify AGNews dataset. The model is exposed through a **model** variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHx2Dvm3AAxN"
      },
      "source": [
        "from dropbox.exceptions import ApiError\n",
        "\n",
        "def save_and_upload_model(m, overwrite=False):\n",
        "  m.save_weights(str(INITIAL_WEIGHTS))\n",
        "  print('Uploading initial_weights to remote...')\n",
        "  dbx_upload_initial_weights(overwrite)\n",
        "\n",
        "def configure_initial_weights(m, overwrite=False):\n",
        "  # Download Project Weights\n",
        "  if not INITIAL_WEIGHTS.parent.exists():\n",
        "    try:\n",
        "      print('Downloading remote initial weights...')\n",
        "      dbx_download_initial_weights()\n",
        "      print('Download complete!')\n",
        "    except ApiError:\n",
        "      print('Remote initial weights are unavailable')\n",
        "\n",
        "  if INITIAL_WEIGHTS.parent.exists():\n",
        "    print('Loading initial_weights...')\n",
        "    try:\n",
        "      m.load_weights(str(INITIAL_WEIGHTS))\n",
        "    except ValueError:\n",
        "      try:\n",
        "        print('Available weights are incompatible with the model. They will be updated.')\n",
        "        save_and_upload_model(m, overwrite)\n",
        "      except:\n",
        "        rmtree(INITIAL_WEIGHTS.parent)\n",
        "        raise EnvironmentError('Load failed. Please enable overwrite.')\n",
        "  else:\n",
        "    save_and_upload_model(m, overwrite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psGrQPdX4Cxy"
      },
      "source": [
        "LEARNING_RATE = 0.0002"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl_tlslfdh7T"
      },
      "source": [
        "# # HUB_LAYER_URL = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "# HUB_LAYER_URL = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
        "\n",
        "# def get_hub_layer():\n",
        "#   hl = hub.KerasLayer(HUB_LAYER_URL, input_shape=[], dtype=tf.string, trainable=True)\n",
        "#   return hl\n",
        "\n",
        "# def create_model():\n",
        "#   model = tf.keras.Sequential([\n",
        "#     get_hub_layer(),\n",
        "#     layers.Dense(16, activation='relu'),\n",
        "#     layers.Dense(1, activation='sigmoid')\n",
        "#   ])\n",
        "\n",
        "#   adam = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "#   model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#   return model\n",
        "\n",
        "# model = create_model()\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs9RW9eXnRsP",
        "outputId": "9aab2648-b755-4b63-d595-3ad21e27276b"
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(MAX_SEQ_LEN, WORD2VEC_LEN)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Bidirectional(layers.LSTM(32, return_sequences=False)),\n",
        "\t  layers.Dropout(0.5),\n",
        "    layers.Dense(20, activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "  ])\n",
        "  adam = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 500, 128)          186880    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 20)                1300      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 229,417\n",
            "Trainable params: 229,417\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TMnagkAAYz7"
      },
      "source": [
        "# text, label = next(iter(train_prep_ds))\n",
        "# model(tf.expand_dims(text, axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOzzYD-_W9zr"
      },
      "source": [
        "# Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE8M5WjRjEzz"
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "def checkpoint_uploadfn(name):\n",
        "  upload_path = dbx_upload_chkpt(name)\n",
        "  #print('Uploaded: ' + upload_path)\n",
        "\n",
        "def get_upload_callback(name, epochs=20):\n",
        "  on_epoch_end = lambda epoch, logs: checkpoint_uploadfn(name) if (epoch + 1) % epochs == 0 else None\n",
        "  on_train_end = lambda logs: checkpoint_uploadfn(name)\n",
        "  callback = LambdaCallback(on_epoch_end=on_epoch_end, on_train_end=on_train_end)\n",
        "  return callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtwbsYgEW873"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def train(experiment, model, tds, vds, epochs=100):\n",
        "  tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  \n",
        "  cb_upload = get_upload_callback(experiment, epochs=UPLOAD_FREQUENCY)\n",
        "  cb_checkpoint = ModelCheckpoint(str(CHKPT_TMPL), save_best_only=True, monitor=\"val_accuracy\", save_weights_only=True)\n",
        "  cb_earlystopping = EarlyStopping(monitor='val_accuracy', patience= 15, mode='auto', restore_best_weights=True)\n",
        "\n",
        "  callbacks = [cb_checkpoint, cb_earlystopping]\n",
        "  # callbacks = [cb_checkpoint, cb_upload, cb_earlystopping]\n",
        "  history = model.fit(tds, validation_data=vds, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAb8OdCLlXyj"
      },
      "source": [
        "## NLP Augmentations\n",
        "\n",
        "Use *randaugfn()* function with the [NLPAug](https://nlpaug.readthedocs.io/en/latest/index.html) augmenter as a parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgVLRxrJlaPh"
      },
      "source": [
        "def nlpaugfn(aug, is_batch=False):\n",
        "  def pyfn(text):\n",
        "    try:\n",
        "        utext = np.char.decode(text.numpy().astype(np.bytes_), 'UTF-8') if is_batch else text.numpy().decode(\"utf-8\")\n",
        "        aug_text = aug.augment(utext)\n",
        "    except ValueError as e:\n",
        "        print(text, utext, e)\n",
        "        raise e\n",
        "    \n",
        "    return aug_text\n",
        "\n",
        "  def augtext(text, label):\n",
        "    aug_text = tf.py_function(pyfn, inp=[text], Tout=tf.string)\n",
        "    aug_text.set_shape(text.shape)\n",
        "\n",
        "    return aug_text, label\n",
        "\n",
        "  return augtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA07oV3ZXIQM"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2G-oTjyXMAD"
      },
      "source": [
        "## baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy6hvZz-XJxm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "bf5a9df8-fd1d-4fe2-beb1-45d1f6933c6a"
      },
      "source": [
        "experiment = 'baseline'\n",
        "\n",
        "tds = train_prep_ds.map(vectorizefn).cache().shuffle(500, reshuffle_each_iteration=True).batch(16)\n",
        "# tds = train_prep_ds.cache().shuffle(10000, reshuffle_each_iteration=True).batch(512)\n",
        "vds = val_prep_ds.take(1000).map(vectorizefn).batch(512).cache()\n",
        "\n",
        "configure_initial_weights(model, True)\n",
        "# hist = train(experiment, model, tds, vds, epochs=1)\n",
        "hist = train(experiment, model, tds, vds, epochs=50)\n",
        "\n",
        "save_history(hist)\n",
        "dbx_upload_history(experiment)\n",
        "plot_metric(hist)\n",
        "\n",
        "baseline_hist = hist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading initial_weights...\n",
            "Epoch 1/50\n",
            "32/32 [==============================] - 93s 2s/step - loss: 7.6246 - accuracy: 0.5000 - val_loss: 7.9144 - val_accuracy: 0.4810\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 6s 199ms/step - loss: 7.6246 - accuracy: 0.5000 - val_loss: 7.9144 - val_accuracy: 0.4810\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 5s 149ms/step - loss: 7.6246 - accuracy: 0.5000 - val_loss: 7.9144 - val_accuracy: 0.4810\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 4s 141ms/step - loss: 7.6246 - accuracy: 0.5000 - val_loss: 7.9144 - val_accuracy: 0.4810\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 5s 142ms/step - loss: 7.6246 - accuracy: 0.5000 - val_loss: 7.9144 - val_accuracy: 0.4810\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 5s 142ms/step - loss: 7.6246 - accuracy: 0.5000 - val_loss: 7.9144 - val_accuracy: 0.4810\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfbSV1WHn8e9PQBHERAGVANNLUpqImKIcqa1NlzXFQRMvtkTRqNVOqulqXL6M7RSn7STVTEdTazK2riSoWJI4vgTj9GaMob5hmvoSDhZfAI3EhfUSlQsqoBV5+80f57n0cD1cuPicewL8Pmvddc+z97Ofs7cu+LH3fs55ZJuIiIgy7NfqDkRExN4joRIREaVJqERERGkSKhERUZqESkRElCahEhERpUmoRLSApH+Q9OVdPHeFpN95v9eJ6A8JlYiIKE1CJSIiSpNQidiBYtnpTyU9LeltSbdIOlzSfZLWS3pA0iF157dLWiLpTUkLJB1ZV3eMpCeLdncCg3u816clLS7aPirp47vZ5wslLZf0uqQOSR8qyiXpq5JWSVon6RlJE4u6UyUtLfq2UtKf7NZ/sAgSKhE7MwOYCvwKcBpwH/DfgZHU/vxcAiDpV4DbgcuKuh8A35e0v6T9gf8LfBs4FPhucV2KtscAc4DPA8OBbwIdkg7oS0clnQT8L+BMYBTwEnBHUX0y8FvFOD5QnLOmqLsF+LztYcBE4KG+vG9EvYRKRO/+zvZrtlcC/ww8YftfbW8A7gGOKc6bCdxr+37bm4DrgAOB3wCOBwYBX7O9yfY8YGHde1wEfNP2E7a32J4LvFu064tzgDm2n7T9LnAl8OuS2oBNwDDgY4BsL7P9StFuEzBB0sG237D9ZB/fN2KbhEpE716re/1Og+ODitcfojYzAMD2VuBlYHRRt9Lbf3vrS3Wvfwm4olj6elPSm8DYol1f9OzDW9RmI6NtPwT8PXAjsErSbEkHF6fOAE4FXpL0iKRf7+P7RmyTUIkox8+phQNQ28OgFgwrgVeA0UVZt/9U9/pl4H/a/mDdzxDbt7/PPgyltpy2EsD2DbYnAxOoLYP9aVG+0PZ04DBqy3R39fF9I7ZJqESU4y7gU5I+KWkQcAW1JaxHgceAzcAlkgZJ+j1gSl3bm4A/kvRrxYb6UEmfkjSsj324HfgDSZOK/Zi/prZct0LSccX1BwFvAxuArcWezzmSPlAs260Dtr6P/w6xj0uoRJTA9vPAucDfAaupbeqfZnuj7Y3A7wEXAK9T23/5Xl3bKnAhteWpN4Dlxbl97cMDwF8Cd1ObHX0EOKuoPphaeL1BbYlsDfA3Rd15wApJ64A/orY3E7FblId0RUREWTJTiYiI0iRUIiKiNAmViIgoTUIlIiJKM7DVHWilESNGuK2trdXdiIjYoyxatGi17ZGN6vbpUGlra6Narba6GxERexRJL+2oLstfERFRmoRKRESUJqESERGl2af3VBrZtGkTnZ2dbNiwodVdaarBgwczZswYBg0a1OquRMReJKHSQ2dnJ8OGDaOtrY3tv1R272GbNWvW0NnZybhx41rdnYjYizR1+UvSNEnPF483ndWg/gJJXcVjVBdL+sO6uvMlvVD8nF9XPrl4FOpySTd0f524pEMl3V+cf3/9Y177YsOGDQwfPnyvDRQASQwfPnyvn41FRP9rWqhIGkDtgUCnUHt+w9mSJjQ49U7bk4qfm4u2hwJfBH6N2leEf7EuJL5O7Rtdxxc/04ryWcCDtscDDxbHu9v33W26x9gXxhgR/a+Zy19TgOW2XwSQdAcwHVi6C23/M3C/7deLtvcD0yQtAA62/XhR/i3gdGrPDZ8OnFi0nwssAP6spLFs5+dvvsM7m7Y049L9qmv9u3zpm4+1uhsR0QITPnQwXzztqNKv28zlr9HUnmjXrbMo62mGpKclzZM0didtRxevG13z8Lpnbr8KHN6oU5IuklSVVO3q6urTgPrDurVv8p05N/W53efOnsG6tW82oUcREbuu1Rv13wdut/2upM9Tm2Gc9H4vatuSGj4oxvZsYDZApVLZrYfJfOiDB76P3vVuxdur+e63b+GLf3b5duWbN29m4MAd/+9a8MD8Pr/XxtUHcOfnJ/W5XUTEjjRzprKS2jO6u40pyraxvcb2u8XhzcDknbRdWbxudM3XJI0CKH6vKmEM/W7WrFn87Gc/Y9KkSRx33HF84hOfoL29nQkTattRp59+OpMnT+aoo45i9uzZ29q1tbWxevVqVqxYwZFHHsmFF17IUUcdxcknn8w777zTquFExD6mmTOVhcB4SeOo/cV/FvDZ+hMkjapbsmoHlhWv5wN/Xbc5fzJwpe3XJa2TdDzwBPD71B7fCtABnA9cU/z+x/c7gL/6/hKW/nzd+73Mdna2jnnNNdfw7LPPsnjxYhYsWMCnPvUpnn322W23/s6ZM4dDDz2Ud955h+OOO44ZM2YwfPjw7a7xwgsvcPvtt3PTTTdx5plncvfdd3PuueeWOo6IiEaaFiq2N0u6mFpADADm2F4i6SqgarsDuERSO7CZ2rO7Lyjavi7pamrBBHBV96Y98MfAPwAHUtugv68ovwa4S9LnqD2D+8xmja0/TZkyZbvPktxwww3cc889ALz88su88MIL7wmVcePGMWlSbVlr8uTJrFixot/6GxH7tqbuqdj+AfCDHmX/o+71lcCVO2g7B5jToLwKTGxQvgb45Pvs8naacWdEXw0dOnTb6wULFvDAAw/w2GOPMWTIEE488cSGnzU54IADtr0eMGBAlr8iot/ku79+wQwbNoz169c3rFu7di2HHHIIQ4YM4bnnnuPxxx/v595FRPSu1Xd/RQ/Dhw/nhBNOYOLEiRx44IEcfvh/3Bk9bdo0vvGNb3DkkUfy0Y9+lOOPP76FPY2IeC/Zu3VX7V6hUqm450O6li1bxpFHHtmiHvWvfWmsEVEeSYtsVxrVZfkrIiJKk1CJiIjSJFQiIqI0CZWIiChNQiUiIkqTUImIiNIkVPZwBx10UKu7EBGxTUIlIiJKk0/U/4KZNWsWY8eO5Qtf+AIAX/rSlxg4cCAPP/wwb7zxBps2beLLX/4y06dPb3FPIyLeK6HSm/tmwavPlHvNI46GU67ZYfXMmTO57LLLtoXKXXfdxfz587nkkks4+OCDWb16Nccffzzt7e15znxE/MJJqPyCOeaYY1i1ahU///nP6erq4pBDDuGII47g8ssv50c/+hH77bcfK1eu5LXXXuOII45odXcjIraTUOlNLzOKZjrjjDOYN28er776KjNnzuS2226jq6uLRYsWMWjQINra2hp+5X1ERKslVH4BzZw5kwsvvJDVq1fzyCOPcNddd3HYYYcxaNAgHn74YV566aVWdzEioqGEyi+go446ivXr1zN69GhGjRrFOeecw2mnncbRRx9NpVLhYx/7WKu7GBHRUFNDRdI04H9Te5zwzbYbridJmgHMA46zXZW0P/BNoAJsBS61vUDSMOCf65qOAb5j+zJJFwB/A6ws6v7e9s3NGFd/eOaZ/7hBYMSIETz22GMNz3vrrbf6q0sRETvVtFCRNAC4EZgKdAILJXXYXtrjvGHApcATdcUXAtg+WtJhwH2SjrO9HphU13YR8L26dnfavrgpA4qIiJ1q5ocfpwDLbb9oeyNwB9DowxVXA9cC9TvPE4CHAGyvAt6kNmvZRtKvAIex/cwlIiJaqJmhMhp4ue64syjbRtKxwFjb9/Zo+xTQLmmgpHHAZGBsj3POojYzqX905QxJT0uaJ6nn+d3veZGkqqRqV1dXw47vC0/D3BfGGBH9r2Vf0yJpP+B64IoG1XOohVAV+BrwKLClxzlnAbfXHX8faLP9ceB+YG6j97U923bFdmXkyJHvqR88eDBr1qzZq//Stc2aNWsYPHhwq7sSEXuZZm7Ur2T72cUY/mMTHWAYMBFYUHwy/AigQ1K77SpwefeJkh4Fflp3/KvAQNuLustsr6m79s3AV3an02PGjKGzs5MdzWL2FoMHD2bMmDGt7kZE7GWaGSoLgfHF8tVKajOLz3ZX2l4LjOg+lrQA+JPi7q8hgGy/LWkqsLnHBv/ZbD9LQdIo268Uh+3Ast3p9KBBgxg3btzuNI2I2Oc1LVRsb5Z0MTCf2i3Fc2wvkXQVULXd0Uvzw4D5krZSC6TzetSfCZzao+wSSe3AZuB14IIShhEREX2gvXnvYGcqlYqr1WqruxERsUeRtMh2pVFdnqcSERGlSahERERpEioREVGahEpERJQmoRIREaVJqERERGkSKhERUZqESkRElCahEhERpUmoREREaRIqERFRmoRKRESUJqESERGlSahERERpEioREVGahEpERJQmoRIREaVpaqhImibpeUnLJc3q5bwZkiypUhzvL+lWSc9IekrSiXXnLiiuubj4OawoP0DSncV7PSGprZlji4iI92raM+olDQBuBKYCncBCSR22l/Y4bxhwKfBEXfGFALaPLkLjPknH2d5a1J9ju+dzgD8HvGH7lyWdBVwLzCx9YBERsUPNnKlMAZbbftH2RuAOYHqD866mFgAb6somAA8B2F4FvAk0fB5ynenA3OL1POCTkrT73Y+IiL5qZqiMBl6uO+4syraRdCww1va9Pdo+BbRLGihpHDAZGFtXf2ux9PWXdcGx7f1sbwbWAsN7dkrSRZKqkqpdXV3vY3gREdFTyzbqJe0HXA9c0aB6DrUQqgJfAx4FthR159g+GvhE8XNeX97X9mzbFduVkSNH7m73IyKigWaGykq2n12MKcq6DQMmAgskrQCOBzokVWxvtn257Um2pwMfBH4KYHtl8Xs98H+oLbNt936SBgIfANY0aWwREdFAM0NlITBe0jhJ+wNnAR3dlbbX2h5hu812G/A40G67KmmIpKEAkqYCm20vLZbDRhTlg4BPA88Wl+wAzi9efwZ4yLabOL6IiOihaXd/2d4s6WJgPjAAmGN7iaSrgKrtjl6aHwbMl7SV2gyke4nrgKJ8UHHNB4CbirpbgG9LWg68Ti3EIiKiH2lf/sd8pVJxtdrzzuSIiOiNpEW2G96Rm0/UR0REaRIqERFRmoRKRESUJqESERGlSahERERpEioREVGahEpERJQmoRIREaVJqERERGkSKhERUZqESkRElCahEhERpUmoREREaRIqERFRmoRKRESUJqESERGlSahERERpmhoqkqZJel7SckmzejlvhiRLqhTH+0u6VdIzkp6SdGJRPkTSvZKek7RE0jV117hAUpekxcXPHzZzbBER8V5Ne0a9pAHAjcBUoBNYKKnD9tIe5w0DLgWeqCu+EMD20ZIOA+6TdFxRd53thyXtDzwo6RTb9xV1d9q+uFljioiI3jVzpjIFWG77RdsbgTuA6Q3Ouxq4FthQVzYBeAjA9irgTaBi+99tP1yUbwSeBMY0bwgREdEXzQyV0cDLdcedRdk2ko4Fxtq+t0fbp4B2SQMljQMmA2N7tP0gcBrwYF3xDElPS5onabvz69pdJKkqqdrV1bVbA4uIiMZatlEvaT/geuCKBtVzqIVQFfga8Ciwpa7tQOB24AbbLxbF3wfabH8cuB+Y2+h9bc+2XbFdGTlyZFnDiYgImrinAqxk+9nFmKKs2zBgIrBAEsARQIekdttV4PLuEyU9Cvy0ru1s4AXbX+susL2mrv5m4CsljSMiInZRM2cqC4HxksYVm+pnAR3dlbbX2h5hu812G/A40G67WtzlNRRA0lRgc/cGv6QvAx8ALqt/M0mj6g7bgWVNHFtERDTQtJmK7c2SLgbmAwOAObaXSLoKqNru6KX5YcB8SVupzW7OA5A0Bvhz4DngyWKG8/e2bwYukdQObAZeBy5ozsgiImJHZLvVfWiZSqXiarXa6m5EROxRJC2yXWlUl0/UR0REaRIqERFRmoRKRESUJqESERGlSahERERpEioREVGahEpERJQmoRIREaVJqERERGl2KVQkXSrpYNXcIulJSSc3u3MREbFn2dWZyn+xvQ44GTiE2ndxXdN7k4iI2Nfsaqio+H0q8G3bS+rKIiIigF0PlUWS/olaqMwvniu/tXndioiIPdGufvX954BJwIu2/13SocAfNK9bERGxJ9rVmcqvA8/bflPSucBfAGub162IiNgT7WqofB34d0m/Su2Z8j8DvtW0XkVExB5pV0Nls2tP85pO7UmLN1J7xnxERMQ2uxoq6yVdSe1W4nsl7QcM2lkjSdMkPS9puaRZvZw3Q5IlVYrj/SXdKukZSU9JOrHu3MlF+XJJN6h4prCkQyXdL+mF4vchuzi2iIgoya6GykzgXWqfV3kVGAP8TW8NJA0AbgROASYAZ0ua0OC8YcClwBN1xRcC2D4amAr8bRFkUFuKuxAYX/xMK8pnAQ/aHg88WBxHREQ/2qVQKYLkNuADkj4NbLC9sz2VKcBy2y/a3gjcQW35rKergWuBDXVlE4CHivdeBbwJVCSNAg62/XixHPct4PSizXRgbvF6bl15RET0k139mpYzgZ8AZwBnAk9I+sxOmo0GXq477izK6q97LDDW9r092j4FtEsaKGkcMBkYW7Tv3ME1D7f9SvH6VeDwHYzlIklVSdWurq6dDCEiIvpiVz+n8ufAccWsAUkjgQeAebv7xsVy1vXABQ2q5wBHAlXgJeBRYMuuXtu2JXkHdbOB2QCVSqXhORERsXt2NVT26w6Uwhp2PstZSW120W1MUdZtGDARWFDstR8BdEhqt10FLu8+UdKjwE+BN4rrNLrma5JG2X6lWCar729ERPSDXd2o/6Gk+ZIukHQBcC/wg520WQiMlzRO0v7AWUBHd6XttbZH2G6z3QY8DrTbrkoaImkogKSp1G5pXlosb62TdHxx19fvA/9YXLIDOL94fX5deURE9JNdmqnY/lNJM4ATiqLZtu/ZSZvNki4G5gMDgDm2l0i6Cqja7uil+WHUvmNsK7WZyHl1dX8M/ANwIHBf8QO1b02+S9LnqC2ZnbkrY4uIiPKodhPVvqlSqbharba6GxERexRJi2xXGtX1OlORtB5olDqith9+cAn9i4iIvUSvoWI7X8USERG7LM+oj4iI0iRUIiKiNAmViIgoTUIlIiJKk1CJiIjSJFQiIqI0CZWIiChNQiUiIkqTUImIiNIkVCIiojQJlYiIKE1CJSIiSpNQiYiI0iRUIiKiNAmViIgoTVNDRdI0Sc9LWi5pVi/nzZBkSZXieJCkuZKekbRM0pVF+UclLa77WSfpsqLuS5JW1tWd2syxRUTEe+3SM+p3h6QBwI3AVKATWCipw/bSHucNAy4FnqgrPgM4wPbRkoYASyXdbvt5YFLd9VcC99S1+6rt65o1poiI6F0zZypTgOW2X7S9EbgDmN7gvKuBa4ENdWUGhkoaCBwIbATW9Wj3SeBntl8qvecREbFbmhkqo4GX6447i7JtJB0LjLV9b4+284C3gVeAfwOus/16j3POAm7vUXaxpKclzZF0SKNOSbpIUlVStaurq28jioiIXrVso17SfsD1wBUNqqcAW4APAeOAKyR9uK7t/kA78N26Nl8HPkJteewV4G8bva/t2bYrtisjR44sYygREVFoZqisBMbWHY8pyroNAyYCCyStAI4HOorN+s8CP7S9yfYq4F+ASl3bU4Anbb/WXWD7NdtbbG8FbqIWTBER0Y+aGSoLgfGSxhUzi7OAju5K22ttj7DdZrsNeBxot12ltuR1EoCkodQC57m6a59Nj6UvSaPqDn8XeLb8IUVERG+adveX7c2SLgbmAwOAObaXSLoKqNru6KX5jcCtkpYAAm61/TRsC5mpwOd7tPmKpEnUNvlXNKiPiIgmk+1W96FlKpWKq9Vqq7sREbFHkbTIdqVRXT5RHxERpUmoREREaRIqERFRmoRKRESUJqESERGlSahERERpEioREVGahEpERJQmoRIREaVJqERERGkSKhERUZqESkRElCahEhERpUmoREREaRIqERFRmoRKRESUJqESERGlaWqoSJom6XlJyyXN6uW8GZIsqVIcD5I0V9IzkpZJurLu3BVF+WJJ1bryQyXdL+mF4vchzRxbRES8V9NCRdIAas+aPwWYAJwtaUKD84YBlwJP1BWfARxg+2hgMvB5SW119b9te1KPx1nOAh60PR54sDiOiIh+1MyZyhRgue0XbW8E7gCmNzjvauBaYENdmYGhkgYCBwIbgXU7eb/pwNzi9Vzg9PfR94iI2A3NDJXRwMt1x51F2TaSjgXG2r63R9t5wNvAK8C/AdfZfr2oM/BPkhZJuqiuzeG2Xylevwoc3qhTki6SVJVU7erq2p1xRUTEDrRso17SfsD1wBUNqqcAW4APAeOAKyR9uKj7TdvHUltW+4Kk3+rZ2Laphc972J5tu2K7MnLkyBJGEhER3ZoZKiuBsXXHY4qybsOAicACSSuA44GOYrP+s8APbW+yvQr4F6ACYHtl8XsVcA+1AAJ4TdIogOL3qiaNKyIidqCZobIQGC9pnKT9gbOAju5K22ttj7DdZrsNeBxot12ltuR1EoCkodQC5zlJQ4uN/e7yk4Fni0t2AOcXr88H/rGJY4uIiAaaFiq2NwMXA/OBZcBdtpdIukpS+06a3wgcJGkJtXC61fbT1PZJfizpKeAnwL22f1i0uQaYKukF4HeK44iI6EeqbT/smyqViqvV6s5PjIiIbSQt6vGRjm3yifqIiChNQiUiIkqTUImIiNIkVCIiojQJlYiIKE1CJSIiSpNQiYiI0iRUIiKiNAmViIgoTUIlIiJKk1CJiIjSJFQiIqI0CZWIiChNQiUiIkqTUImIiNIkVCIiojQJlYiIKE1TQ0XSNEnPS1ouaVYv582QZEmV4niQpLmSnpG0TNKVRflYSQ9LWippiaRL667xJUkrJS0ufk5t5tgiIuK9BjbrwpIGUHvW/FSgE1goqcP20h7nDQMuBZ6oKz4DOMD20ZKGAEsl3Q68C1xh+8mi3SJJ99dd86u2r2vWmCIionfNnKlMAZbbftH2RuAOYHqD864GrgU21JUZGCppIHAgsBFYZ/sV208C2F4PLANGN3EMERHRB80MldHAy3XHnfQIAEnHAmNt39uj7TzgbeAV4N+A62y/3qNtG3AM289wLpb0tKQ5kg5p1ClJF0mqSqp2dXX1fVQREbFDLduol7QfcD1wRYPqKcAW4EPAOOAKSR+ua3sQcDdwme11RfHXgY8Ak6iF0d82el/bs21XbFdGjhxZ1nAiIoLmhspKYGzd8ZiirNswYCKwQNIK4Higo9is/yzwQ9ubbK8C/gXYtolPLVBus/297ovZfs32FttbgZuoBVNERPSjZobKQmC8pHGS9gfOAjq6K22vtT3CdpvtNuBxoN12ldqS10kAkoZSC5znJAm4BVhm+/r6N5M0qu7wd4Fnmze0iIhopGmhYnszcDEwn9qG+l22l0i6SlL7TprfCBwkaQm1cLrV9tPACcB5wEkNbh3+SnEL8tPAbwOXN2NcERGxY7Ld6j60TKVScbVabXU3IiL2KJIW2a40qssn6iMiojQJlYiIKE1CJSIiSpNQiYiI0iRUIiKiNAmViIgoTUIlIiJKk1CJiIjSJFQiIqI0TXtI117tvlnw6jOt7kVExO474mg45ZrSL5uZSkRElCYzld3RhHSPiNgbZKYSERGlSahERERpEioREVGahEpERJQmoRIREaVJqERERGkSKhERUZqESkRElEa2W92HlpHUBby0m81HAKtL7M6eIGPeN2TM+4b3M+Zfsj2yUcU+HSrvh6Sq7Uqr+9GfMuZ9Q8a8b2jWmLP8FRERpUmoREREaRIqu292qzvQAhnzviFj3jc0ZczZU4mIiNJkphIREaVJqERERGkSKrtB0jRJz0taLmlWq/vTbJLmSFol6dlW96W/SBor6WFJSyUtkXRpq/vUbJIGS/qJpKeKMf9Vq/vUHyQNkPSvkv5fq/vSHyStkPSMpMWSqqVfP3sqfSNpAPBTYCrQCSwEzra9tKUdayJJvwW8BXzL9sRW96c/SBoFjLL9pKRhwCLg9L38/7OAobbfkjQI+DFwqe3HW9y1ppL0X4EKcLDtT7e6P80maQVQsd2UD3tmptJ3U4Dltl+0vRG4A5je4j41le0fAa+3uh/9yfYrtp8sXq8HlgGjW9ur5nLNW8XhoOJnr/5Xp6QxwKeAm1vdl71FQqXvRgMv1x13spf/ZbOvk9QGHAM80dqeNF+xFLQYWAXcb3tvH/PXgP8GbG11R/qRgX+StEjSRWVfPKES0QtJBwF3A5fZXtfq/jSb7S22JwFjgCmS9trlTkmfBlbZXtTqvvSz37R9LHAK8IViebs0CZW+WwmMrTseU5TFXqbYV7gbuM3291rdn/5k+03gYWBaq/vSRCcA7cUewx3ASZK+09ouNZ/tlcXvVcA91Jb0S5NQ6buFwHhJ4yTtD5wFdLS4T1GyYtP6FmCZ7etb3Z/+IGmkpA8Wrw+kdjPKc63tVfPYvtL2GNtt1P4cP2T73BZ3q6kkDS1uPEHSUOBkoNS7OhMqfWR7M3AxMJ/a5u1dtpe0tlfNJel24DHgo5I6JX2u1X3qBycA51H71+vi4ufUVneqyUYBD0t6mto/nu63vU/cZrsPORz4saSngJ8A99r+YZlvkFuKIyKiNJmpREREaRIqERFRmoRKRESUJqESERGlSahERERpEioReyhJJ+4r36wbe46ESkRElCahEtFkks4tnlOyWNI3iy9tfEvSV4vnljwoaWRx7iRJj0t6WtI9kg4pyn9Z0gPFs06elPSR4vIHSZon6TlJtxXfBBDRMgmViCaSdCQwEzih+KLGLcA5wFCgavso4BHgi0WTbwF/ZvvjwDN15bcBN9r+VeA3gFeK8mOAy4AJwIepfRNARMsMbHUHIvZynwQmAwuLScSB1L5WfitwZ3HOd4DvSfoA8EHbjxTlc4HvFt/VNNr2PQC2NwAU1/uJ7c7ieDHQRu3hWhEtkVCJaC4Bc21fuV2h9Jc9ztvd70t6t+71FvJnOlosy18RzfUg8BlJhwFIOlTSL1H7s/eZ4pzPAj+2vRZ4Q9InivLzgEeKJ092Sjq9uMYBkob06ygidlH+VRPRRLaXSvoLak/a2w/YBMs07bMAAABnSURBVHwBeJvaQ7D+gtpy2MyiyfnAN4rQeBH4g6L8POCbkq4qrnFGPw4jYpflW4ojWkDSW7YPanU/IsqW5a+IiChNZioREVGazFQiIqI0CZWIiChNQiUiIkqTUImIiNIkVCIiojT/H889FfMqQH+BAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWJytehVs0Ni"
      },
      "source": [
        "## nlpaugs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLxxdjy4s2ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b2c1fc-85ad-44ea-d80a-df17e73408ff"
      },
      "source": [
        "from nltk import download as nltk_download\n",
        "from nlpaug.augmenter import sentence as nas\n",
        "from nlpaug.augmenter import word as naw\n",
        "from nlpaug.augmenter import char as nac\n",
        "from nlpaug import flow as naf\n",
        "\n",
        "[nltk_download(item) for item in ['punkt', 'averaged_perceptron_tagger', 'wordnet']]\n",
        "\n",
        "aug_args = dict(aug_p=0.3, aug_max=40)\n",
        "\n",
        "chain = [\n",
        "    nas.random.RandomSentAug(**aug_args),\n",
        "    naw.RandomWordAug(action='delete', **aug_args),\n",
        "    naw.RandomWordAug(action='swap', **aug_args),\n",
        "    naw.RandomWordAug(action='crop', **aug_args),\n",
        "    naw.RandomWordAug(action='substitute', **aug_args),\n",
        "    nac.KeyboardAug()\n",
        "]\n",
        "flow = naf.Sometimes(chain, pipeline_p=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfuQHnBesm4l"
      },
      "source": [
        "def extract_partial(text, size):\n",
        "    start = np.random.randint(0, len(text)-size-1)\n",
        "    end = start + size\n",
        "    return text[start:end]\n",
        "\n",
        "def get_partial(text, size=500):\n",
        "    text = text.numpy().decode(\"utf-8\")\n",
        "    text = extract_partial(text, size) if len(text) > (size + 10) else text\n",
        "    return text\n",
        "\n",
        "def handlepyfn(fn, fntype='first'):\n",
        "    def callfirst(text, label):\n",
        "        result_text = tf.py_function(fn, inp=[text], Tout=tf.string)\n",
        "        result_text.set_shape(text.shape)\n",
        "        return result_text, label\n",
        "    \n",
        "    switches = {\n",
        "        'first': callfirst,\n",
        "#         'second': callsecond,\n",
        "#         'all': callall\n",
        "    }\n",
        "    \n",
        "    return switches[fntype]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Jkrxlht2QW"
      },
      "source": [
        "# experiment = 'nlpaugs'\n",
        "\n",
        "# # batch_size = 64 * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "# tds = train_prep_ds.cache().shuffle(10000, reshuffle_each_iteration=True).map(randaugfn(senaug)).map(randaugfn(synaug)).batch(512)\n",
        "# vds = val_prep_ds.batch(512).cache()\n",
        "\n",
        "# configure_initial_weights(model)\n",
        "# history = train(experiment, model, tds, vds, epochs=30)\n",
        "\n",
        "# hfile = save_history(history)\n",
        "# dbx_upload_history(experiment)\n",
        "# plot_metric(history)\n",
        "\n",
        "# nlpaugs = {'history': history, 'hfile': hfile}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ7xPb3nGE4e",
        "outputId": "9f0f6b53-e2d7-49e8-db36-6e94e0056cb8"
      },
      "source": [
        "experiment = 'nlpaugs'\n",
        "\n",
        "tds = train_prep_ds.shuffle(500, reshuffle_each_iteration=True).map(handlepyfn(get_partial)).map(nlpaugfn(flow)).map(vectorizefn).batch(16)\n",
        "vds = test_prep_ds.take(1000).map(vectorizefn).batch(512).cache()\n",
        "\n",
        "# tds = train_prep_ds.take(10).cache().shuffle(10000, reshuffle_each_iteration=True).map(randaugfn(senaug)).map(randaugfn(synaug)).batch(512)\n",
        "# vds = val_prep_ds.take(10).batch(512).cache()\n",
        "\n",
        "configure_initial_weights(model, True)\n",
        "history = train(experiment, model, tds, vds, epochs=50)\n",
        "# history = train(experiment, model, tds, vds, epochs=1)\n",
        "\n",
        "hfile = save_history(history)\n",
        "dbx_upload_history(experiment)\n",
        "plot_metric(history)\n",
        "\n",
        "nlpaugs = {'history': history, 'hfile': hfile}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading initial_weights...\n",
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<keras.layers.core.Dense object at 0x7fdcf1570d90> and <keras.layers.core.Dropout object at 0x7fdcf2db8490>).\n",
            "Available weights are incompatible with the model. They will be updated.\n",
            "Uploading initial_weights to remote...\n",
            "Epoch 1/50\n",
            "32/32 [==============================] - 79s 2s/step - loss: 0.7057 - accuracy: 0.4820 - val_loss: 0.6914 - val_accuracy: 0.5010\n",
            "Epoch 2/50\n",
            "32/32 [==============================] - 22s 677ms/step - loss: 0.6905 - accuracy: 0.5300 - val_loss: 0.6852 - val_accuracy: 0.5710\n",
            "Epoch 3/50\n",
            "32/32 [==============================] - 24s 754ms/step - loss: 0.6957 - accuracy: 0.5260 - val_loss: 0.6832 - val_accuracy: 0.5740\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - 24s 748ms/step - loss: 0.6852 - accuracy: 0.5660 - val_loss: 0.6736 - val_accuracy: 0.6080\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - 24s 744ms/step - loss: 0.6682 - accuracy: 0.5820 - val_loss: 0.6882 - val_accuracy: 0.5860\n",
            "Epoch 6/50\n",
            "32/32 [==============================] - 24s 737ms/step - loss: 0.6827 - accuracy: 0.5780 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
            "Epoch 7/50\n",
            "32/32 [==============================] - 21s 660ms/step - loss: 0.7016 - accuracy: 0.5000 - val_loss: 0.6927 - val_accuracy: 0.5320\n",
            "Epoch 8/50\n",
            "32/32 [==============================] - 25s 764ms/step - loss: 0.7023 - accuracy: 0.4680 - val_loss: 0.6917 - val_accuracy: 0.4970\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - 23s 727ms/step - loss: 0.6933 - accuracy: 0.5280 - val_loss: 0.6928 - val_accuracy: 0.5050\n",
            "Epoch 10/50\n",
            "32/32 [==============================] - 21s 661ms/step - loss: 0.6948 - accuracy: 0.4680 - val_loss: 0.6918 - val_accuracy: 0.5460\n",
            "Epoch 11/50\n",
            "32/32 [==============================] - 23s 709ms/step - loss: 0.6970 - accuracy: 0.4780 - val_loss: 0.6915 - val_accuracy: 0.5480\n",
            "Epoch 12/50\n",
            "32/32 [==============================] - 22s 670ms/step - loss: 0.6923 - accuracy: 0.4980 - val_loss: 0.6910 - val_accuracy: 0.5160\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - 25s 772ms/step - loss: 0.6926 - accuracy: 0.5120 - val_loss: 0.6911 - val_accuracy: 0.5060\n",
            "Epoch 14/50\n",
            "32/32 [==============================] - 22s 684ms/step - loss: 0.6880 - accuracy: 0.5560 - val_loss: 0.6916 - val_accuracy: 0.5290\n",
            "Epoch 15/50\n",
            " 9/32 [=======>......................] - ETA: 15s - loss: 0.6845 - accuracy: 0.5694"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAxlor8C8Ki5",
        "outputId": "4766e323-a9c0-443e-e54e-029ff19a7416"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "texts = np.array(['his was an absolutely terrible', 'I have been known to fall asleep', 'As others have mentioned']).astype(np.str_)\n",
        "print(texts)\n",
        "\n",
        "seq_len = 500\n",
        "vec_len = 300\n",
        "batch_size = texts.shape[0]\n",
        "vector = np.zeros(shape=(batch_size, seq_len, vec_len))\n",
        "\n",
        "for tidx, doc in enumerate(nlp.pipe(texts.tolist())):\n",
        "  for didx, token in enumerate(doc):\n",
        "    vector[tidx][didx] = token.vector\n",
        "\n",
        "print(vector)\n",
        "# wordvecs = [[1, 2, 4, 4, 8, 1]]\n",
        "\n",
        "# for idx in range(min(seq_len, len(wordvecs))):\n",
        "#     vector[idx] = wordvecs[idx]\n",
        "\n",
        "# print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['his was an absolutely terrible' 'I have been known to fall asleep'\n",
            " 'As others have mentioned']\n",
            "[[[ 1.90970004e-02  2.31189996e-01 -1.68099999e-01 ...  5.23289979e-01\n",
            "    1.47510007e-01 -1.68540001e-01]\n",
            "  [-4.40579988e-02  3.66109997e-01  1.80319995e-01 ...  1.86250001e-01\n",
            "   -9.78169963e-02 -6.71040034e-05]\n",
            "  [-1.16619999e-02  1.94830000e-01  8.88540000e-02 ... -5.46660013e-02\n",
            "   -1.93399996e-01  1.39950007e-01]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 1.87329993e-01  4.05950010e-01 -5.11740029e-01 ...  1.64949998e-01\n",
            "    1.87570006e-01  5.38739979e-01]\n",
            "  [ 3.56700011e-02  1.85599998e-01 -3.05519998e-01 ... -2.21410006e-01\n",
            "    3.82809997e-01 -3.37430015e-02]\n",
            "  [ 4.12199982e-02  1.26959994e-01 -1.44409999e-01 ... -1.50330007e-01\n",
            "   -6.64590001e-02 -1.27169997e-01]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[-1.06480002e-01 -1.62950009e-02 -2.27550000e-01 ... -3.13430011e-01\n",
            "    8.74240026e-02 -1.66099995e-01]\n",
            "  [-2.75180012e-01  8.64050016e-02 -2.83479989e-01 ... -1.24909997e-01\n",
            "    1.19790003e-01  2.65540004e-01]\n",
            "  [ 3.56700011e-02  1.85599998e-01 -3.05519998e-01 ... -2.21410006e-01\n",
            "    3.82809997e-01 -3.37430015e-02]\n",
            "  ...\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00]]]\n"
          ]
        }
      ]
    }
  ]
}