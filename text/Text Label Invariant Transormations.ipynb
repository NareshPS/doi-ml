{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Label Invariant Transormations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76BAR6-gX51V"
      },
      "source": [
        "# Wordnet Synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfHr-O6q_rtQ",
        "outputId": "d913469b-7515-4487-bf29-c95fd20e0393"
      },
      "source": [
        "from random import choice\n",
        "\n",
        "# NLTK Import\n",
        "try:\n",
        "  from nltk.corpus import wordnet\n",
        "  # Placeholder search to ensure wordnet data is available.\n",
        "  wordnet.synsets('hello')\n",
        "except LookupError as e:\n",
        "  import nltk\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "\"\"\"\n",
        "  It returns a list of synonyms of the input word.\n",
        "  The output list may contain the original word.\n",
        "\"\"\"\n",
        "def synonyms(word):\n",
        "  results = set()\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lemma in syn.lemmas():\n",
        "      results.add(lemma.name())\n",
        "\n",
        "  return list(results)\n",
        "\n",
        "\"\"\"\n",
        "  It handles the cases when the synonyms for a word are unavailable.\n",
        "  It returns the original word in such cases.\n",
        "\"\"\"\n",
        "def synonym_or_self(word):\n",
        "  return choice(synonyms(word) or [word])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8NA8pMT0ztr"
      },
      "source": [
        "# Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3SiFI8O_zp"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "original = 'We enjoyed our short vacation in Mexico'\n",
        "words = text_to_word_sequence(original) # Tokenize the sentence to a list of words."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UVogxHZ07KW"
      },
      "source": [
        "## Synonym Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYCb-6Tnkx_U",
        "outputId": "d7f27804-7da6-4a66-fc69-a57d38645309"
      },
      "source": [
        "candidates = ['vacation'] # These are the words that can be replaced with their synonyms\n",
        "\n",
        "def syn_transformation(words, candidates):\n",
        "  transformed_words = []\n",
        "  for word in words:\n",
        "    if word in candidates:\n",
        "      transformed_words.append(synonym_or_self(word))\n",
        "    else:\n",
        "      transformed_words.append(word)\n",
        "\n",
        "  return transformed_words\n",
        "\n",
        "syn_transformation(words, candidates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'enjoyed', 'our', 'short', 'holiday', 'in', 'mexico']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kOahAK81Dr4"
      },
      "source": [
        "## Random Insertion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex3AJS6W1B3a",
        "outputId": "c8c038e7-6df8-4ff0-f5fb-3ff5cdda9c87"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "candidates = ['vacation'] # These are the words whose synonyms will be inserted.\n",
        "\n",
        "\"\"\"\n",
        "  It inserts a synonym for every candidate word at a random position in the word sequence.\n",
        "\"\"\"\n",
        "def ins_transformation(words, candidates):\n",
        "  for candidate in candidates:\n",
        "    pos = randint(0, len(words) - 1) # Random insertion position for the candidate\n",
        "    syn_word = synonym_or_self(candidate) # Get a random synonym\n",
        "    words.insert(pos, syn_word)\n",
        "\n",
        "  return words\n",
        "\n",
        "ins_transformation(words.copy(), candidates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'enjoyed', 'our', 'short', 'vacation', 'in', 'vacation', 'mexico']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awScaooWJ2ua"
      },
      "source": [
        "## Random Deletion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQDulPt6J1E0",
        "outputId": "0a3e18f1-3cc4-43e5-fac7-e9bf425d6a54"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "\"\"\"\n",
        "  It inserts a synonym for every candidate word at a random position in the word sequence.\n",
        "\"\"\"\n",
        "def del_transformation(words):\n",
        "  pos = randint(0, len(words) - 1) # Random deletion position\n",
        "  words.pop(pos)\n",
        "\n",
        "  return words\n",
        "\n",
        "del_transformation(words.copy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'enjoyed', 'our', 'short', 'in', 'mexico']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3t8gD0mN1au"
      },
      "source": [
        "## Random Swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSwkCrqFNzWp",
        "outputId": "1e59f06a-8d5a-4f9f-bf7b-34983c883076"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "\"\"\"\n",
        "  It inserts a synonym for every candidate word at a random position in the word sequence.\n",
        "\"\"\"\n",
        "def swap_transformation(words):\n",
        "  random_pos = lambda: randint(0, len(words) - 1)\n",
        "  pos1 = random_pos() # First random position\n",
        "  pos2 = random_pos() # Second random position\n",
        "\n",
        "  temp = words[pos1]\n",
        "  words[pos1] = words[pos2]\n",
        "  words[pos2] = temp\n",
        "\n",
        "  return words\n",
        "\n",
        "swap_transformation(words.copy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'enjoyed', 'our', 'short', 'vacation', 'in', 'mexico']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'enjoyed', 'our', 'short', 'mexico', 'in', 'vacation']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xch5v9tjsqSK"
      },
      "source": [
        "## Random Shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIz0hmb9smK9",
        "outputId": "4459cfa7-cb8f-4963-fbee-172cdc08a24d"
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "# NLTK Import\n",
        "try:\n",
        "  from nltk.tokenize import sent_tokenize\n",
        "  # Placeholder search to ensure wordnet data is available.\n",
        "  sent_tokenize('hello')\n",
        "except LookupError as e:\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "\n",
        "# Input paragraph\n",
        "paragraph = \"In some ways, \\\"telegram style\\\" was the precursor to the modern language abbreviations employed in \\\"texting\\\" or the use of short message standard (SMS) services such as Twitter. For telegrams, space was at a premium—economically speaking—and abbreviations were used as necessity. This motivation was revived for compressing information into the 160-character limit of a costly SMS before the advent of multi-message capabilities. Length constraints, and the initial handicap of having to enter each individual letter using multiple keypresses on a numeric pad, drove readoption of telegraphic style, and continued space limits and high per-message cost meant the practice persisted for some time after the introduction of built-in predictive text assistance despite it then needing more effort to write (and read).\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = list(sent_tokenize(paragraph))\n",
        "\n",
        "# Sentence shuffe\n",
        "shuffle(sentences)\n",
        "\n",
        "# Paragraph recomposition\n",
        "shuffled_paragraph = ' '.join(sentences)\n",
        "print(shuffled_paragraph)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This motivation was revived for compressing information into the 160-character limit of a costly SMS before the advent of multi-message capabilities. For telegrams, space was at a premium—economically speaking—and abbreviations were used as necessity. In some ways, \"telegram style\" was the precursor to the modern language abbreviations employed in \"texting\" or the use of short message standard (SMS) services such as Twitter. Length constraints, and the initial handicap of having to enter each individual letter using multiple keypresses on a numeric pad, drove readoption of telegraphic style, and continued space limits and high per-message cost meant the practice persisted for some time after the introduction of built-in predictive text assistance despite it then needing more effort to write (and read).\n"
          ]
        }
      ]
    }
  ]
}