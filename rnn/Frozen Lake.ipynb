{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install gym gym[box2d]\n",
    "! pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple neural network implementation of qlearning\n",
    "import gym\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# RL using OpenAI Gym: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "# Deep Q-Learning: https://github.com/adventuresinML/adventures-in-ml-code/blob/master/r_learning_tensorflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x165852520>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\"\"\"\n",
    "Observation Space has 8 states\n",
    "===============================\n",
    "\n",
    "1- The coordinates of the lander in x & y.\n",
    "2- Its linear velocities in x & y.\n",
    "3- Its angle.\n",
    "4- Its angular velocity.\n",
    "5- Two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "Action Space has 4 discrete actions\n",
    "===================================\n",
    "1- Do nothing.\n",
    "2- Fire left orientation engine.\n",
    "3- Fire main engine.\n",
    "4- Fire right orientation engine.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation, info = env.reset(seed=42, return_info=True)\n",
    "observation, info\n",
    "\n",
    "image = env.render(mode='rgb_array')\n",
    "plt.imshow(image)\n",
    "# for _ in range(1000):\n",
    "#    env.render()\n",
    "#    action = policy(observation)  # User-defined policy function\n",
    "#    observation, reward, done, info = env.step(action)\n",
    "\n",
    "#    if done:\n",
    "#       observation, info = env.reset(return_info=True)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n",
      "The action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: The new observation is [-0.01112251  1.4179975  -0.5687932   0.14427991  0.01476502  0.16727251\n",
      "  0.          0.        ] False -1.4359782430063308\n",
      "Step 1: The new observation is [-0.0168169   1.420644   -0.5802097   0.11748739  0.02540762  0.21287195\n",
      "  0.          0.        ] False -1.8823423231084166\n",
      "Step 2: The new observation is [-0.02251177  1.4226922  -0.580242    0.09080859  0.03604814  0.21283007\n",
      "  0.          0.        ] False -0.808668612310953\n",
      "Step 3: The new observation is [-0.02820702  1.4241415  -0.58027333  0.0641324   0.04668745  0.21280567\n",
      "  0.          0.        ] False -0.86916850316004\n",
      "Step 4: The new observation is [-0.03390265  1.4249926  -0.58030427  0.03745612  0.05732501  0.21277097\n",
      "  0.          0.        ] False -0.931777177619864\n",
      "Step 5: The new observation is [-0.03971519  1.4264959  -0.5916588   0.06638227  0.06763296  0.20617802\n",
      "  0.          0.        ] False -2.8820015174821547\n",
      "Step 6: The new observation is [-0.04543791  1.4274173  -0.580368    0.04056494  0.07565304  0.16041599\n",
      "  0.          0.        ] False 0.41754286262087814\n",
      "Step 7: The new observation is [-0.05109062  1.4277447  -0.57156795  0.01422073  0.08189983  0.12494739\n",
      "  0.          0.        ] False 0.29740233035212893\n",
      "Step 8: The new observation is [-0.05694237  1.4286923  -0.5908352   0.04179658  0.08752028  0.11241923\n",
      "  0.          0.        ] False -3.0355426385644817\n",
      "Step 9: The new observation is [-0.0627346   1.4290404  -0.5833858   0.01522774  0.09164587  0.0825193\n",
      "  0.          0.        ] False 0.37114301836084107\n",
      "Step 10: The new observation is [-0.06844263  1.4288076  -0.57278234 -0.01047708  0.0936188   0.0394618\n",
      "  0.          0.        ] False 0.8404362985167506\n",
      "Step 11: The new observation is [-0.07415056  1.4279746  -0.57278717 -0.03714621  0.095592    0.03946779\n",
      "  0.          0.        ] False -0.25380419789621556\n",
      "Step 12: The new observation is [-0.07993431  1.4265298  -0.58229643 -0.06446752  0.09948423  0.07785162\n",
      "  0.          0.        ] False -1.4925120974058348\n",
      "Step 13: The new observation is [-0.08565426  1.4244859  -0.57430065 -0.09099434  0.10177284  0.04577615\n",
      "  0.          0.        ] False 0.3509755655564686\n",
      "Step 14: The new observation is [-0.09145431  1.421834   -0.5843249  -0.11816388  0.10607878  0.08612676\n",
      "  0.          0.        ] False -1.7007603680432044\n",
      "Step 15: The new observation is [-0.09725437  1.4185822  -0.5843383  -0.14483495  0.11038289  0.08609018\n",
      "  0.          0.        ] False -0.731133306035531\n",
      "Step 16: The new observation is [-0.1031312   1.4147214  -0.5939553  -0.17207003  0.11662343  0.12482174\n",
      "  0.          0.        ] False -1.9461347650924654\n",
      "Step 17: The new observation is [-0.10909452  1.4102474  -0.60478985 -0.19952422  0.12504943  0.16853498\n",
      "  0.          0.        ] False -2.318588974016676\n",
      "Step 18: The new observation is [-0.11505804  1.4051744  -0.60481346 -0.22620049  0.1334736   0.16849917\n",
      "  0.          0.        ] False -1.2717321192906752\n",
      "Step 19: The new observation is [-0.12110329  1.3994789  -0.6150511  -0.25410125  0.14398722  0.21029118\n",
      "  0.          0.        ] False -2.5390183757945395\n",
      "Step 20: The new observation is [-0.12723836  1.3931745  -0.6262816  -0.28148037  0.15675732  0.25542504\n",
      "  0.          0.        ] False -2.848953100189503\n",
      "Step 21: The new observation is [-0.13351384  1.3868318  -0.6399116  -0.2832535   0.16915384  0.2479527\n",
      "  0.          0.        ] False -2.2837750058277946\n",
      "Step 22: The new observation is [-0.13982506  1.3807261  -0.6437362  -0.27285036  0.1818228   0.2534015\n",
      "  0.          0.        ] False -0.9587106035487409\n",
      "Step 23: The new observation is [-0.1462036   1.3740054  -0.6521618  -0.3005074   0.19620928  0.28775498\n",
      "  0.          0.        ] False -2.755363967078749\n",
      "Step 24: The new observation is [-0.15277214  1.3678539  -0.670974   -0.27533096  0.21043599  0.28453445\n",
      "  0.          0.        ] False -1.9024593466854924\n",
      "Step 25: The new observation is [-0.15965176  1.3624146  -0.7014721  -0.24373583  0.22410011  0.27328265\n",
      "  0.          0.        ] False -2.9384873165157215\n",
      "Step 26: The new observation is [-0.16646862  1.3563869  -0.6935979  -0.26974836  0.23617157  0.24142928\n",
      "  0.          0.        ] False -0.8793969052201749\n",
      "Step 27: The new observation is [-0.17321149  1.3497934  -0.684204   -0.29467422  0.24625961  0.2017607\n",
      "  0.          0.        ] False -0.5441605769036346\n",
      "Step 28: The new observation is [-0.17995453  1.3426011  -0.6842009  -0.32134917  0.25634757  0.20175919\n",
      "  0.          0.        ] False -1.4779534229809599\n",
      "Step 29: The new observation is [-0.18661252  1.3348347  -0.6735043  -0.3465418   0.26422206  0.15749043\n",
      "  0.          0.        ] False -0.29048826019211904\n",
      "Step 30: The new observation is [-0.1932706   1.3264692  -0.6735023  -0.37321347  0.2720965   0.15748946\n",
      "  0.          0.        ] False -1.3100351329787827\n",
      "Step 31: The new observation is [-0.19999894  1.3174703  -0.68236005 -0.40175498  0.28186995  0.19546881\n",
      "  0.          0.        ] False -2.401384921170289\n",
      "Step 32: The new observation is [-0.20672755  1.3078725  -0.6823567  -0.42842934  0.29164332  0.19546732\n",
      "  0.          0.        ] False -1.517774045266492\n",
      "Step 33: The new observation is [-0.21336165  1.297709   -0.67046446 -0.45315298  0.29892457  0.14562465\n",
      "  0.          0.        ] False -0.21375330943169388\n",
      "Step 34: The new observation is [-0.22021627  1.2883896  -0.69283086 -0.41572544  0.30654162  0.15234095\n",
      "  0.          0.        ] False -0.13054445977001022\n",
      "Step 35: The new observation is [-0.22712736  1.2784386  -0.69996554 -0.44416702  0.3157232   0.18363261\n",
      "  0.          0.        ] False -2.187539568150298\n",
      "Step 36: The new observation is [-0.23424116  1.2692001  -0.72055686 -0.41262963  0.3252697   0.19092982\n",
      "  0.          0.        ] False -0.6067199465027102\n",
      "Step 37: The new observation is [-0.24128886  1.2593949  -0.7121845  -0.43747923  0.3330078   0.15476203\n",
      "  0.          0.        ] False -0.5183645628440263\n",
      "Step 38: The new observation is [-0.24824509  1.2490273  -0.7006897  -0.4619597   0.33830047  0.10585332\n",
      "  0.          0.        ] False -0.019881428572630283\n",
      "Step 39: The new observation is [-0.25552553  1.2389514  -0.7326183  -0.4488968   0.34309062  0.09580343\n",
      "  0.          0.        ] False -1.9298147584510048\n",
      "Step 40: The new observation is [-0.26287603  1.2282335  -0.74149966 -0.47790238  0.34985313  0.13524982\n",
      "  0.          0.        ] False -2.1040907559590196\n",
      "Step 41: The new observation is [-0.2702856   1.2168758  -0.74899065 -0.5067655   0.35830948  0.16912682\n",
      "  0.          0.        ] False -2.139653765365607\n",
      "Step 42: The new observation is [-0.27769536  1.204919   -0.74898756 -0.5334378   0.36676577  0.16912596\n",
      "  0.          0.        ] False -1.3639204303798635\n",
      "Step 43: The new observation is [-0.28502688  1.1924063  -0.7390779  -0.557651    0.37303695  0.12542383\n",
      "  0.          0.        ] False -0.2390376818904076\n",
      "Step 44: The new observation is [-0.29235855  1.1792939  -0.7390762  -0.5843207   0.37930813  0.1254235\n",
      "  0.          0.        ] False -1.156780418690687\n",
      "Step 45: The new observation is [-0.2998701   1.1661345  -0.75692004 -0.58641016  0.38543695  0.12257625\n",
      "  0.          0.        ] False -1.354881292139129\n",
      "Step 46: The new observation is [-0.30730638  1.1524093  -0.7474602  -0.6110532   0.38952306  0.08172239\n",
      "  0.          0.        ] False -0.09377145087012081\n",
      "Step 47: The new observation is [-0.31474274  1.1380843  -0.74745935 -0.6377211   0.39360917  0.0817223\n",
      "  0.          0.        ] False -0.9306042777489267\n",
      "Step 48: The new observation is [-0.3222459   1.1231076  -0.75599134 -0.6672135   0.39968297  0.12147623\n",
      "  0.          0.        ] False -1.9767174391641322\n",
      "Step 49: The new observation is [-0.3296593   1.1075795  -0.74468195 -0.69108284  0.4032593   0.07152711\n",
      "  0.          0.        ] False 0.13161981649719337\n",
      "Step 50: The new observation is [-0.33700672  1.0914959  -0.7362902  -0.71526873  0.40492198  0.03325398\n",
      "  0.          0.        ] False 0.07288950287582566\n",
      "Step 51: The new observation is [-0.34435415  1.0748124  -0.7362901  -0.74193555  0.40658465  0.03325384\n",
      "  0.          0.        ] False -0.6708704757283499\n",
      "Step 52: The new observation is [-0.35177732  1.057492   -0.74579084 -0.7707929   0.41032186  0.07474419\n",
      "  0.          0.        ] False -1.7137484767556816\n",
      "Step 53: The new observation is [-0.35928935  1.0395217  -0.7569874  -0.8003767   0.41655457  0.12465429\n",
      "  0.          0.        ] False -2.1044312893552317\n",
      "Step 54: The new observation is [-0.36708364  1.0218959  -0.785189   -0.78508186  0.42277095  0.12432741\n",
      "  0.          0.        ] False -0.38821785259203806\n",
      "Step 55: The new observation is [-0.37495017  1.0036337  -0.7942152  -0.8139432   0.43097553  0.16409162\n",
      "  0.          0.        ] False -2.093922636281006\n",
      "Step 56: The new observation is [-0.38317838  0.9856467  -0.82999754 -0.80164844  0.4387865   0.15621975\n",
      "  0.          0.        ] False -1.3630578482439433\n",
      "Step 57: The new observation is [-0.39133397  0.9671069  -0.8207871  -0.825639    0.44449812  0.11423244\n",
      "  0.          0.        ] False -0.20670052568490746\n",
      "Step 58: The new observation is [-0.39943537  0.94800454 -0.81391466 -0.8501991   0.44862494  0.08253615\n",
      "  0.          0.        ] False -0.2644494270323594\n",
      "Step 59: The new observation is [-0.40780893  0.92964953 -0.8417266  -0.81719166  0.4534189   0.09587933\n",
      "  0.          0.        ] False 0.9586304112857078\n",
      "Step 60: The new observation is [-0.41610765  0.9107397  -0.8322937  -0.84122634  0.4560794   0.05320937\n",
      "  0.          0.        ] False 0.06937432884646569\n",
      "Step 61: The new observation is [-0.42440644  0.89123    -0.83229315 -0.86789346  0.45873985  0.05320933\n",
      "  0.          0.        ] False -0.7590803123186447\n",
      "Step 62: The new observation is [-0.43276817  0.87106794 -0.8403411  -0.8974732   0.46333292  0.09186156\n",
      "  0.          0.        ] False -1.7427082287739768\n",
      "Step 63: The new observation is [-0.44112998  0.85030603 -0.84033996 -0.92414147  0.46792597  0.09186102\n",
      "  0.          0.        ] False -0.9464980476232654\n",
      "Step 64: The new observation is [-0.44954818  0.82961583 -0.84660184 -0.9211845   0.4732201   0.10588263\n",
      "  0.          0.        ] False 0.399922718515154\n",
      "Step 65: The new observation is [-0.4580502   0.8082546  -0.85730016 -0.95183194  0.48110685  0.15773478\n",
      "  0.          0.        ] False -2.3492956477948312\n",
      "Step 66: The new observation is [-0.4665525   0.7862942  -0.85729665 -0.97850317  0.48899356  0.15773413\n",
      "  0.          0.        ] False -1.3092720278871752\n",
      "Step 67: The new observation is [-0.47505504  0.7637345  -0.8572931  -1.0051744   0.49688023  0.1577338\n",
      "  0.          0.        ] False -1.3196499019002204\n",
      "Step 68: The new observation is [-0.4834721   0.74064004 -0.8464283  -1.0281427   0.50218815  0.10615865\n",
      "  0.          0.        ] False -0.1281650903935929\n",
      "Step 69: The new observation is [-0.49195558  0.7168855  -0.8549163  -1.0581762   0.5095983   0.14820395\n",
      "  0.          0.        ] False -2.132738569048995\n",
      "Step 70: The new observation is [-0.5003543   0.692596   -0.8441478  -1.0811367   0.51444304  0.09689467\n",
      "  0.          0.        ] False -0.14018130348634258\n",
      "Step 71: The new observation is [-0.5086978   0.66774416 -0.83719367 -1.1055993   0.5176659   0.06445773\n",
      "  0.          0.        ] False -0.3688450725892085\n",
      "Step 72: The new observation is [-0.5169564   0.6423632  -0.8263715  -1.1282382   0.5182526   0.01173411\n",
      "  0.          0.        ] False 0.2313351376932087\n",
      "Step 73: The new observation is [-0.525142    0.6164556  -0.8169695  -1.1508454   0.51644367 -0.03617861\n",
      "  0.          0.        ] False 0.34060768066436364\n",
      "Step 74: The new observation is [-0.53368306  0.5913225  -0.85309285 -1.1166476   0.51529723 -0.02292872\n",
      "  0.          0.        ] False 1.7525728604299047\n",
      "Step 75: The new observation is [-0.5426334   0.5664693  -0.8937572  -1.104097    0.5138398  -0.02914965\n",
      "  0.          0.        ] False -0.47102156090760444\n",
      "Step 76: The new observation is [-0.55164856  0.540969   -0.9019324  -1.1335043   0.5143167   0.00953835\n",
      "  0.          0.        ] False -1.7027558248112473\n",
      "Step 77: The new observation is [-0.5607444   0.5147969  -0.912247   -1.1642127   0.5173432   0.06053075\n",
      "  0.          0.        ] False -2.240163282940641\n",
      "Step 78: The new observation is [-0.5703465   0.48909703 -0.96252644 -1.1430976   0.5199684   0.05250393\n",
      "  0.          0.        ] False -1.1065437698779192\n",
      "Step 79: The new observation is [-0.5800289   0.46274072 -0.9726232  -1.1730777   0.5249698   0.10002911\n",
      "  0.          0.        ] False -2.5441171559255893\n",
      "Step 80: The new observation is [-0.58982503  0.43644056 -0.9844867  -1.170787    0.53052914  0.11118708\n",
      "  0.          0.        ] False -0.6145375890824198\n",
      "Step 81: The new observation is [-0.599562    0.4095832  -0.97700006 -1.1949607   0.534319    0.07579684\n",
      "  0.          0.        ] False -1.0288715688214143\n",
      "Step 82: The new observation is [-0.60929894  0.38212597 -0.97699916 -1.2216283   0.5381088   0.07579635\n",
      "  0.          0.        ] False -1.762951328083716\n",
      "Step 83: The new observation is [-0.61910677  0.35399723 -0.98609954 -1.252298    0.54423887  0.12259914\n",
      "  0.          0.        ] False -3.006663895389208\n",
      "Step 84: The new observation is [-0.6289147   0.32526898 -0.98609716 -1.2789673   0.5503687   0.1225988\n",
      "  0.          0.        ] False -2.204788686086715\n",
      "Step 85: The new observation is [-0.6387228   0.29594108 -0.9860946  -1.3056368   0.5564987   0.12259849\n",
      "  0.          0.        ] False -2.3232627952907023\n",
      "Step 86: The new observation is [-0.64846957  0.26605704 -0.9783751  -1.3297211   0.560801    0.08604698\n",
      "  0.          0.        ] False -1.6273902960825513\n",
      "Step 87: The new observation is [-0.6581414   0.23563321 -0.96888703 -1.3528842   0.56279075  0.03979431\n",
      "  0.          0.        ] False -1.3585266022953124\n",
      "Step 88: The new observation is [-0.6680432   0.20516282 -0.9918947  -1.3549618   0.5647908   0.0400003\n",
      "  0.          0.        ] False -1.9962195640333562\n",
      "Step 89: The new observation is [-0.67800987  0.17403646 -1.0001237  -1.384858    0.56883556  0.08089601\n",
      "  0.          0.        ] False -3.4517178834718423\n",
      "Step 90: The new observation is [-0.6878984   0.1423766  -0.9902066  -1.4076823   0.5704255   0.03179936\n",
      "  0.          0.        ] False -1.7208232546031172\n",
      "Step 91: The new observation is [-0.6978465   0.11006571 -0.997766   -1.4373072   0.5738929   0.06934737\n",
      "  0.          0.        ] False -3.6376032147643103\n",
      "Step 92: The new observation is [-0.7081494   0.07759903 -1.0326935  -1.4440067   0.5767254   0.05665081\n",
      "  0.          0.        ] False -3.7343031633337658\n",
      "Step 93: The new observation is [-0.7185247   0.04447411 -1.041834   -1.4740897   0.5817956   0.10140356\n",
      "  0.          0.        ] False -4.269615815066259\n",
      "Step 94: The new observation is [-0.72883373  0.01080924 -1.0333917  -1.497313    0.5847386   0.05886071\n",
      "  0.          1.        ] False 7.353735620968991\n",
      "Step 95: The new observation is [-0.7389261  -0.02257571 -1.0194491  -1.4975319   0.58383214 -0.05038689\n",
      "  0.          1.        ] False -0.20481970697366478\n",
      "Step 96: The new observation is [-0.7475886  -0.02992529 -0.7014135  -0.18387103  0.4329285  -3.1976857\n",
      "  0.          1.        ] True -100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Number of steps you run the agent for \n",
    "num_steps = 150\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # take random action, but you can also do something more intelligent\n",
    "    # action = my_intelligent_agent_fn(obs) \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # apply the action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"Step {}: The new observation is {}\".format(step, obs), done, reward)\n",
    "\n",
    "    \n",
    "    # Render the env\n",
    "    image = env.render(mode='rgb_array')\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Wait a bit before the next frame unless you want to see a crazy fast video\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    # If the epsiode is up, then start another one\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Close the env\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/Users/broxoli/miniforge3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n",
      "2022-06-17 15:49:13.372096: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-17 15:49:13.372399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 4 5 0.1\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Needed for training the network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "# Needed for animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def construct_q_network(state_dim, action_dim):\n",
    "    \"\"\"Construct the critic network with q-values per action as output\"\"\"\n",
    "    inputs = layers.Input(shape=(state_dim))  # input dimension\n",
    "    hidden1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "    hidden2 = layers.Dense(10, activation=\"relu\")(hidden1)\n",
    "    hidden3 = layers.Dense(10, activation=\"relu\")(hidden2)\n",
    "    q_values = layers.Dense(action_dim, activation='linear')(hidden3)\n",
    "\n",
    "    network = keras.Model(inputs=inputs, outputs=[q_values])\n",
    "    network.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def mean_squared_error_loss(q_value, reward):\n",
    "    \"\"\"Compute mean squared error loss\"\"\"\n",
    "    loss = 0.5 * (q_value - reward) ** 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_EPISODES = 5\n",
    "EXPLORATION_RATE = .1\n",
    "\n",
    "print(N_STATES, N_ACTIONS, N_EPISODES, EXPLORATION_RATE)\n",
    "\n",
    "model = construct_q_network(N_STATES, N_ACTIONS)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Loss:  0.2374959\n",
      "Step:  1 Loss:  0.25885937\n",
      "Step:  2 Loss:  0.2699691\n",
      "Step:  3 Loss:  0.9541627\n",
      "Step:  4 Loss:  0.9707625\n",
      "Step:  5 Loss:  0.9775095\n",
      "Step:  6 Loss:  0.98116446\n",
      "Step:  7 Loss:  0.30525386\n",
      "Step:  8 Loss:  0.30178857\n",
      "Step:  9 Loss:  0.2929194\n",
      "Step:  10 Loss:  0.27786842\n",
      "Step:  11 Loss:  0.25622568\n",
      "Step:  12 Loss:  0.4174358\n",
      "Step:  13 Loss:  0.40927434\n",
      "Step:  14 Loss:  0.39235103\n",
      "Step:  15 Loss:  0.36840346\n",
      "Step:  16 Loss:  0.15063646\n",
      "Step:  17 Loss:  0.097043455\n",
      "Step:  18 Loss:  0.05298369\n",
      "Step:  19 Loss:  0.024042325\n",
      "Step:  20 Loss:  0.014255213\n",
      "Step:  21 Loss:  0.025192752\n",
      "Step:  22 Loss:  0.055516332\n",
      "Step:  23 Loss:  0.10134377\n",
      "Step:  24 Loss:  0.09467633\n",
      "Step:  25 Loss:  0.07847481\n",
      "Step:  26 Loss:  0.088016376\n",
      "Step:  27 Loss:  0.09290188\n",
      "Step:  28 Loss:  0.15413482\n",
      "Step:  29 Loss:  0.22231515\n",
      "Step:  30 Loss:  0.18149409\n",
      "Step:  31 Loss:  0.18355937\n",
      "Step:  32 Loss:  0.1777664\n",
      "Step:  33 Loss:  0.16462779\n",
      "Step:  34 Loss:  0.21984836\n",
      "Step:  35 Loss:  0.21983115\n",
      "Step:  36 Loss:  0.87302136\n",
      "Step:  37 Loss:  1.0220964\n",
      "Step:  38 Loss:  1.1691866\n",
      "Step:  39 Loss:  1.1994399\n",
      "Step:  40 Loss:  0.6203376\n",
      "Step:  41 Loss:  0.5743003\n",
      "Step:  42 Loss:  0.48556095\n",
      "Step:  43 Loss:  1.5409286\n",
      "Step:  44 Loss:  1.803112\n",
      "Step:  45 Loss:  1.7439029\n",
      "Step:  46 Loss:  1.6324784\n",
      "Step:  47 Loss:  0.66417944\n",
      "Step:  48 Loss:  0.7040337\n",
      "Step:  49 Loss:  0.81864476\n",
      "Step:  50 Loss:  1.2169503\n",
      "Step:  51 Loss:  1.1512644\n",
      "Step:  52 Loss:  1.005312\n",
      "Step:  53 Loss:  0.80683947\n",
      "Step:  54 Loss:  0.49156997\n",
      "Step:  55 Loss:  0.4397927\n",
      "Step:  56 Loss:  0.23121016\n",
      "Step:  57 Loss:  0.24516024\n",
      "Step:  58 Loss:  0.19438173\n",
      "Step:  59 Loss:  0.28632373\n",
      "Step:  60 Loss:  0.33432716\n",
      "Step:  61 Loss:  0.3372387\n",
      "Step:  62 Loss:  0.33134922\n",
      "Step:  63 Loss:  0.28231\n",
      "Step:  64 Loss:  0.18073156\n",
      "Step:  65 Loss:  0.4188136\n",
      "Step:  66 Loss:  1.0169601\n",
      "Step:  67 Loss:  1.5454779\n",
      "Step:  68 Loss:  1.8413379\n",
      "Step:  69 Loss:  1.8542168\n",
      "Step:  70 Loss:  1.5957358\n",
      "Step:  71 Loss:  1.3511436\n",
      "Step:  72 Loss:  1.6953685\n",
      "Step:  73 Loss:  1.5140373\n",
      "Step:  74 Loss:  2.126204\n",
      "Step:  75 Loss:  1.8936875\n",
      "Step:  76 Loss:  1.3875976\n",
      "Step:  77 Loss:  1.4597857\n",
      "Step:  78 Loss:  0.673599\n",
      "Step:  79 Loss:  0.72300535\n",
      "Step:  80 Loss:  0.77400273\n",
      "Step:  81 Loss:  0.826364\n",
      "Step:  82 Loss:  0.87937665\n",
      "Step:  83 Loss:  0.92937684\n",
      "Step:  84 Loss:  1.0420034\n",
      "Step:  85 Loss:  0.87857026\n",
      "Step:  86 Loss:  0.73735297\n",
      "Step:  87 Loss:  0.91675663\n",
      "Step:  88 Loss:  1.1103876\n",
      "Step:  89 Loss:  1.5742495\n",
      "Step:  90 Loss:  1.9949187\n",
      "Step:  91 Loss:  1.5951684\n",
      "Step:  92 Loss:  1.7887536\n",
      "Step:  93 Loss:  1.7825828\n",
      "Step:  94 Loss:  1.7774328\n",
      "Step:  95 Loss:  1.8138487\n",
      "Step:  96 Loss:  1.16294\n",
      "Step:  97 Loss:  0.7509397\n",
      "Step:  98 Loss:  0.57628196\n",
      "Step:  99 Loss:  0.61245465\n",
      "Step:  100 Loss:  0.85746026\n",
      "Step:  101 Loss:  0.87967\n",
      "Step:  102 Loss:  0.7803871\n",
      "Step:  103 Loss:  0.8736605\n",
      "Step:  104 Loss:  2.03138\n",
      "Step:  105 Loss:  2.6476824\n",
      "Step:  106 Loss:  2.7106953\n",
      "Step:  107 Loss:  3.4994674\n",
      "Step:  108 Loss:  2.073851\n",
      "Step:  109 Loss:  1.373206\n",
      "Step:  110 Loss:  1.2993133\n",
      "Step:  111 Loss:  0.42708266\n",
      "Step:  112 Loss:  0.51683027\n",
      "Step:  113 Loss:  0.6107108\n",
      "Step:  114 Loss:  0.5454463\n",
      "Step:  115 Loss:  0.6151121\n",
      "Step:  116 Loss:  0.6793623\n",
      "Step:  117 Loss:  1.7517228\n",
      "Step:  118 Loss:  2.517087\n",
      "Step:  119 Loss:  3.0387216\n",
      "Step:  120 Loss:  3.817853\n",
      "Step:  121 Loss:  3.3551056\n",
      "Step:  122 Loss:  2.426487\n",
      "Step:  123 Loss:  2.8102365\n",
      "Step:  124 Loss:  3.7966442\n",
      "Step:  125 Loss:  3.6380005\n",
      "Step:  126 Loss:  5.646287\n",
      "Step:  127 Loss:  6.2553477\n",
      "Step:  128 Loss:  5.300275\n",
      "Step:  129 Loss:  4.8821106\n",
      "Step:  130 Loss:  3.4590611\n",
      "Step:  131 Loss:  1.8045182\n",
      "Step:  132 Loss:  0.80726075\n",
      "Step:  133 Loss:  0.6318211\n",
      "Step:  134 Loss:  0.06449349\n",
      "Step:  135 Loss:  0.03292693\n",
      "Step:  136 Loss:  0.03705906\n",
      "Step:  137 Loss:  0.04114747\n",
      "Step:  138 Loss:  0.045125335\n",
      "Step:  139 Loss:  0.048924394\n",
      "Step:  140 Loss:  0.052481115\n",
      "Step:  141 Loss:  0.05574359\n",
      "Step:  142 Loss:  0.058658313\n",
      "Step:  143 Loss:  0.061189316\n",
      "Step:  144 Loss:  0.06330733\n",
      "Step:  145 Loss:  0.06498733\n",
      "Step:  146 Loss:  0.066222124\n",
      "Step:  147 Loss:  0.06700668\n",
      "Step:  148 Loss:  0.0673451\n",
      "Step:  149 Loss:  0.06725621\n",
      "Step:  150 Loss:  0.06660956\n",
      "Step:  151 Loss:  0.23046497\n",
      "Step:  152 Loss:  0.24662316\n",
      "Step:  153 Loss:  0.44607735\n",
      "Step:  154 Loss:  0.48254186\n",
      "Step:  155 Loss:  0.496325\n",
      "Step:  156 Loss:  0.5342467\n",
      "Step:  157 Loss:  0.5956375\n",
      "Step:  158 Loss:  0.63943005\n",
      "Step:  159 Loss:  0.5407185\n",
      "Step:  160 Loss:  0.5636151\n",
      "Step:  161 Loss:  0.3808896\n",
      "Step:  162 Loss:  0.548234\n",
      "Step:  163 Loss:  0.5696409\n",
      "Step:  164 Loss:  0.5915511\n",
      "Step:  165 Loss:  0.6140089\n",
      "Step:  166 Loss:  0.4642565\n",
      "Step:  167 Loss:  0.462103\n",
      "Step:  168 Loss:  0.46160007\n",
      "Step:  169 Loss:  0.4630792\n",
      "Step:  170 Loss:  0.46691558\n",
      "Step:  171 Loss:  0.6131371\n",
      "Step:  172 Loss:  0.8304224\n",
      "Step:  173 Loss:  0.88671535\n",
      "Step:  174 Loss:  0.9490875\n",
      "Step:  175 Loss:  0.8782009\n",
      "Step:  176 Loss:  0.74991345\n",
      "Step:  177 Loss:  0.79717326\n",
      "Step:  178 Loss:  1.1015757\n",
      "Step:  179 Loss:  1.2241621\n",
      "Step:  180 Loss:  1.3664724\n",
      "Step:  181 Loss:  1.5325062\n",
      "Step:  182 Loss:  1.4814241\n",
      "Step:  183 Loss:  2.053647\n",
      "Step:  184 Loss:  2.9480338\n",
      "Step:  185 Loss:  3.4276686\n",
      "Step:  186 Loss:  3.6438298\n",
      "Step:  187 Loss:  3.3503368\n",
      "Step:  188 Loss:  3.1399293\n",
      "Step:  189 Loss:  3.4412875\n",
      "Step:  190 Loss:  4.6977673\n",
      "Step:  191 Loss:  5.6412625\n",
      "Step:  192 Loss:  6.1071177\n",
      "Step:  193 Loss:  87.5361\n",
      "Step:  194 Loss:  94.543625\n",
      "Step:  195 Loss:  106.526955\n",
      "Step:  196 Loss:  108.70323\n",
      "Step:  197 Loss:  26.911678\n",
      "Step:  198 Loss:  17.982567\n",
      "Step:  199 Loss:  5.3927965\n",
      "Step:  200 Loss:  2.3279116\n",
      "Step:  201 Loss:  2.6216698\n",
      "Step:  202 Loss:  2.5592072\n",
      "Step:  203 Loss:  2.2112021\n",
      "Step:  204 Loss:  10.727355\n",
      "Step:  205 Loss:  90.44609\n",
      "Step:  206 Loss:  97.46856\n",
      "Step:  207 Loss:  161.60721\n",
      "Step:  208 Loss:  165.06267\n",
      "Step:  209 Loss:  714.326\n",
      "Finished after Step:  209\n",
      "Step:  210 Loss:  707.0926\n",
      "Step:  211 Loss:  641.4624\n",
      "Step:  212 Loss:  625.4762\n",
      "Step:  213 Loss:  0.04580248\n",
      "Step:  214 Loss:  0.1008313\n",
      "Step:  215 Loss:  0.37127537\n",
      "Step:  216 Loss:  0.49225903\n",
      "Step:  217 Loss:  0.6188861\n",
      "Step:  218 Loss:  0.74071425\n",
      "Step:  219 Loss:  0.66226023\n",
      "Step:  220 Loss:  0.7349379\n",
      "Step:  221 Loss:  0.7898606\n",
      "Step:  222 Loss:  0.82805496\n",
      "Step:  223 Loss:  0.85135496\n",
      "Step:  224 Loss:  0.8619037\n",
      "Step:  225 Loss:  0.861621\n",
      "Step:  226 Loss:  1.0980043\n",
      "Step:  227 Loss:  0.9938303\n",
      "Step:  228 Loss:  0.88824475\n",
      "Step:  229 Loss:  1.022924\n",
      "Step:  230 Loss:  0.6449673\n",
      "Step:  231 Loss:  0.5766121\n",
      "Step:  232 Loss:  0.50512934\n",
      "Step:  233 Loss:  0.23072302\n",
      "Step:  234 Loss:  0.17970625\n",
      "Step:  235 Loss:  0.16980472\n",
      "Step:  236 Loss:  0.16363326\n",
      "Step:  237 Loss:  0.12149\n",
      "Step:  238 Loss:  0.10125321\n",
      "Step:  239 Loss:  0.0731864\n",
      "Step:  240 Loss:  0.04738895\n",
      "Step:  241 Loss:  0.019295871\n",
      "Step:  242 Loss:  0.02120215\n",
      "Step:  243 Loss:  0.012844272\n",
      "Step:  244 Loss:  0.008221202\n",
      "Step:  245 Loss:  0.0122137405\n",
      "Step:  246 Loss:  0.008089973\n",
      "Step:  247 Loss:  0.10746737\n",
      "Step:  248 Loss:  0.25022838\n",
      "Step:  249 Loss:  0.38124982\n",
      "Step:  250 Loss:  0.5060698\n",
      "Step:  251 Loss:  0.5381064\n",
      "Step:  252 Loss:  0.5417993\n",
      "Step:  253 Loss:  0.5453292\n",
      "Step:  254 Loss:  0.5489942\n",
      "Step:  255 Loss:  0.56172395\n",
      "Step:  256 Loss:  0.5561824\n",
      "Step:  257 Loss:  0.59322196\n",
      "Step:  258 Loss:  0.6736861\n",
      "Step:  259 Loss:  0.70637715\n",
      "Step:  260 Loss:  0.73105097\n",
      "Step:  261 Loss:  0.71313703\n",
      "Step:  262 Loss:  0.73514795\n",
      "Step:  263 Loss:  0.8240173\n",
      "Step:  264 Loss:  0.8752117\n",
      "Step:  265 Loss:  1.0040839\n",
      "Step:  266 Loss:  1.0955067\n",
      "Step:  267 Loss:  1.2130905\n",
      "Step:  268 Loss:  1.2973089\n",
      "Step:  269 Loss:  1.3048093\n",
      "Step:  270 Loss:  1.4028136\n",
      "Step:  271 Loss:  1.53515\n",
      "Step:  272 Loss:  1.6399789\n",
      "Step:  273 Loss:  1.7574141\n",
      "Step:  274 Loss:  1.7595539\n",
      "Step:  275 Loss:  1.6938107\n",
      "Step:  276 Loss:  1.7952309\n",
      "Step:  277 Loss:  2.022866\n",
      "Step:  278 Loss:  3.6253262\n",
      "Step:  279 Loss:  30.64877\n",
      "Step:  280 Loss:  652.6873\n",
      "Finished after Step:  280\n",
      "Step:  281 Loss:  652.05725\n",
      "Step:  282 Loss:  650.15295\n",
      "Step:  283 Loss:  622.6589\n",
      "Step:  284 Loss:  1.4206651\n",
      "Step:  285 Loss:  1.6572948\n",
      "Step:  286 Loss:  1.8544953\n",
      "Step:  287 Loss:  2.1084929\n",
      "Step:  288 Loss:  2.336112\n",
      "Step:  289 Loss:  2.5774298\n",
      "Step:  290 Loss:  2.8029146\n",
      "Step:  291 Loss:  3.0743887\n",
      "Step:  292 Loss:  3.3242521\n",
      "Step:  293 Loss:  3.655293\n",
      "Step:  294 Loss:  3.3095093\n",
      "Step:  295 Loss:  3.4625916\n",
      "Step:  296 Loss:  3.5772939\n",
      "Step:  297 Loss:  3.6811826\n",
      "Step:  298 Loss:  4.5101748\n",
      "Step:  299 Loss:  4.698017\n",
      "Step:  300 Loss:  5.0798073\n",
      "Step:  301 Loss:  5.3942013\n",
      "Step:  302 Loss:  5.6391125\n",
      "Step:  303 Loss:  6.121097\n",
      "Step:  304 Loss:  6.5292854\n",
      "Step:  305 Loss:  7.0815763\n",
      "Step:  306 Loss:  7.01515\n",
      "Step:  307 Loss:  6.1648107\n",
      "Step:  308 Loss:  5.6248665\n",
      "Step:  309 Loss:  4.8741865\n",
      "Step:  310 Loss:  4.6248693\n",
      "Step:  311 Loss:  4.9151964\n",
      "Step:  312 Loss:  4.81188\n",
      "Step:  313 Loss:  4.708411\n",
      "Step:  314 Loss:  5.245084\n",
      "Step:  315 Loss:  5.8801985\n",
      "Step:  316 Loss:  6.802471\n",
      "Step:  317 Loss:  7.84613\n",
      "Step:  318 Loss:  9.597135\n",
      "Step:  319 Loss:  9.977695\n",
      "Step:  320 Loss:  10.097647\n",
      "Step:  321 Loss:  11.5292845\n",
      "Step:  322 Loss:  10.53747\n",
      "Step:  323 Loss:  9.915825\n",
      "Step:  324 Loss:  9.076509\n",
      "Step:  325 Loss:  8.120417\n",
      "Step:  326 Loss:  7.4100256\n",
      "Step:  327 Loss:  8.708998\n",
      "Step:  328 Loss:  13.505602\n",
      "Step:  329 Loss:  14.2341\n",
      "Step:  330 Loss:  16.470966\n",
      "Step:  331 Loss:  18.51742\n",
      "Step:  332 Loss:  627.53564\n",
      "Finished after Step:  332\n",
      "Step:  333 Loss:  625.34106\n",
      "Step:  334 Loss:  622.3124\n",
      "Step:  335 Loss:  618.33276\n",
      "Step:  336 Loss:  0.30902183\n",
      "Step:  337 Loss:  0.31634256\n",
      "Step:  338 Loss:  0.30324042\n",
      "Step:  339 Loss:  0.28597927\n",
      "Step:  340 Loss:  0.3620153\n",
      "Step:  341 Loss:  0.5017797\n",
      "Step:  342 Loss:  0.5236877\n",
      "Step:  343 Loss:  0.45167565\n",
      "Step:  344 Loss:  0.3620708\n",
      "Step:  345 Loss:  0.12910013\n",
      "Step:  346 Loss:  0.024080206\n",
      "Step:  347 Loss:  0.011474187\n",
      "Step:  348 Loss:  0.025547594\n",
      "Step:  349 Loss:  0.04653068\n",
      "Step:  350 Loss:  0.08201309\n",
      "Step:  351 Loss:  0.0755338\n",
      "Step:  352 Loss:  0.27219594\n",
      "Step:  353 Loss:  0.5032648\n",
      "Step:  354 Loss:  0.8035611\n",
      "Step:  355 Loss:  1.1875615\n",
      "Step:  356 Loss:  1.3855612\n",
      "Step:  357 Loss:  1.6430697\n",
      "Step:  358 Loss:  1.7828017\n",
      "Step:  359 Loss:  1.9293431\n",
      "Step:  360 Loss:  2.0788267\n",
      "Step:  361 Loss:  2.1049657\n",
      "Step:  362 Loss:  2.1813462\n",
      "Step:  363 Loss:  2.3143554\n",
      "Step:  364 Loss:  2.5010839\n",
      "Step:  365 Loss:  2.7511783\n",
      "Step:  366 Loss:  3.0967498\n",
      "Step:  367 Loss:  3.2695615\n",
      "Step:  368 Loss:  3.4966655\n",
      "Step:  369 Loss:  3.606132\n",
      "Step:  370 Loss:  3.7528183\n",
      "Step:  371 Loss:  3.8427527\n",
      "Step:  372 Loss:  4.0332947\n",
      "Step:  373 Loss:  4.3913374\n",
      "Step:  374 Loss:  3.347847\n",
      "Step:  375 Loss:  3.867542\n",
      "Step:  376 Loss:  2.7150855\n",
      "Step:  377 Loss:  1.971937\n",
      "Step:  378 Loss:  2.776671\n",
      "Step:  379 Loss:  1.4373345\n",
      "Step:  380 Loss:  2.8795166\n",
      "Step:  381 Loss:  3.1723692\n",
      "Step:  382 Loss:  3.7926235\n",
      "Step:  383 Loss:  5.198584\n",
      "Step:  384 Loss:  5.4115553\n",
      "Step:  385 Loss:  5.9223633\n",
      "Step:  386 Loss:  5.8274236\n",
      "Step:  387 Loss:  6.286252\n",
      "Step:  388 Loss:  6.638274\n",
      "Step:  389 Loss:  7.559828\n",
      "Step:  390 Loss:  9.070205\n",
      "Step:  391 Loss:  7.7775507\n",
      "Step:  392 Loss:  46.49635\n",
      "Step:  393 Loss:  652.48755\n",
      "Finished after Step:  393\n",
      "Step:  394 Loss:  651.90674\n",
      "Step:  395 Loss:  653.5289\n",
      "Step:  396 Loss:  616.3588\n",
      "Step:  397 Loss:  0.09464501\n",
      "Step:  398 Loss:  0.08707096\n",
      "Step:  399 Loss:  0.07791996\n",
      "Step:  400 Loss:  0.069682494\n",
      "Step:  401 Loss:  0.020250501\n",
      "Step:  402 Loss:  0.022904318\n",
      "Step:  403 Loss:  0.026152927\n",
      "Step:  404 Loss:  0.030060453\n",
      "Step:  405 Loss:  0.034692988\n",
      "Step:  406 Loss:  0.04011023\n",
      "Step:  407 Loss:  0.04636173\n",
      "Step:  408 Loss:  0.053486686\n",
      "Step:  409 Loss:  0.06150599\n",
      "Step:  410 Loss:  0.07041812\n",
      "Step:  411 Loss:  0.08020311\n",
      "Step:  412 Loss:  0.09081131\n",
      "Step:  413 Loss:  0.102168106\n",
      "Step:  414 Loss:  0.11417483\n",
      "Step:  415 Loss:  0.1265347\n",
      "Step:  416 Loss:  0.37894845\n",
      "Step:  417 Loss:  0.41313404\n",
      "Step:  418 Loss:  0.37762424\n",
      "Step:  419 Loss:  0.33951306\n",
      "Step:  420 Loss:  0.07159867\n",
      "Step:  421 Loss:  0.03019993\n",
      "Step:  422 Loss:  0.06860342\n",
      "Step:  423 Loss:  0.1276918\n",
      "Step:  424 Loss:  0.121851\n",
      "Step:  425 Loss:  0.22447595\n",
      "Step:  426 Loss:  0.21854514\n",
      "Step:  427 Loss:  0.19900131\n",
      "Step:  428 Loss:  0.27553177\n",
      "Step:  429 Loss:  0.25547242\n",
      "Step:  430 Loss:  0.23466012\n",
      "Step:  431 Loss:  0.31838167\n",
      "Step:  432 Loss:  0.40195382\n",
      "Step:  433 Loss:  0.45200154\n",
      "Step:  434 Loss:  0.6618421\n",
      "Step:  435 Loss:  0.5439664\n",
      "Step:  436 Loss:  0.6426002\n",
      "Step:  437 Loss:  0.72091067\n",
      "Step:  438 Loss:  0.7526532\n",
      "Step:  439 Loss:  0.7608709\n",
      "Step:  440 Loss:  0.8381674\n",
      "Step:  441 Loss:  1.0529418\n",
      "Step:  442 Loss:  1.2883337\n",
      "Step:  443 Loss:  1.8940003\n",
      "Step:  444 Loss:  2.2680044\n",
      "Step:  445 Loss:  2.6306586\n",
      "Step:  446 Loss:  2.129016\n",
      "Step:  447 Loss:  2.441889\n",
      "Step:  448 Loss:  2.7683587\n",
      "Step:  449 Loss:  3.1998756\n",
      "Step:  450 Loss:  4.3982973\n",
      "Step:  451 Loss:  4.696606\n",
      "Step:  452 Loss:  5.0039005\n",
      "Step:  453 Loss:  5.088788\n",
      "Step:  454 Loss:  5.231336\n",
      "Step:  455 Loss:  5.558325\n",
      "Step:  456 Loss:  5.950699\n",
      "Step:  457 Loss:  4.8020525\n",
      "Step:  458 Loss:  3.4960842\n",
      "Step:  459 Loss:  2.1804702\n",
      "Step:  460 Loss:  0.56319153\n",
      "Step:  461 Loss:  0.53792256\n",
      "Step:  462 Loss:  0.8694397\n",
      "Step:  463 Loss:  1.9043096\n",
      "Step:  464 Loss:  2.898998\n",
      "Step:  465 Loss:  4.5010924\n",
      "Step:  466 Loss:  6.1611032\n",
      "Step:  467 Loss:  6.9476004\n",
      "Step:  468 Loss:  6.43484\n",
      "Step:  469 Loss:  6.9606056\n",
      "Step:  470 Loss:  7.3281546\n",
      "Step:  471 Loss:  6.045969\n",
      "Step:  472 Loss:  7.5324135\n",
      "Step:  473 Loss:  7.33651\n",
      "Step:  474 Loss:  7.0724382\n",
      "Step:  475 Loss:  9.3161335\n",
      "Step:  476 Loss:  9.625757\n",
      "Step:  477 Loss:  9.978481\n",
      "Step:  478 Loss:  10.381405\n",
      "Step:  479 Loss:  9.989273\n",
      "Step:  480 Loss:  9.745088\n",
      "Step:  481 Loss:  9.197796\n",
      "Step:  482 Loss:  10.620426\n",
      "Step:  483 Loss:  11.4396\n",
      "Step:  484 Loss:  12.998572\n",
      "Step:  485 Loss:  592.1946\n",
      "Finished after Step:  485\n",
      "Step:  486 Loss:  590.6351\n",
      "Step:  487 Loss:  589.03357\n",
      "Step:  488 Loss:  586.36316\n",
      "Step:  489 Loss:  0.54762244\n",
      "Step:  490 Loss:  0.6469189\n",
      "Step:  491 Loss:  0.6908548\n",
      "Step:  492 Loss:  0.64428496\n",
      "Step:  493 Loss:  0.59864676\n",
      "Step:  494 Loss:  0.55412877\n",
      "Step:  495 Loss:  0.51089513\n",
      "Step:  496 Loss:  0.4690706\n",
      "Step:  497 Loss:  0.42876667\n",
      "Step:  498 Loss:  0.39007235\n",
      "Step:  499 Loss:  0.35305032\n",
      "Step:  500 Loss:  0.31776345\n",
      "Step:  501 Loss:  0.28425187\n",
      "Step:  502 Loss:  0.2525484\n",
      "Step:  503 Loss:  0.2226855\n",
      "Step:  504 Loss:  0.19467899\n",
      "Step:  505 Loss:  0.16855267\n",
      "Step:  506 Loss:  0.14431465\n",
      "Step:  507 Loss:  0.10771452\n",
      "Step:  508 Loss:  0.0725378\n",
      "Step:  509 Loss:  0.04315555\n",
      "Step:  510 Loss:  0.024511626\n",
      "Step:  511 Loss:  0.03375411\n",
      "Step:  512 Loss:  0.07821348\n",
      "Step:  513 Loss:  0.14644104\n",
      "Step:  514 Loss:  0.19909614\n",
      "Step:  515 Loss:  0.25644267\n",
      "Step:  516 Loss:  0.29963136\n",
      "Step:  517 Loss:  0.32650626\n",
      "Step:  518 Loss:  0.40677485\n",
      "Step:  519 Loss:  0.5146632\n",
      "Step:  520 Loss:  0.6575492\n",
      "Step:  521 Loss:  0.8193259\n",
      "Step:  522 Loss:  0.9664302\n",
      "Step:  523 Loss:  0.9265292\n",
      "Step:  524 Loss:  0.82450795\n",
      "Step:  525 Loss:  0.622441\n",
      "Step:  526 Loss:  0.37919244\n",
      "Step:  527 Loss:  0.24606797\n",
      "Step:  528 Loss:  0.13548657\n",
      "Step:  529 Loss:  0.06485455\n",
      "Step:  530 Loss:  0.028830335\n",
      "Step:  531 Loss:  0.021116123\n",
      "Step:  532 Loss:  1.6402\n",
      "Step:  533 Loss:  1.6412084\n",
      "Step:  534 Loss:  1.623567\n",
      "Step:  535 Loss:  1.6115634\n",
      "Step:  536 Loss:  1.2839735\n",
      "Step:  537 Loss:  1.2669559\n",
      "Step:  538 Loss:  1.2515872\n",
      "Step:  539 Loss:  1.2093511\n",
      "Step:  540 Loss:  0.07754256\n",
      "Step:  541 Loss:  5.018316\n",
      "Step:  542 Loss:  587.7705\n",
      "Finished after Step:  542\n",
      "Step:  543 Loss:  588.0978\n",
      "Step:  544 Loss:  588.10376\n",
      "Step:  545 Loss:  582.9627\n",
      "Step:  546 Loss:  0.15571961\n",
      "Step:  547 Loss:  0.14802516\n",
      "Step:  548 Loss:  0.13374597\n",
      "Step:  549 Loss:  0.116214186\n",
      "Step:  550 Loss:  0.09650774\n",
      "Step:  551 Loss:  0.075700164\n",
      "Step:  552 Loss:  0.055230677\n",
      "Step:  553 Loss:  0.03682628\n",
      "Step:  554 Loss:  0.022378035\n",
      "Step:  555 Loss:  0.0137425605\n",
      "Step:  556 Loss:  0.012495537\n",
      "Step:  557 Loss:  0.01970043\n",
      "Step:  558 Loss:  0.035709262\n",
      "Step:  559 Loss:  0.21680827\n",
      "Step:  560 Loss:  0.2661485\n",
      "Step:  561 Loss:  0.3240235\n",
      "Step:  562 Loss:  0.3880859\n",
      "Step:  563 Loss:  0.31429353\n",
      "Step:  564 Loss:  0.36223334\n",
      "Step:  565 Loss:  0.44355738\n",
      "Step:  566 Loss:  0.45019597\n",
      "Step:  567 Loss:  0.4597243\n",
      "Step:  568 Loss:  0.45880467\n",
      "Step:  569 Loss:  0.41478142\n",
      "Step:  570 Loss:  0.43531397\n",
      "Step:  571 Loss:  0.44436306\n",
      "Step:  572 Loss:  0.37982345\n",
      "Step:  573 Loss:  0.33522907\n",
      "Step:  574 Loss:  0.48489618\n",
      "Step:  575 Loss:  0.3952586\n",
      "Step:  576 Loss:  0.43046314\n",
      "Step:  577 Loss:  0.39188653\n",
      "Step:  578 Loss:  0.17066917\n",
      "Step:  579 Loss:  0.31084502\n",
      "Step:  580 Loss:  0.5776947\n",
      "Step:  581 Loss:  0.5797326\n",
      "Step:  582 Loss:  0.6087904\n",
      "Step:  583 Loss:  0.7187904\n",
      "Step:  584 Loss:  0.981887\n",
      "Step:  585 Loss:  1.0414062\n",
      "Step:  586 Loss:  1.2836959\n",
      "Step:  587 Loss:  1.5265199\n",
      "Step:  588 Loss:  1.3911362\n",
      "Step:  589 Loss:  1.6502023\n",
      "Step:  590 Loss:  2.0350113\n",
      "Step:  591 Loss:  2.2991066\n",
      "Step:  592 Loss:  1.7769682\n",
      "Step:  593 Loss:  1.4941674\n",
      "Step:  594 Loss:  0.88791025\n",
      "Step:  595 Loss:  0.089303516\n",
      "Step:  596 Loss:  0.101980306\n",
      "Step:  597 Loss:  0.115100995\n",
      "Step:  598 Loss:  0.12844613\n",
      "Step:  599 Loss:  0.14179054\n",
      "Step:  600 Loss:  0.15489596\n",
      "Step:  601 Loss:  0.1675427\n",
      "Step:  602 Loss:  0.17952107\n",
      "Step:  603 Loss:  0.19064005\n",
      "Step:  604 Loss:  0.20074302\n",
      "Step:  605 Loss:  0.20969857\n",
      "Step:  606 Loss:  0.21740709\n",
      "Step:  607 Loss:  0.22380292\n",
      "Step:  608 Loss:  0.2288537\n",
      "Step:  609 Loss:  0.5015204\n",
      "Step:  610 Loss:  0.77314514\n",
      "Step:  611 Loss:  1.1001198\n",
      "Step:  612 Loss:  1.5637232\n",
      "Step:  613 Loss:  1.6878409\n",
      "Step:  614 Loss:  1.9652228\n",
      "Step:  615 Loss:  2.2450461\n",
      "Step:  616 Loss:  2.5273488\n",
      "Step:  617 Loss:  2.2494068\n",
      "Step:  618 Loss:  1.7795968\n",
      "Step:  619 Loss:  1.1995966\n",
      "Step:  620 Loss:  0.49441868\n",
      "Step:  621 Loss:  0.32867554\n",
      "Step:  622 Loss:  0.3058219\n",
      "Step:  623 Loss:  0.2451244\n",
      "Step:  624 Loss:  0.29413462\n",
      "Step:  625 Loss:  0.2918467\n",
      "Step:  626 Loss:  0.22673304\n",
      "Step:  627 Loss:  0.20737731\n",
      "Step:  628 Loss:  0.102465376\n",
      "Step:  629 Loss:  0.1149299\n",
      "Step:  630 Loss:  0.06763822\n",
      "Step:  631 Loss:  0.07339217\n",
      "Step:  632 Loss:  0.0864002\n",
      "Step:  633 Loss:  0.07143037\n",
      "Step:  634 Loss:  0.0459573\n",
      "Step:  635 Loss:  0.09380081\n",
      "Step:  636 Loss:  0.08091186\n",
      "Step:  637 Loss:  0.0902058\n",
      "Step:  638 Loss:  0.088058315\n",
      "Step:  639 Loss:  0.12497927\n",
      "Step:  640 Loss:  0.13048202\n",
      "Step:  641 Loss:  0.20922898\n",
      "Step:  642 Loss:  0.22165678\n",
      "Step:  643 Loss:  0.2185198\n",
      "Step:  644 Loss:  0.22783162\n",
      "Step:  645 Loss:  0.20155576\n",
      "Step:  646 Loss:  0.31675228\n",
      "Step:  647 Loss:  0.26118886\n",
      "Step:  648 Loss:  0.3803267\n",
      "Step:  649 Loss:  0.41060877\n",
      "Step:  650 Loss:  0.4378097\n",
      "Step:  651 Loss:  0.5431775\n",
      "Step:  652 Loss:  0.5189525\n",
      "Step:  653 Loss:  0.44604522\n",
      "Step:  654 Loss:  0.39870363\n",
      "Step:  655 Loss:  0.38934758\n",
      "Step:  656 Loss:  0.37834978\n",
      "Step:  657 Loss:  148.60193\n",
      "Step:  658 Loss:  150.49031\n",
      "Step:  659 Loss:  719.0297\n",
      "Finished after Step:  659\n",
      "Step:  660 Loss:  717.92175\n",
      "Step:  661 Loss:  564.1058\n",
      "Step:  662 Loss:  551.682\n",
      "Step:  663 Loss:  0.3072557\n",
      "Step:  664 Loss:  0.26554704\n",
      "Step:  665 Loss:  0.32800007\n",
      "Step:  666 Loss:  0.36172807\n",
      "Step:  667 Loss:  0.40814915\n",
      "Step:  668 Loss:  0.5803704\n",
      "Step:  669 Loss:  0.6822603\n",
      "Step:  670 Loss:  0.7941798\n",
      "Step:  671 Loss:  0.93173754\n",
      "Step:  672 Loss:  1.1111128\n",
      "Step:  673 Loss:  1.3086\n",
      "Step:  674 Loss:  1.5253452\n",
      "Step:  675 Loss:  1.7403177\n",
      "Step:  676 Loss:  1.9134941\n",
      "Step:  677 Loss:  2.0864732\n",
      "Step:  678 Loss:  2.3255363\n",
      "Step:  679 Loss:  2.5801077\n",
      "Step:  680 Loss:  2.8918078\n",
      "Step:  681 Loss:  3.2345812\n",
      "Step:  682 Loss:  3.5055692\n",
      "Step:  683 Loss:  3.784132\n",
      "Step:  684 Loss:  4.0854015\n",
      "Step:  685 Loss:  4.3747287\n",
      "Step:  686 Loss:  4.7728386\n",
      "Step:  687 Loss:  5.0849824\n",
      "Step:  688 Loss:  5.3698378\n",
      "Step:  689 Loss:  5.83286\n",
      "Step:  690 Loss:  6.047392\n",
      "Step:  691 Loss:  6.3673296\n",
      "Step:  692 Loss:  6.896742\n",
      "Step:  693 Loss:  7.1176686\n",
      "Step:  694 Loss:  7.8118525\n",
      "Step:  695 Loss:  6.760515\n",
      "Step:  696 Loss:  6.917136\n",
      "Step:  697 Loss:  5.772897\n",
      "Step:  698 Loss:  5.670343\n",
      "Step:  699 Loss:  5.7322726\n",
      "Step:  700 Loss:  4.941784\n",
      "Step:  701 Loss:  6.804563\n",
      "Step:  702 Loss:  7.986952\n",
      "Step:  703 Loss:  9.595678\n",
      "Step:  704 Loss:  9.8124\n",
      "Step:  705 Loss:  8.989354\n",
      "Step:  706 Loss:  8.025913\n",
      "Step:  707 Loss:  7.8587236\n",
      "Step:  708 Loss:  8.341322\n",
      "Step:  709 Loss:  9.025155\n",
      "Step:  710 Loss:  9.918441\n",
      "Step:  711 Loss:  450.28516\n",
      "Finished after Step:  711\n",
      "Step:  712 Loss:  484.32697\n",
      "Step:  713 Loss:  504.3672\n",
      "Step:  714 Loss:  507.01498\n",
      "Step:  715 Loss:  0.42465192\n",
      "Step:  716 Loss:  0.39300895\n",
      "Step:  717 Loss:  0.26350105\n",
      "Step:  718 Loss:  0.029753687\n",
      "Step:  719 Loss:  0.023694586\n",
      "Step:  720 Loss:  0.02001518\n",
      "Step:  721 Loss:  0.018177453\n",
      "Step:  722 Loss:  0.16204797\n",
      "Step:  723 Loss:  0.29233426\n",
      "Step:  724 Loss:  0.3732732\n",
      "Step:  725 Loss:  0.47508422\n",
      "Step:  726 Loss:  0.388869\n",
      "Step:  727 Loss:  0.36558387\n",
      "Step:  728 Loss:  0.3337986\n",
      "Step:  729 Loss:  0.5082243\n",
      "Step:  730 Loss:  0.3755982\n",
      "Step:  731 Loss:  0.36937922\n",
      "Step:  732 Loss:  0.35880354\n",
      "Step:  733 Loss:  0.11921886\n",
      "Step:  734 Loss:  0.11468312\n",
      "Step:  735 Loss:  0.10908368\n",
      "Step:  736 Loss:  0.10848588\n",
      "Step:  737 Loss:  0.10093886\n",
      "Step:  738 Loss:  0.0928158\n",
      "Step:  739 Loss:  0.084338784\n",
      "Step:  740 Loss:  0.07048351\n",
      "Step:  741 Loss:  0.062223107\n",
      "Step:  742 Loss:  2.2724571\n",
      "Step:  743 Loss:  2.2307353\n",
      "Step:  744 Loss:  2.1601856\n",
      "Step:  745 Loss:  2.0614202\n",
      "Step:  746 Loss:  0.06705695\n",
      "Step:  747 Loss:  0.058329437\n",
      "Step:  748 Loss:  0.05006117\n",
      "Step:  749 Loss:  0.042356726\n",
      "Step:  750 Loss:  0.03530319\n",
      "Step:  751 Loss:  0.028972192\n",
      "Step:  752 Loss:  0.023421543\n",
      "Step:  753 Loss:  0.018691722\n",
      "Step:  754 Loss:  0.14802629\n",
      "Step:  755 Loss:  0.13803735\n",
      "Step:  756 Loss:  0.13105768\n",
      "Step:  757 Loss:  0.12751387\n",
      "Step:  758 Loss:  0.012120638\n",
      "Step:  759 Loss:  0.016417589\n",
      "Step:  760 Loss:  0.021079008\n",
      "Step:  761 Loss:  0.026116803\n",
      "Step:  762 Loss:  0.031446848\n",
      "Step:  763 Loss:  0.03689956\n",
      "Step:  764 Loss:  1.160713\n",
      "Step:  765 Loss:  2.9282675\n",
      "Step:  766 Loss:  3.5641305\n",
      "Step:  767 Loss:  3.4496608\n",
      "Step:  768 Loss:  2.729807\n",
      "Step:  769 Loss:  2.3033502\n",
      "Step:  770 Loss:  2.82474\n",
      "Step:  771 Loss:  3.2614994\n",
      "Step:  772 Loss:  2.660078\n",
      "Step:  773 Loss:  2.7944534\n",
      "Step:  774 Loss:  1.8158015\n",
      "Step:  775 Loss:  2.2118754\n",
      "Step:  776 Loss:  2.4801297\n",
      "Step:  777 Loss:  2.1231399\n",
      "Step:  778 Loss:  1.8570654\n",
      "Step:  779 Loss:  1.0034878\n",
      "Step:  780 Loss:  0.63564515\n",
      "Step:  781 Loss:  0.11557935\n",
      "Step:  782 Loss:  0.11582872\n",
      "Step:  783 Loss:  0.15193045\n",
      "Step:  784 Loss:  0.47373182\n",
      "Step:  785 Loss:  0.4968104\n",
      "Step:  786 Loss:  0.446698\n",
      "Step:  787 Loss:  0.50067544\n",
      "Step:  788 Loss:  9.145596\n",
      "Step:  789 Loss:  20.27635\n",
      "Step:  790 Loss:  21.447495\n",
      "Step:  791 Loss:  580.39667\n",
      "Finished after Step:  791\n",
      "Step:  792 Loss:  572.9139\n",
      "Step:  793 Loss:  548.95245\n",
      "Step:  794 Loss:  528.33307\n",
      "Step:  795 Loss:  1.555895\n",
      "Step:  796 Loss:  1.2085531\n",
      "Step:  797 Loss:  1.7442597\n",
      "Step:  798 Loss:  1.9365597\n",
      "Step:  799 Loss:  2.096507\n",
      "Step:  800 Loss:  2.5603628\n",
      "Step:  801 Loss:  2.8053794\n",
      "Step:  802 Loss:  3.5494838\n",
      "Step:  803 Loss:  3.3683882\n",
      "Step:  804 Loss:  4.044713\n",
      "Step:  805 Loss:  4.098585\n",
      "Step:  806 Loss:  4.487321\n",
      "Step:  807 Loss:  6.024456\n",
      "Step:  808 Loss:  5.754788\n",
      "Step:  809 Loss:  5.0740757\n",
      "Step:  810 Loss:  3.626429\n",
      "Step:  811 Loss:  1.9499757\n",
      "Step:  812 Loss:  0.96938014\n",
      "Step:  813 Loss:  0.5757401\n",
      "Step:  814 Loss:  0.84859955\n",
      "Step:  815 Loss:  1.102778\n",
      "Step:  816 Loss:  1.3637314\n",
      "Step:  817 Loss:  1.7350608\n",
      "Step:  818 Loss:  2.0376117\n",
      "Step:  819 Loss:  2.3366232\n",
      "Step:  820 Loss:  2.7403698\n",
      "Step:  821 Loss:  3.0278363\n",
      "Step:  822 Loss:  2.4401083\n",
      "Step:  823 Loss:  1.7701654\n",
      "Step:  824 Loss:  0.93417907\n",
      "Step:  825 Loss:  0.115268014\n",
      "Step:  826 Loss:  0.101426795\n",
      "Step:  827 Loss:  0.14113379\n",
      "Step:  828 Loss:  0.043142624\n",
      "Step:  829 Loss:  0.07261088\n",
      "Step:  830 Loss:  0.1739921\n",
      "Step:  831 Loss:  0.26350433\n",
      "Step:  832 Loss:  0.43948802\n",
      "Step:  833 Loss:  0.62526035\n",
      "Step:  834 Loss:  0.78299385\n",
      "Step:  835 Loss:  1.0907934\n",
      "Step:  836 Loss:  1.044074\n",
      "Step:  837 Loss:  1.0084229\n",
      "Step:  838 Loss:  1.2894135\n",
      "Step:  839 Loss:  1.6966255\n",
      "Step:  840 Loss:  2.440237\n",
      "Step:  841 Loss:  2.552341\n",
      "Step:  842 Loss:  3.0946712\n",
      "Step:  843 Loss:  3.642897\n",
      "Step:  844 Loss:  4.1978106\n",
      "Step:  845 Loss:  6.1891136\n",
      "Step:  846 Loss:  7.347799\n",
      "Step:  847 Loss:  8.472532\n",
      "Step:  848 Loss:  9.815596\n",
      "Step:  849 Loss:  10.649654\n",
      "Step:  850 Loss:  11.021673\n",
      "Step:  851 Loss:  11.471046\n",
      "Step:  852 Loss:  11.887474\n",
      "Step:  853 Loss:  9.200826\n",
      "Step:  854 Loss:  9.245777\n",
      "Step:  855 Loss:  9.248978\n",
      "Step:  856 Loss:  8.611229\n",
      "Step:  857 Loss:  10.50523\n",
      "Step:  858 Loss:  9.773633\n",
      "Step:  859 Loss:  8.698444\n",
      "Step:  860 Loss:  7.9125977\n",
      "Step:  861 Loss:  6.960249\n",
      "Step:  862 Loss:  6.12717\n",
      "Step:  863 Loss:  5.3079515\n",
      "Step:  864 Loss:  4.538728\n",
      "Step:  865 Loss:  3.8599052\n",
      "Step:  866 Loss:  3.1304283\n",
      "Step:  867 Loss:  2.6959608\n",
      "Step:  868 Loss:  2.255244\n",
      "Step:  869 Loss:  1.9117234\n",
      "Step:  870 Loss:  2.6973877\n",
      "Step:  871 Loss:  3.4858603\n",
      "Step:  872 Loss:  4.762476\n",
      "Step:  873 Loss:  5.3993764\n",
      "Step:  874 Loss:  6.0828485\n",
      "Step:  875 Loss:  7.9019217\n",
      "Step:  876 Loss:  6.333522\n",
      "Step:  877 Loss:  5.346518\n",
      "Step:  878 Loss:  3.3144822\n",
      "Step:  879 Loss:  0.24577636\n",
      "Step:  880 Loss:  0.12215545\n",
      "Step:  881 Loss:  0.08273088\n",
      "Step:  882 Loss:  0.06874503\n",
      "Step:  883 Loss:  0.069439456\n",
      "Step:  884 Loss:  0.08708055\n",
      "Step:  885 Loss:  0.09115961\n",
      "Step:  886 Loss:  0.13353853\n",
      "Step:  887 Loss:  0.18962029\n",
      "Step:  888 Loss:  0.30721104\n",
      "Step:  889 Loss:  0.45645106\n",
      "Step:  890 Loss:  0.52307284\n",
      "Step:  891 Loss:  0.5366623\n",
      "Step:  892 Loss:  0.78016686\n",
      "Step:  893 Loss:  1.028583\n",
      "Step:  894 Loss:  1.2309618\n",
      "Step:  895 Loss:  1.577382\n",
      "Step:  896 Loss:  1.8932403\n",
      "Step:  897 Loss:  2.2775178\n",
      "Step:  898 Loss:  2.678384\n",
      "Step:  899 Loss:  3.0692635\n",
      "Step:  900 Loss:  3.4517245\n",
      "Step:  901 Loss:  3.836568\n",
      "Step:  902 Loss:  4.2354565\n",
      "Step:  903 Loss:  4.71041\n",
      "Step:  904 Loss:  5.136592\n",
      "Step:  905 Loss:  5.552158\n",
      "Step:  906 Loss:  5.983968\n",
      "Step:  907 Loss:  6.2958336\n",
      "Step:  908 Loss:  6.7202744\n",
      "Step:  909 Loss:  7.0236387\n",
      "Step:  910 Loss:  7.180665\n",
      "Step:  911 Loss:  7.2461104\n",
      "Step:  912 Loss:  6.914913\n",
      "Step:  913 Loss:  6.333482\n",
      "Step:  914 Loss:  5.5376883\n",
      "Step:  915 Loss:  478.35086\n",
      "Finished after Step:  915\n",
      "Step:  916 Loss:  482.87268\n",
      "Step:  917 Loss:  471.35696\n",
      "Step:  918 Loss:  442.9316\n",
      "Step:  919 Loss:  1.2438679\n",
      "Step:  920 Loss:  1.3136978\n",
      "Step:  921 Loss:  1.4135779\n",
      "Step:  922 Loss:  1.4865841\n",
      "Step:  923 Loss:  1.1421589\n",
      "Step:  924 Loss:  1.3708103\n",
      "Step:  925 Loss:  1.0834682\n",
      "Step:  926 Loss:  1.1411583\n",
      "Step:  927 Loss:  1.5788237\n",
      "Step:  928 Loss:  1.6252022\n",
      "Step:  929 Loss:  1.5201681\n",
      "Step:  930 Loss:  1.5703621\n",
      "Step:  931 Loss:  1.6997521\n",
      "Step:  932 Loss:  1.9166113\n",
      "Step:  933 Loss:  2.4199896\n",
      "Step:  934 Loss:  2.7020924\n",
      "Step:  935 Loss:  2.9123898\n",
      "Step:  936 Loss:  3.1205459\n",
      "Step:  937 Loss:  2.784841\n",
      "Step:  938 Loss:  2.1763096\n",
      "Step:  939 Loss:  1.6747893\n",
      "Step:  940 Loss:  1.0869817\n",
      "Step:  941 Loss:  0.8110857\n",
      "Step:  942 Loss:  0.67526853\n",
      "Step:  943 Loss:  0.73339385\n",
      "Step:  944 Loss:  0.69254863\n",
      "Step:  945 Loss:  0.7686249\n",
      "Step:  946 Loss:  0.90338844\n",
      "Step:  947 Loss:  0.7108666\n",
      "Step:  948 Loss:  0.65909195\n",
      "Step:  949 Loss:  0.92223716\n",
      "Step:  950 Loss:  1.1892254\n",
      "Step:  951 Loss:  1.4954988\n",
      "Step:  952 Loss:  2.0008693\n",
      "Step:  953 Loss:  2.2651825\n",
      "Step:  954 Loss:  2.5955796\n",
      "Step:  955 Loss:  2.963537\n",
      "Step:  956 Loss:  3.1219733\n",
      "Step:  957 Loss:  3.2292867\n",
      "Step:  958 Loss:  3.5147233\n",
      "Step:  959 Loss:  3.5751815\n",
      "Step:  960 Loss:  3.620813\n",
      "Step:  961 Loss:  3.8426306\n",
      "Step:  962 Loss:  4.5452266\n",
      "Step:  963 Loss:  4.6263866\n",
      "Step:  964 Loss:  4.833872\n",
      "Step:  965 Loss:  3.9128885\n",
      "Step:  966 Loss:  2.588896\n",
      "Step:  967 Loss:  1.9812568\n",
      "Step:  968 Loss:  1.4454958\n",
      "Step:  969 Loss:  1.7814467\n",
      "Step:  970 Loss:  1.5319066\n",
      "Step:  971 Loss:  1.9843403\n",
      "Step:  972 Loss:  2.1320252\n",
      "Step:  973 Loss:  2.5416057\n",
      "Step:  974 Loss:  4.072846\n",
      "Step:  975 Loss:  5.538322\n",
      "Step:  976 Loss:  8.049275\n",
      "Step:  977 Loss:  240.87677\n",
      "Finished after Step:  977\n",
      "Step:  978 Loss:  315.21497\n",
      "Step:  979 Loss:  347.57614\n",
      "Step:  980 Loss:  339.0907\n",
      "Step:  981 Loss:  1.5734996\n",
      "Step:  982 Loss:  0.380378\n",
      "Step:  983 Loss:  0.52302396\n",
      "Step:  984 Loss:  0.757868\n",
      "Step:  985 Loss:  0.8979978\n",
      "Step:  986 Loss:  1.0684074\n",
      "Step:  987 Loss:  1.137387\n",
      "Step:  988 Loss:  1.2533392\n",
      "Step:  989 Loss:  1.4142929\n",
      "Step:  990 Loss:  1.5900855\n",
      "Step:  991 Loss:  1.7858377\n",
      "Step:  992 Loss:  1.6573029\n",
      "Step:  993 Loss:  1.4415739\n",
      "Step:  994 Loss:  1.0906363\n",
      "Step:  995 Loss:  0.92831486\n",
      "Step:  996 Loss:  0.75233346\n",
      "Step:  997 Loss:  0.6034279\n",
      "Step:  998 Loss:  0.5927776\n",
      "Step:  999 Loss:  0.5115957\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.95\n",
    "\n",
    "def choose_action(model, state):\n",
    "    random_action = lambda: np.random.randint(N_ACTIONS)\n",
    "    \n",
    "    def predicted_action():\n",
    "        q_values = model(tf.expand_dims(state, 0))\n",
    "        action = tf.squeeze(tf.argmax(q_values, axis=1))\n",
    "        return action.numpy()\n",
    "\n",
    "    # Choose between the greey move or the random move.\n",
    "    # Greedy move take the greedy approach to chose the best current action.\n",
    "    epsilon = np.random.rand()\n",
    "    action =  random_action() if epsilon <= EXPLORATION_RATE else predicted_action()\n",
    "\n",
    "    return action\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    state = np.squeeze(obs)\n",
    "    return state\n",
    "\n",
    "def play(model, state):\n",
    "    return choose_action(model, state)\n",
    "\n",
    "def capped_append(memory, data, max_size=16):\n",
    "    memory.append(data)\n",
    "    if len(memory) > max_size: memory.pop(0)\n",
    "\n",
    "def batch_train(step_id, model, memory, bs=4):\n",
    "    memory_batch = memory[-bs:]\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for state, action, reward, next_state, done in memory_batch:\n",
    "        reward += 0 if done else GAMMA * np.amax(model.predict(tf.expand_dims(next_state, 0))[0])\n",
    "\n",
    "        target = model.predict(tf.expand_dims(state, 0))\n",
    "        target[0][action] = reward\n",
    "\n",
    "        X.append(state)\n",
    "        Y.append(target)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(tf.stack(X))\n",
    "        loss = tf.reduce_mean(tf.square(q_values - tf.stack(Y)))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        print('Step: ', step_id, 'Loss: ', loss.numpy())\n",
    "    \n",
    "obs = env.reset()\n",
    "memory = []\n",
    "images = []\n",
    "\n",
    "for step_id in range(1000):\n",
    "    state = obs_to_state(obs)\n",
    "    action = play(model, state)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    capped_append(memory, (state, action, reward, obs_to_state(obs), done))\n",
    "    \n",
    "    # Render the env\n",
    "    image = env.render(mode='rgb_array')\n",
    "    images.append(image)\n",
    "\n",
    "    batch_train(step_id, model, memory)\n",
    "\n",
    "    if done:\n",
    "        print('Finished after Step: ', step_id)\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "pil_image = Image.fromarray(np.copy(images[0]))\n",
    "\n",
    "pil_image.save(\n",
    "    'pillow_imagedraw.gif',\n",
    "    append_images=list(map(lambda im: Image.fromarray(im), images[1:])),\n",
    "    save_all=True, duration=5, loop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef4ac4ea1ec422be6b4eb59e3fa0ded4ce016edaf83e8378f1dbc473945965d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
