{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Props(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Props, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "    \n",
    "    def __getattribute__(self, name):\n",
    "        try:\n",
    "            return super(Props, self).__getattribute__(name)\n",
    "        except AttributeError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "datapath = Path('./input.txt')\n",
    "\n",
    "with open(datapath) as f:\n",
    "    tiny_shakespere = list(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen', ':', '\\n', 'before']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_word_1gram_frequencies(data, tokenizer):\n",
    "    words = map(lambda x:x.lower_, itertools.chain(*map(tokenizer, data)))\n",
    "    return Counter(words)\n",
    "\n",
    "word_tokenizer = spacy.blank(\"en\")\n",
    "word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "\n",
    "print(\n",
    "    f'{list(word_vocab.keys())[:5]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = nlp('and to')\n",
    "# ! pip install levenshtein\n",
    "from functools import reduce\n",
    "\n",
    "from Levenshtein import distance as edit_distance\n",
    "\n",
    "def get_proximity_with_vocab(text, tokenizer, vocab):\n",
    "    def proximity_fn(word):\n",
    "        if word in vocab:\n",
    "            distance = 0\n",
    "        else:\n",
    "            distance = min(*map(\n",
    "                lambda vocab_word: edit_distance(word, vocab_word),\n",
    "                vocab.keys()\n",
    "            ))\n",
    "        \n",
    "        return distance\n",
    "\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    distance = reduce(lambda y,token: y+proximity_fn(str(token)), tokens, 0)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def get_all_proximity_with_vocab(texts, tokenizer, vocab):\n",
    "    return np.fromiter(\n",
    "        map(\n",
    "            lambda text: get_proximity_with_vocab(text, tokenizer, vocab),\n",
    "            texts,\n",
    "        ),\n",
    "        dtype=object\n",
    "    )\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "texts = [\n",
    "    'hello, how are you doing?',\n",
    "    'hello, how are you doing?',\n",
    "]\n",
    "\n",
    "get_all_proximity_with_vocab(texts, nlp, word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n"
     ]
    }
   ],
   "source": [
    "! head input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ''.join(tiny_shakespere)\n",
    "\n",
    "all_data_length = len(all_data)\n",
    "train_data_length = int(all_data_length*.9)\n",
    "valid_data_length = all_data_length - train_data_length\n",
    "\n",
    "train_data = all_data[:train_data_length]\n",
    "valid_data = all_data[-valid_data_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(train_data)=1003854 train_data_vectorized.shape=(1003854,)\n",
      "len(valid_data)=111540 valid_data_vectorized.shape=(111540,)\n",
      "\n",
      "train_data[:log_size]='First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\n'\n",
      "train_data_vectorized[:log_size]=array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
      "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
      "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
      "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
      "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
      "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39,\n",
      "       56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56,\n",
      "       39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,\n",
      "        1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50, 10,\n",
      "        0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50, 60,\n",
      "       43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n",
      "       52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59,  1, 49, 52, 53,\n",
      "       61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41, 47, 59, 57,  1, 47,\n",
      "       57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,  1, 58, 53,  1,\n",
      "       58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13, 50, 50, 10,\n",
      "        0])\n",
      "\n",
      "valid_data[:log_size]=\"?\\n\\nGREMIO:\\nGood morrow, neighbour Baptista.\\n\\nBAPTISTA:\\nGood morrow, neighbour Gremio.\\nGod save you, gentlemen!\\n\\nPETRUCHIO:\\nAnd you, good sir! Pray, have you not a daughter\\nCall'd Katharina, fair and virtuous?\\n\\nBAPTISTA:\\nI have a daughter, sir, called Katha\"\n",
      "valid_data_vectorized[:log_size]=array([12,  0,  0, 19, 30, 17, 25, 21, 27, 10,  0, 19, 53, 53, 42,  1, 51,\n",
      "       53, 56, 56, 53, 61,  6,  1, 52, 43, 47, 45, 46, 40, 53, 59, 56,  1,\n",
      "       14, 39, 54, 58, 47, 57, 58, 39,  8,  0,  0, 14, 13, 28, 32, 21, 31,\n",
      "       32, 13, 10,  0, 19, 53, 53, 42,  1, 51, 53, 56, 56, 53, 61,  6,  1,\n",
      "       52, 43, 47, 45, 46, 40, 53, 59, 56,  1, 19, 56, 43, 51, 47, 53,  8,\n",
      "        0, 19, 53, 42,  1, 57, 39, 60, 43,  1, 63, 53, 59,  6,  1, 45, 43,\n",
      "       52, 58, 50, 43, 51, 43, 52,  2,  0,  0, 28, 17, 32, 30, 33, 15, 20,\n",
      "       21, 27, 10,  0, 13, 52, 42,  1, 63, 53, 59,  6,  1, 45, 53, 53, 42,\n",
      "        1, 57, 47, 56,  2,  1, 28, 56, 39, 63,  6,  1, 46, 39, 60, 43,  1,\n",
      "       63, 53, 59,  1, 52, 53, 58,  1, 39,  1, 42, 39, 59, 45, 46, 58, 43,\n",
      "       56,  0, 15, 39, 50, 50,  5, 42,  1, 23, 39, 58, 46, 39, 56, 47, 52,\n",
      "       39,  6,  1, 44, 39, 47, 56,  1, 39, 52, 42,  1, 60, 47, 56, 58, 59,\n",
      "       53, 59, 57, 12,  0,  0, 14, 13, 28, 32, 21, 31, 32, 13, 10,  0, 21,\n",
      "        1, 46, 39, 60, 43,  1, 39,  1, 42, 39, 59, 45, 46, 58, 43, 56,  6,\n",
      "        1, 57, 47, 56,  6,  1, 41, 39, 50, 50, 43, 42,  1, 23, 39, 58, 46,\n",
      "       39])\n"
     ]
    }
   ],
   "source": [
    "def get_vectorizer(config):\n",
    "    c_idx_mapping = dict(map(\n",
    "        lambda item: (item[1], item[0]),\n",
    "        enumerate(config.vocab)\n",
    "    ))\n",
    "    encoder = lambda text: np.fromiter(map(c_idx_mapping.get, text), dtype=int)\n",
    "    decoder = lambda ids: ''.join([config.vocab[idx] for idx in ids])\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "config = Props(\n",
    "    valid_split=0.1,\n",
    "    vocab=\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\",\n",
    "    model=Props(block_size=256),\n",
    ")\n",
    "\n",
    "encoder, decoder = get_vectorizer(config)\n",
    "\n",
    "train_data_vectorized = encoder(train_data)\n",
    "valid_data_vectorized = encoder(valid_data)\n",
    "log_size = config.model.block_size\n",
    "\n",
    "print(\n",
    "    f'\\n{len(train_data)=} {train_data_vectorized.shape=}'\n",
    "    f'\\n{len(valid_data)=} {valid_data_vectorized.shape=}'\n",
    ")\n",
    "print(\n",
    "    f'\\n{train_data[:log_size]=}\\n{train_data_vectorized[:log_size]=}'\n",
    "    f'\\n\\n{valid_data[:log_size]=}\\n{valid_data_vectorized[:log_size]=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 08:27:05.041971: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-05-02 08:27:05.041997: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-05-02 08:27:05.042004: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-05-02 08:27:05.042068: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-02 08:27:05.042115: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-05-02 08:27:15.248655: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 618993 of 1003597\n",
      "2024-05-02 08:27:21.975306: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Set\n",
      "==============\n",
      "X.shape=TensorShape([2, 256]) y.shape=TensorShape([2, 256])\n",
      "X.numpy()=array([[50, 50,  1, 57, 47, 52, 45,  8,  0,  0, 29, 33, 17, 17, 26, 10,\n",
      "         0,  5, 32, 47, 57,  1, 61, 43, 50, 50,  1, 58, 46, 39, 58,  1,\n",
      "        58, 46, 53, 59,  1, 46, 39, 57, 58,  1, 41, 39, 59, 57, 43,  0,\n",
      "        14, 59, 58,  1, 58, 46, 53, 59,  1, 57, 46, 53, 59, 50, 42, 57,\n",
      "        58,  1, 54, 50, 43, 39, 57, 43,  1, 51, 43,  1, 40, 43, 58, 58,\n",
      "        43, 56,  6,  1, 61, 53, 59, 50, 42, 57, 58,  1, 58, 46, 53, 59,\n",
      "         1, 61, 43, 43, 54,  8,  0,  0, 24, 39, 42, 63, 10,  0, 21,  1,\n",
      "        41, 53, 59, 50, 42,  1, 61, 43, 43, 54,  6,  1, 51, 39, 42, 39,\n",
      "        51,  6,  1, 61, 53, 59, 50, 42,  1, 47, 58,  1, 42, 53,  1, 63,\n",
      "        53, 59,  1, 45, 53, 53, 42,  8,  0,  0, 29, 33, 17, 17, 26, 10,\n",
      "         0, 13, 52, 42,  1, 21,  1, 41, 53, 59, 50, 42,  1, 57, 47, 52,\n",
      "        45,  6,  1, 61, 53, 59, 50, 42,  1, 61, 43, 43, 54, 47, 52, 45,\n",
      "         1, 42, 53,  1, 51, 43,  1, 45, 53, 53, 42,  6,  0, 13, 52, 42,\n",
      "         1, 52, 43, 60, 43, 56,  1, 40, 53, 56, 56, 53, 61,  1, 39, 52,\n",
      "        63,  1, 58, 43, 39, 56,  1, 53, 44,  1, 58, 46, 43, 43,  8,  0,\n",
      "        14, 59, 58,  1, 57, 58, 39, 63,  6,  1, 46, 43, 56, 43,  1, 41],\n",
      "       [24, 21, 26, 19, 14, 30, 27, 23, 17, 10,  0, 25, 63,  1, 45, 56,\n",
      "        39, 41, 47, 53, 59, 57,  1, 59, 52, 41, 50, 43,  6,  1, 50, 43,\n",
      "        58,  1, 51, 43,  1, 49, 52, 53, 61,  1, 51, 63,  1, 44, 39, 59,\n",
      "        50, 58, 10,  0, 27, 52,  1, 61, 46, 39, 58,  1, 41, 53, 52, 42,\n",
      "        47, 58, 47, 53, 52,  1, 57, 58, 39, 52, 42, 57,  1, 47, 58,  1,\n",
      "        39, 52, 42,  1, 61, 46, 43, 56, 43, 47, 52, 12,  0,  0, 16, 33,\n",
      "        23, 17,  1, 27, 18,  1, 37, 27, 30, 23, 10,  0, 17, 60, 43, 52,\n",
      "         1, 47, 52,  1, 41, 53, 52, 42, 47, 58, 47, 53, 52,  1, 53, 44,\n",
      "         1, 58, 46, 43,  1, 61, 53, 56, 57, 58,  1, 42, 43, 45, 56, 43,\n",
      "        43,  6,  0, 21, 52,  1, 45, 56, 53, 57, 57,  1, 56, 43, 40, 43,\n",
      "        50, 50, 47, 53, 52,  1, 39, 52, 42,  1, 42, 43, 58, 43, 57, 58,\n",
      "        43, 42,  1, 58, 56, 43, 39, 57, 53, 52, 10,  0, 32, 46, 53, 59,\n",
      "         1, 39, 56, 58,  1, 39,  1, 40, 39, 52, 47, 57, 46,  5, 42,  1,\n",
      "        51, 39, 52,  6,  1, 39, 52, 42,  1, 46, 43, 56, 43,  1, 39, 56,\n",
      "        58,  1, 41, 53, 51, 43,  0, 14, 43, 44, 53, 56, 43,  1, 58, 46,\n",
      "        43,  1, 43, 62, 54, 47, 56, 39, 58, 47, 53, 52,  1, 53, 44,  1]])\n",
      "y.numpy()=array([[50,  1, 57, 47, 52, 45,  8,  0,  0, 29, 33, 17, 17, 26, 10,  0,\n",
      "         5, 32, 47, 57,  1, 61, 43, 50, 50,  1, 58, 46, 39, 58,  1, 58,\n",
      "        46, 53, 59,  1, 46, 39, 57, 58,  1, 41, 39, 59, 57, 43,  0, 14,\n",
      "        59, 58,  1, 58, 46, 53, 59,  1, 57, 46, 53, 59, 50, 42, 57, 58,\n",
      "         1, 54, 50, 43, 39, 57, 43,  1, 51, 43,  1, 40, 43, 58, 58, 43,\n",
      "        56,  6,  1, 61, 53, 59, 50, 42, 57, 58,  1, 58, 46, 53, 59,  1,\n",
      "        61, 43, 43, 54,  8,  0,  0, 24, 39, 42, 63, 10,  0, 21,  1, 41,\n",
      "        53, 59, 50, 42,  1, 61, 43, 43, 54,  6,  1, 51, 39, 42, 39, 51,\n",
      "         6,  1, 61, 53, 59, 50, 42,  1, 47, 58,  1, 42, 53,  1, 63, 53,\n",
      "        59,  1, 45, 53, 53, 42,  8,  0,  0, 29, 33, 17, 17, 26, 10,  0,\n",
      "        13, 52, 42,  1, 21,  1, 41, 53, 59, 50, 42,  1, 57, 47, 52, 45,\n",
      "         6,  1, 61, 53, 59, 50, 42,  1, 61, 43, 43, 54, 47, 52, 45,  1,\n",
      "        42, 53,  1, 51, 43,  1, 45, 53, 53, 42,  6,  0, 13, 52, 42,  1,\n",
      "        52, 43, 60, 43, 56,  1, 40, 53, 56, 56, 53, 61,  1, 39, 52, 63,\n",
      "         1, 58, 43, 39, 56,  1, 53, 44,  1, 58, 46, 43, 43,  8,  0, 14,\n",
      "        59, 58,  1, 57, 58, 39, 63,  6,  1, 46, 43, 56, 43,  1, 41, 53],\n",
      "       [21, 26, 19, 14, 30, 27, 23, 17, 10,  0, 25, 63,  1, 45, 56, 39,\n",
      "        41, 47, 53, 59, 57,  1, 59, 52, 41, 50, 43,  6,  1, 50, 43, 58,\n",
      "         1, 51, 43,  1, 49, 52, 53, 61,  1, 51, 63,  1, 44, 39, 59, 50,\n",
      "        58, 10,  0, 27, 52,  1, 61, 46, 39, 58,  1, 41, 53, 52, 42, 47,\n",
      "        58, 47, 53, 52,  1, 57, 58, 39, 52, 42, 57,  1, 47, 58,  1, 39,\n",
      "        52, 42,  1, 61, 46, 43, 56, 43, 47, 52, 12,  0,  0, 16, 33, 23,\n",
      "        17,  1, 27, 18,  1, 37, 27, 30, 23, 10,  0, 17, 60, 43, 52,  1,\n",
      "        47, 52,  1, 41, 53, 52, 42, 47, 58, 47, 53, 52,  1, 53, 44,  1,\n",
      "        58, 46, 43,  1, 61, 53, 56, 57, 58,  1, 42, 43, 45, 56, 43, 43,\n",
      "         6,  0, 21, 52,  1, 45, 56, 53, 57, 57,  1, 56, 43, 40, 43, 50,\n",
      "        50, 47, 53, 52,  1, 39, 52, 42,  1, 42, 43, 58, 43, 57, 58, 43,\n",
      "        42,  1, 58, 56, 43, 39, 57, 53, 52, 10,  0, 32, 46, 53, 59,  1,\n",
      "        39, 56, 58,  1, 39,  1, 40, 39, 52, 47, 57, 46,  5, 42,  1, 51,\n",
      "        39, 52,  6,  1, 39, 52, 42,  1, 46, 43, 56, 43,  1, 39, 56, 58,\n",
      "         1, 41, 53, 51, 43,  0, 14, 43, 44, 53, 56, 43,  1, 58, 46, 43,\n",
      "         1, 43, 62, 54, 47, 56, 39, 58, 47, 53, 52,  1, 53, 44,  1, 58]])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def make_dataset(data, config):\n",
    "    t_data = tf.constant(data)\n",
    "    offsets = tf.range(config.model.block_size + 1, dtype=tf.int64)\n",
    "\n",
    "    @tf.function\n",
    "    def to_item(idx):\n",
    "        item = tf.gather(t_data, offsets + idx)\n",
    "        return item[:-1], item[1:]\n",
    "\n",
    "    return tf.data.Dataset.range(data.shape[0] - config.model.block_size - 1).map(\n",
    "        to_item\n",
    "    )\n",
    "\n",
    "ds = make_dataset(train_data_vectorized, config).shuffle(\n",
    "    train_data_vectorized.shape[0] - config.model.block_size - 1,\n",
    ").batch(2)\n",
    "X, y = next(iter(ds))\n",
    "\n",
    "print(\n",
    "    f'\\n\\nTrain Set\\n=============='\n",
    "    f'\\n{X.shape=} {y.shape=}'\n",
    "    f'\\n{X.numpy()=}\\n{y.numpy()=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config={'dataset_size': None, 'valid_split': 0.1, 'model': {'layers': 6, 'heads': 6, 'dims': 384, 'dropout': 0.2, 'block_size': 256, 'bias': False, 'vocab_size': 65, 'pos_encoding': 'rope', 'use_cache': True}, 'vocab': \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\", 'train': {'epochs': 1, 'log_steps': 200, 'max_steps': 600, 'batch_size': 64}, 'lr': 0.001, 'beta1': 0.9, 'beta2': 0.99}\n"
     ]
    }
   ],
   "source": [
    "# config = Props(\n",
    "#     valid_split=0.1,\n",
    "#     vocab=\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\",\n",
    "#     model=Props(\n",
    "#         layers=2,\n",
    "#         heads=2,\n",
    "#         dims=8,\n",
    "#         dropout=0.2,\n",
    "#         block_size=4,\n",
    "#         bias=False,\n",
    "#         vocab_size=65,\n",
    "#         pos_encoding='rope',\n",
    "#         use_cache=True,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# Configuration for NanoGPT is sourced from\n",
    "# https://github.com/karpathy/nanoGPT/blob/master/config/train_shakespeare_char.py\n",
    "\n",
    "def is_interactive():\n",
    "    return False\n",
    "\n",
    "config = Props(\n",
    "    # Dataset\n",
    "    dataset_size=300 if is_interactive()  else None,\n",
    "    valid_split=0.1,\n",
    "\n",
    "    # Model\n",
    "    model=Props(\n",
    "        layers=6,\n",
    "        heads=6,\n",
    "        dims=384,\n",
    "        dropout=0.2,\n",
    "        block_size=32 if is_interactive() else 256,\n",
    "        bias=False,\n",
    "        vocab_size=65,\n",
    "        pos_encoding='rope',\n",
    "        use_cache=True,\n",
    "    ),\n",
    "    \n",
    "    # Vocab\n",
    "    vocab=\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\",\n",
    "\n",
    "    # Training\n",
    "    train=Props(\n",
    "        epochs=1,\n",
    "        log_steps=1 if is_interactive() else 200,\n",
    "#         max_steps=2 if is_interactive() else 5000,\n",
    "        max_steps=2 if is_interactive() else 600,\n",
    "        batch_size=2 if is_interactive() else 64,\n",
    "    ),\n",
    "    \n",
    "    # Optimizer\n",
    "    lr=1e-3,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'{config=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, initializers\n",
    "\n",
    "def dense_init(config, size, activation=None, kernel_initializer=None):\n",
    "    kernel_initializer = kernel_initializer or initializers.RandomNormal(\n",
    "        mean=0.,\n",
    "        stddev=0.02,\n",
    "    )\n",
    "    return layers.Dense(\n",
    "        size,\n",
    "        use_bias=config.bias,\n",
    "        activation=activation,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        bias_initializer=initializers.Zeros(),\n",
    "    )\n",
    "\n",
    "def embed_init(config, size):\n",
    "    return layers.Embedding(\n",
    "        size,\n",
    "        config.dims,\n",
    "        embeddings_initializer=initializers.RandomNormal(\n",
    "            mean=0.,\n",
    "            stddev=0.02,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RotaryPositionalEncodings(layers.Layer):\n",
    "    def __init__(self, block_size, dims, base=10000.):\n",
    "        super(RotaryPositionalEncodings, self).__init__()\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.base = base\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.theta = 1. / (self.base ** (tf.range(0, self.dims, 2, dtype=tf.float32) / self.dims))\n",
    "        self.positions = tf.range(self.block_size, dtype=tf.float32)\n",
    "        self.mtheta = self.positions[..., None]*self.theta[None, ...]\n",
    "        self.mtheta_paired = tf.concat([self.mtheta]*2, axis=-1)\n",
    "\n",
    "        self.cos_mtheta = tf.math.cos(self.mtheta_paired)\n",
    "        self.sin_mtheta = tf.math.sin(self.mtheta_paired)\n",
    "\n",
    "    def call(self, x, index=0):\n",
    "        shape = tf.shape(x)\n",
    "        T = shape[-2]\n",
    "\n",
    "        x_real = x*self.cos_mtheta[index:index+T, ...]\n",
    "\n",
    "        x_img = tf.concat(\n",
    "            [-x[..., self.dims//2:], x[..., :self.dims//2]],\n",
    "            axis=-1\n",
    "        )*self.sin_mtheta[index:index+T, ...]\n",
    "\n",
    "        output = x_real + x_img\n",
    "        return output\n",
    "\n",
    "# B, H, T, D = 2, 4, 8, 4\n",
    "# B_i = 0\n",
    "# x = tf.random.uniform((B, H, T, D))\n",
    "# l = RotaryPositionalEncodings(64, D)\n",
    "# output = l(x)\n",
    "# # tf.print(f'{l.theta=} {l.theta.shape}')\n",
    "# # tf.print(f'{l.positions=} {l.positions.shape}')\n",
    "# # tf.print(f'{x[B_i]=}')\n",
    "# # tf.print(f'{l.cos_mtheta=}')\n",
    "# # tf.print(f'{l.sin_mtheta=}')\n",
    "# tf.print(f'{output[B_i]=}\\n{output[B_i]=} {output.shape}')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dims = config.dims\n",
    "        self.head_size = config.dims // config.heads\n",
    "        self.heads = config.heads\n",
    "        \n",
    "        # Key, query and value projection layers\n",
    "        self.key = dense_init(config, config.dims)\n",
    "        self.query = dense_init(config, config.dims)\n",
    "        self.value = dense_init(config, config.dims)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.attn_dropout = layers.Dropout(config.dropout)\n",
    "        self.residual_dropout = layers.Dropout(config.dropout)\n",
    "        \n",
    "        # RoPE\n",
    "        if config.pos_encoding == 'rope':\n",
    "            self.rope = RotaryPositionalEncodings(config.block_size, self.head_size)\n",
    "    \n",
    "    def update_cache(self, k, v, cache, token_axis=1):\n",
    "        def glue_fn(x, cache_x):\n",
    "            # Glue the cached sequence and the input on token dimension.\n",
    "            return tf.concat([cache_x, x], axis=token_axis)\n",
    "\n",
    "        # 1. Compose full key and value sequences\n",
    "        k, v = glue_fn(k, cache['key']), glue_fn(v, cache['value'])\n",
    "\n",
    "        # 2. Update cache\n",
    "        cache['key'] = k\n",
    "        cache['value'] = v\n",
    "\n",
    "        return k, v, cache\n",
    "\n",
    "    def call(self, x, training=None, cache=None):\n",
    "        shape = tf.shape(x)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "        C = shape[2]\n",
    "        \n",
    "        # 1. Compute keys, queries and values for all heads\n",
    "        # (B, T, dims) -> (B, T, heads, head_size)\n",
    "        k = tf.reshape(self.key(x), [B, T, self.heads, self.head_size])\n",
    "        q = tf.reshape(self.query(x), [B, T, self.heads, self.head_size])\n",
    "        v = tf.reshape(self.value(x), [B, T, self.heads, self.head_size])\n",
    "\n",
    "        # 1.1. Interact with KVCache\n",
    "        if cache:\n",
    "            # 1.2. Compose full key and value sequences, and update cache.\n",
    "            k, v, cache = self.update_cache(k, v, cache, token_axis=1)\n",
    "\n",
    "            # 1.3. Initialize RoPE encoding index for query\n",
    "            seqlen = tf.shape(k)[1]\n",
    "            rope_q_index = seqlen - 1\n",
    "        else:\n",
    "            rope_q_index = 0\n",
    "        \n",
    "        # 2. Transpose keys, queries and values to facilitate matrix multiplication\n",
    "        # (B, T, heads, head_size) -> (B, heads, T, head_size)\n",
    "        k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
    "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
    "        v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # 2.1. Apply RoPE if it is enabled\n",
    "        if hasattr(self, 'rope'):\n",
    "            q = self.rope(q, index=rope_q_index)\n",
    "            k = self.rope(k)\n",
    "        \n",
    "        # 3. Compute QK^T\n",
    "        # (B, heads, T, head_size) @ (B, heads, head_size, T)\n",
    "        # -> (B, heads, T, T)\n",
    "        attention = q @ tf.transpose(k, perm=[0, 1, 3, 2])\n",
    "        attention /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "        # 4. Conpute masked attention scores\n",
    "        tril = 1. if cache else tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        scores = tf.nn.softmax(tf.where(tril > 0.0, attention, float('-inf')))\n",
    "        \n",
    "        # 5. Apply attention dropout\n",
    "        scores = self.attn_dropout(scores, training=training)\n",
    "        \n",
    "        # 6. Attend values\n",
    "        # (B, heads, T, T) @ (B, heads, T, head_size)\n",
    "        # -> (B, heads, T, head_size)\n",
    "        x = scores @ v\n",
    "        \n",
    "        # 7. Format output\n",
    "        # (B, heads, T, head_size) -> (B, T, dims)\n",
    "        x = tf.reshape(tf.transpose(x, perm=[0, 2, 1, 3]), [B, T, self.dims])\n",
    "        \n",
    "        # 8. Apply residual dropout\n",
    "        x = self.residual_dropout(x, training=training)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# B, T, C = 2, 2, config.model.dims\n",
    "# l = SelfAttention(config.model)\n",
    "# l(tf.random.uniform((B, T, C)))\n",
    "\n",
    "# head_size = config.dims // config.heads\n",
    "# cache = dict(key=tf.zeros([B, 0, config.heads, head_size]), value=tf.zeros([B, 0, config.heads, head_size]))\n",
    "# l(tf.random.uniform((B, 1, C)), cache=cache)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, initializers\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.fc = dense_init(config, 4*config.dims, activation='gelu')\n",
    "        self.projection = dense_init(\n",
    "            config, config.dims,\n",
    "            kernel_initializer=initializers.RandomNormal(\n",
    "                mean=0.,\n",
    "                stddev=0.02/math.sqrt(2 * config.layers),\n",
    "            )\n",
    "        )\n",
    "        self.dropout = layers.Dropout(config.dropout)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        # (..., dims) -> (..., fc.dims)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # (..., fc.dims) -> (..., dims)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# B, T, C = 2, 2, config.model.dims\n",
    "# l = FeedForward(config.model)\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Block(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm_1 = layers.LayerNormalization()\n",
    "        self.attention = SelfAttention(config)\n",
    "        self.norm_2 = layers.LayerNormalization()\n",
    "        self.feed_forward = FeedForward(config)\n",
    "    \n",
    "    def call(self, x, training=None, cache=None):\n",
    "        x = x + self.attention(self.norm_1(x), training=training, cache=cache)\n",
    "        x = x + self.feed_forward(self.norm_2(x), training=training)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# l = Block(config.model)\n",
    "# B, T, C = 2, 2, config.model.dims\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, metrics\n",
    "from functools import reduce\n",
    "\n",
    "class NanoGPT(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input Args\n",
    "        self.use_cache = config.use_cache\n",
    "        self.heads = config.heads\n",
    "        self.head_size = config.dims // config.heads\n",
    "        self.block_size = config.block_size\n",
    "        self.num_layers = config.layers\n",
    "        \n",
    "        # Model elements\n",
    "        self.token_embed = embed_init(config, config.vocab_size)\n",
    "        self.dropout = layers.Dropout(config.dropout)\n",
    "        self.blocks = list(map(\n",
    "            lambda _: Block(config),\n",
    "            range(config.layers),\n",
    "        ))\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.head = dense_init(config, config.vocab_size)\n",
    "        \n",
    "        # Conditional model elements\n",
    "        if config.pos_encoding == 'embed':\n",
    "            self.pos_embed = embed_init(config, config.block_size)\n",
    "        \n",
    "        # # Metrics\n",
    "        # self.trackers = {\n",
    "        #     'loss': metrics.Mean(name=\"loss\"),\n",
    "        #     'val_loss': metrics.Mean(name=\"val_loss\"),\n",
    "        # }\n",
    "    \n",
    "    def call(self, x, training=None, cache=None):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # 1. Get embeddings for input tokens (B, T) -> (B, T, dims)\n",
    "        x_token_embed = self.token_embed(x)\n",
    "        \n",
    "        # 2. Get position embeddings for input tokens (B, T) -> (B, T, dims)\n",
    "        if hasattr(self, 'pos_embed'):\n",
    "            x_pos_embed = self.pos_embed(tf.range(T))\n",
    "        else:\n",
    "            x_pos_embed = 0.\n",
    "        \n",
    "        # 3. Combine token and position embeddings\n",
    "        x = self.dropout(x_token_embed + x_pos_embed, training=training)\n",
    "        \n",
    "        # 4. Apply blocks\n",
    "        x = reduce(\n",
    "            lambda y,item: item[0](y, training=training, cache=item[1]),\n",
    "            zip(self.blocks, cache if cache else [None]*self.num_layers),\n",
    "            x,\n",
    "        )\n",
    "        \n",
    "        # 5. Apply layer norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 6. Prediction head (B, T, dims) -> (B, T, vocab_size)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        # 1. Separate input and target\n",
    "        X, y = data\n",
    "        y = tf.cast(y, dtype=tf.float32)\n",
    "        \n",
    "        # 2. Compute loss (B, T) -> (B, T, vocab_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X, training=True)\n",
    "            loss = self.compute_loss(y=y, y_pred=logits)\n",
    "\n",
    "        # 3. Compute and apply gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return self.record('loss', loss)\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        # 1. Separate input and target\n",
    "        X, y = data\n",
    "\n",
    "        # 2. Compute loss (B, T) -> (B, T, vocab_size)\n",
    "        logits = self(X, training=False)\n",
    "        loss = self.compute_loss(y=y, y_pred=logits)\n",
    "    \n",
    "        return self.record('val_loss', loss)\n",
    "\n",
    "    def init_cache(self, sequence):\n",
    "        # 1. Get the batch size to initialize the cache\n",
    "        B = sequence.shape[0]\n",
    "        \n",
    "        # 2. Initialize cache shape\n",
    "        shape = (B, 0, self.heads, self.head_size)\n",
    "        \n",
    "        # 2. Initialize a cache for each attention block\n",
    "        return list(map(\n",
    "            # 3. Keys and values are initialized with zeros\n",
    "            lambda _: dict(\n",
    "                key=tf.zeros(shape),\n",
    "                value=tf.zeros(shape),\n",
    "            ),\n",
    "            self.blocks,\n",
    "        ))\n",
    "    \n",
    "    @tf.function\n",
    "    def generate(self, token_idx, num_tokens):\n",
    "        # 1. Initial the first token, the cache, and the input sequence length\n",
    "        sequence = token_idx\n",
    "        cache = self.init_cache(sequence) if self.use_cache else None\n",
    "        seq_len = 1 if self.use_cache else self.block_size\n",
    "        \n",
    "        # 2. Token generation loop\n",
    "        for index in range(num_tokens):\n",
    "            logits = self(sequence[:, -seq_len:], training=False, cache=cache)[:, -1, :]\n",
    "            token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "            sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "\n",
    "        return sequence\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return list(self.trackers.values())\n",
    "\n",
    "    def record(self, name, loss):\n",
    "        self.trackers[name].update_state(loss)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# block_size = 16\n",
    "# m = NanoGPT(config)\n",
    "\n",
    "# m(tf.random.uniform((2, block_size)))\n",
    "# m.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "\n",
    "# B, T = 2, 2\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T), (B, T))\n",
    "# m(x)\n",
    "# m.summary(expand_nested=True)\n",
    "\n",
    "m = NanoGPT(config.model)\n",
    "# m.load_weights('nodegpt.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0\n",
      "Token: 10\n",
      "generated_text='\\n&&KeYTCeroKQRncI'\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def generate(self, token_idx, num_tokens):\n",
    "    # 1. Initial the first token, the cache, and the input sequence length\n",
    "    sequence = token_idx\n",
    "    cache = self.init_cache(sequence) if self.use_cache else None\n",
    "    seq_len = 1 if self.use_cache else self.block_size\n",
    "    \n",
    "    # 2. Token generation loop\n",
    "    for index in range(num_tokens):\n",
    "        logits = self(sequence[:, -seq_len:], training=False, cache=cache)[:, -1, :]\n",
    "        token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "# 1. Pick start character\n",
    "NEWLINE_ID = config.vocab.index('\\n')\n",
    "sequence = tf.constant([NEWLINE_ID], shape=(1, 1), dtype=tf.int32)\n",
    "# num_tokens = config.model.block_size\n",
    "num_tokens = 16\n",
    "\n",
    "for _ in range(num_tokens):\n",
    "    if _ % 10 == 0: print(f'Token: {_}')\n",
    "    logits = m(sequence[:, -config.model.block_size:], training=False)[:, -1, :]\n",
    "    token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "    sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "\n",
    "generated_text = decoder(sequence[0].numpy())\n",
    "print(f'{generated_text=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 1), dtype=float32, numpy=\n",
      "array([[[[1.]],\n",
      "\n",
      "        [[1.]]]], dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 1), dtype=float32, numpy=\n",
      "array([[[[1.]],\n",
      "\n",
      "        [[1.]]]], dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 2), dtype=float32, numpy=\n",
      "array([[[[0.49945444, 0.50054556]],\n",
      "\n",
      "        [[0.49998766, 0.50001234]]]], dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 2), dtype=float32, numpy=\n",
      "array([[[[0.4997439 , 0.50025606]],\n",
      "\n",
      "        [[0.5000082 , 0.4999918 ]]]], dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 3), dtype=float32, numpy=\n",
      "array([[[[0.33344847, 0.33319923, 0.3333523 ]],\n",
      "\n",
      "        [[0.3331369 , 0.33317405, 0.33368906]]]], dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 3), dtype=float32, numpy=\n",
      "array([[[[0.33318198, 0.3332842 , 0.33353382]],\n",
      "\n",
      "        [[0.3331056 , 0.33318123, 0.33371317]]]], dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 4), dtype=float32, numpy=\n",
      "array([[[[0.24996194, 0.24999793, 0.24986021, 0.2501799 ]],\n",
      "\n",
      "        [[0.25002787, 0.24999781, 0.24996507, 0.2500093 ]]]],\n",
      "      dtype=float32)>\n",
      "tril=1.0\n",
      "scores=<tf.Tensor: shape=(1, 2, 1, 4), dtype=float32, numpy=\n",
      "array([[[[0.2498421 , 0.25000653, 0.24996728, 0.25018406]],\n",
      "\n",
      "        [[0.24994098, 0.24991803, 0.25001812, 0.25012287]]]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "caches = []\n",
    "\n",
    "def dump_cache(cache):\n",
    "    caches.append(copy.deepcopy(cache))\n",
    "\n",
    "# 1. Pick start character\n",
    "NEWLINE_ID = config.vocab.index('\\n')\n",
    "sequence = tf.constant([NEWLINE_ID], shape=(1, 1), dtype=tf.int32)\n",
    "\n",
    "cache = m.init_cache(sequence) if m.use_cache else None\n",
    "seq_len = 1 if m.use_cache else m.block_size\n",
    "\n",
    "# 2. Token generation loop\n",
    "for index in range(4):\n",
    "    logits = m(sequence[:, -seq_len:], training=False, cache=cache)[:, -1, :]\n",
    "    token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "    sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    dump_cache(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"llama_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  33280     \n",
      "                                                                 \n",
      " llama_block_1 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_2 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 16 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_99 (Dense)          multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_100 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_101 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_13 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_3 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " llama_block_2 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_4 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 17 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_102 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_103 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_104 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_14 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_5 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " llama_block_3 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_6 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 18 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_105 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_106 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_107 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_15 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_7 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " llama_block_4 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_8 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 19 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_108 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_109 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_110 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_16 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_9 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " dense_111 (Dense)           multiple                  33280     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264194 (1.01 MB)\n",
      "Trainable params: 264192 (1.01 MB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class GroupedQueryAttention(tf.keras.Model):\n",
    "    def __init__(self, cache_size, block_size, heads, kv_heads, dims):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.head_size = dims // heads\n",
    "        self.kv_heads = kv_heads or heads\n",
    "        \n",
    "        self.query = tf.keras.layers.Dense(dims, use_bias=False)\n",
    "        self.key = tf.keras.layers.Dense(kv_heads*self.head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(kv_heads*self.head_size, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(cache_size, block_size, self.kv_heads, self.head_size)\n",
    "        self.rope = RotaryPositionalEncodings(block_size, self.head_size)\n",
    "        \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        shape = tf.shape(x)\n",
    "        B, T = shape[0], shape[1]\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # (B, T, dims) -> (B, T, heads/kv_heads, head_dims)\n",
    "        q = tf.reshape(q, [B, T, self.heads, self.head_size])\n",
    "        k = tf.reshape(k, [B, T, self.kv_heads, self.head_size])\n",
    "        v = tf.reshape(v, [B, T, self.kv_heads, self.head_size])\n",
    "        \n",
    "        # RoPE expects inputs in (B, heads/kv_heads, T, head_size) format.\n",
    "        # Transpose(B, T, heads/kv_heads, head_size) -> (B, heads/kv_heads, T, head_size)\n",
    "        q = self.rope(tf.transpose(q, perm=[0, 2, 1, 3]), start=start)\n",
    "        k = self.rope(tf.transpose(k, perm=[0, 2, 1, 3]), start=start)\n",
    "        \n",
    "        if inference:\n",
    "            # Update KV cache\n",
    "            # KV cache expects inputs in (B, T, heads/kv_heads, head_size) format.\n",
    "            self.cache.update(\n",
    "                start,\n",
    "                tf.transpose(k, perm=[0, 2, 1, 3]),\n",
    "                v,\n",
    "            )\n",
    "            \n",
    "            # Get prefix context from the cache.\n",
    "            # (B, start+T, ...)\n",
    "            k, v = self.cache.get(B, start, T)\n",
    "            \n",
    "            ## Replicate KV heads to match query heads.\n",
    "            # (B, start+T, heads/kv_heads, head_size) -> (B, start+T, heads, head_size)\n",
    "            k = tf.tile(k, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "            \n",
    "            # (B, T, heads/kv_heads, head_dims) -> (B, T, heads, head_dims)\n",
    "            v = tf.tile(v, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "            \n",
    "            # (B, heads/kv_heads, T, head_size) @ (B, heads/kv_heads, head_size, start+T)\n",
    "            # -> (B, heads/kv_heads, T, start+T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 2, 3, 1])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "            wei = tf.nn.softmax(wei)\n",
    "        else:\n",
    "            assert start == 0\n",
    "            \n",
    "            ## Replicate KV heads to match query heads.\n",
    "            # (B, heads/kv_heads, T, head_size) -> (B, heads, T, head_size)\n",
    "            k = tf.tile(k, multiples=(1, self.heads//self.kv_heads, 1, 1))\n",
    "            \n",
    "            # (B, T, heads/kv_heads, head_dims) -> (B, T, heads, head_dims)\n",
    "            v = tf.tile(v, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "            \n",
    "            # (B, heads/kv_heads, T, head_size) @ (B, heads/kv_heads, head_size, start+T)\n",
    "            # -> (B, heads/kv_heads, T, start+T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 1, 3, 2])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "            tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "            wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "\n",
    "        # (B, heads/kv_heads, T, start+T) @ (B, heads/kv_heads, start+T, head_dims)\n",
    "        # -> (B, heads/kv_heads, T, head_dims)\n",
    "        out = wei @ tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # (B, heads/kv_heads, T, head_dims) -> (B, T, heads/kv_heads, head_dims)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return tf.reshape(out, shape=(B, T, -1))\n",
    "\n",
    "# l = GroupedQueryAttention(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=16,\n",
    "#     cache_size=4\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C), (B, T, C))\n",
    "# output = l(x, start)\n",
    "\n",
    "# B_i, T_i, C_i = 0, 0, 3\n",
    "# H_i, F_i = 0, C_i\n",
    "\n",
    "# print(\n",
    "#     f'{x[B_i]=}'\n",
    "#     f'\\n{l.cache.cache_k.shape=}\\n{l.cache.cache_k[B_i, :start+T]=}'\n",
    "#     f'\\n{l.cache.cache_v[B_i, :start+T]=}'\n",
    "# )\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SlidingWindowAttention(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.head_size = config.dims // config.heads\n",
    "        \n",
    "        self.query = tf.keras.layers.Dense(config.dims, use_bias=False)\n",
    "        self.key = tf.keras.layers.Dense(config.kv_heads*self.head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(config.kv_heads*self.head_size, use_bias=False)\n",
    "        \n",
    "        self.rope = RotaryPositionalEncodings(config.block_size, self.head_size)\n",
    "    \n",
    "    def as_strided(self, x):\n",
    "        shape = tf.shape(x)\n",
    "        B, T = shape[0], shape[2]\n",
    "\n",
    "        # (B, heads/kv_heads, T, head_size) -> (B, heads/kv_heads, T+2*window_size, head_size)\n",
    "        padded_x = tf.pad(x, [[0, 0], [0, 0], [self.config.window_size]*2, [0, 0]])\n",
    "\n",
    "        # indices.shape = (T, window_size + 1)\n",
    "        indices = tf.tile(tf.reshape(tf.range(T), (-1, 1)), [1, self.config.window_size + 1])\n",
    "        indices = indices + tf.expand_dims(tf.range(self.config.window_size + 1), axis=0)\n",
    "\n",
    "        # (B, heads/kv_heads, T, window_size + 1)\n",
    "        strided_x = tf.gather(padded_x, indices, axis=2)\n",
    "\n",
    "#         print(\n",
    "#             f'{x.shape=}'\n",
    "#             f'\\n{indices=}'\n",
    "#             f'\\n{padded_x.shape=}'\n",
    "#             f'{strided_x=}'\n",
    "#             f'\\n{x=}'\n",
    "#         )\n",
    "\n",
    "        return strided_x\n",
    "    \n",
    "    def as_grid(self, x):\n",
    "        # (B, heads, T, window_size+1) -> (B, heads, T, T)\n",
    "        diagonals = tf.transpose(x, perm=[0, 1, 3, 2])[..., ::-1, :]\n",
    "        grid_x = tf.linalg.diag(diagonals, k=(-self.config.window_size, 0), align='RIGHT_RIGHT')\n",
    "\n",
    "        # print(\n",
    "        #     f'\\n{x.shape=} {diagonals.shape=}'\n",
    "        #     f'\\n{grid_x.shape=}'\n",
    "        #     f'\\n{x[0, 0]=}'\n",
    "        #     # f'\\n{x[0, 0, -2:]=}'\n",
    "        #     f'\\n{grid_x[0, 0]=}'\n",
    "        #     # f'\\n{grid_x[0, 0, -2:]=}'\n",
    "        #     f'\\n{tf.math.reduce_sum(x[..., 0, 0])=}'\n",
    "        #     f'\\n{tf.reduce_all(x[..., 1, :] == grid_x[..., 1, :2])=}'\n",
    "        #     f'\\n{tf.reduce_all(x[..., -1, :] == grid_x[..., -1, -2:])=}'\n",
    "        # )\n",
    "\n",
    "        return grid_x\n",
    "        \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        shape = tf.shape(x)\n",
    "        B, T = shape[0], shape[1]\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # (B, T, dims) -> (B, T, heads/kv_heads, head_dims)\n",
    "        q = tf.reshape(q, [B, T, self.config.heads, self.head_size])\n",
    "        k = tf.reshape(k, [B, T, self.config.kv_heads, self.head_size])\n",
    "        v = tf.reshape(v, [B, T, self.config.kv_heads, self.head_size])\n",
    "        \n",
    "        # RoPE expects inputs in (B, heads/kv_heads, T, head_size) format.\n",
    "        # Transpose(B, T, heads/kv_heads, head_size) -> (B, heads/kv_heads, T, head_size)\n",
    "        q = self.rope(tf.transpose(q, perm=[0, 2, 1, 3]), start=start)\n",
    "        k = self.rope(tf.transpose(k, perm=[0, 2, 1, 3]), start=start)\n",
    "            \n",
    "        ## Replicate KV heads to match query heads.\n",
    "        # (B, heads/kv_heads, T, head_size) -> (B, heads, T, head_size)\n",
    "        k = tf.tile(k, multiples=(1, self.config.heads//self.config.kv_heads, 1, 1))\n",
    "        \n",
    "        # (B, T, heads/kv_heads, head_dims) -> (B, T, heads, head_dims)\n",
    "        v = tf.tile(v, multiples=(1, 1, self.config.heads//self.config.kv_heads, 1))\n",
    "\n",
    "        # Compose strided keys to mimic the sliding window. \n",
    "        # (B, heads, T, head_size) -> (B, heads, T, window_size+1, head_size)\n",
    "        strided_k = self.as_strided(k)\n",
    "\n",
    "        # (B, heads, T, head_size) @ (B, heads, T, window_size+1, head_size)\n",
    "        # -> (B, heads, T, window_size+1)\n",
    "        wei = tf.einsum('bhtd,bhtxd->bhtx', q, strided_k)\n",
    "\n",
    "        # (B, heads, T, window_size+1) -> (B, heads, T, T)\n",
    "        wei = self.as_grid(wei)\n",
    "        wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "        tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "\n",
    "        # (B, heads, T, T) @ (B, heads, T, head_dims)\n",
    "        # -> (B, heads, T, head_dims)\n",
    "        out = wei @ tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # (B, heads, T, head_dims) -> (B, T, heads, head_dims)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return tf.reshape(out, shape=(B, T, -1))\n",
    "\n",
    "# l = SlidingWindowAttention(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=32,\n",
    "#     window_size=2\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 32\n",
    "# x = tf.cast(tf.reshape(tf.range(B*T*C), (B, T, C)), dtype=tf.float32)\n",
    "# output = l(x)\n",
    "\n",
    "# B_i, T_i, C_i = 0, 0, 3\n",
    "# H_i, F_i = 0, C_i\n",
    "\n",
    "# print(\n",
    "#     f'{x[B_i]=}'\n",
    "#     f'{output.shape=}'\n",
    "# )\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, cache_size, block_size, head_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Input args\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(cache_size, block_size, head_size)\n",
    "        self.rope = RotaryPositionalEncodings(block_size, head_size)\n",
    "\n",
    "    def call(self, x, start=0, inference=False):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q = self.rope(q, start=start)\n",
    "        k = self.rope(k, start=start)\n",
    "        \n",
    "        if inference:\n",
    "            # Update KV cache\n",
    "            self.cache.update(start, k, v)\n",
    "            \n",
    "            # Get prefix context from the cache.\n",
    "            # (B, start+T, head_dims)\n",
    "            k, v = self.cache.get(B, start, T)\n",
    "            \n",
    "            # (B, T, head_size) @ (B, head_size, start+T) --> (B, T, start+T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 2, 1])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "            wei = tf.nn.softmax(wei)\n",
    "        else:\n",
    "            assert start == 0\n",
    "            \n",
    "            # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 2, 1])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "            tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "            wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "            \n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, cache_size, block_size, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.attn_layers = [SelfAttentionLayer(cache_size, block_size, head_size) for i in range(num_heads)]\n",
    "\n",
    "    def call(self, x, start=0, inference=False):\n",
    "        return tf.concat([\n",
    "            attn_layer(x, start=start, inference=inference) for attn_layer in self.attn_layers\n",
    "        ], axis=-1)\n",
    "\n",
    "# cache_size, block_size, num_heads, head_size = 2, 16, 1, 16\n",
    "# B, T, C = 2, 2, num_heads*head_size\n",
    "\n",
    "# l = MultiHeadAttentionLayer(cache_size, block_size, num_heads, head_size)\n",
    "\n",
    "# for start in range(T):\n",
    "#     print(\n",
    "#         f'\\n{start=}::{l(tf.random.uniform((B, 1, C)), start=start, inference=True).shape}\\n'\n",
    "#     )\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RMSNorm(layers.Layer):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self, shape):\n",
    "        self.w = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=(1, shape[-1]),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "    \n",
    "    def norm(self, x):\n",
    "        return x*tf.math.rsqrt(tf.math.reduce_mean(x**2, axis=-1, keepdims=True) + self.eps)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.w * self.norm(x)\n",
    "\n",
    "# data = tf.constant(np.arange(20).reshape(5, 2, 2) * 10, dtype=tf.float32)\n",
    "\n",
    "# l=RMSNorm()\n",
    "# l.build(data.shape)\n",
    "# l(data)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, dims):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.hidden_dims = 4*dims\n",
    "        self.linear_1 = layers.Dense(self.hidden_dims, use_bias=False, activation='swish')\n",
    "        self.linear_2 = layers.Dense(self.hidden_dims, use_bias=False)\n",
    "        self.linear_3 = layers.Dense(dims, use_bias=False)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # (..., dims) -> (..., hidden_dims)\n",
    "        x = self.linear_1(x)*self.linear_2(x)\n",
    "        \n",
    "        # (..., hidden_dims) -> (..., dims)\n",
    "        x = self.linear_3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# l = FeedForward(C)\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class AttentionSelector(object):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(AttentionSelector, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.config = config\n",
    "    \n",
    "    @property\n",
    "    def choice(self):\n",
    "        return (self.config.attention or 'msa').lower()\n",
    "    \n",
    "    def select(self):\n",
    "        if self.choice in ['slidingwindow', 'slidingwindowattention', 'swa']:\n",
    "            return SlidingWindowAttention(self.config)\n",
    "        elif self.choice in ['groupedquery', 'groupedqueryattention', 'gqa']:\n",
    "            return GroupedQueryAttention(self.config)\n",
    "        else:\n",
    "            return MultiHeadAttentionLayer(self.config)\n",
    "    \n",
    "    def call(self, l, x, start=0, inference=False):\n",
    "        if self.choice in ['slidingwindow', 'slidingwindowattention', 'swa']:\n",
    "            return l(x)\n",
    "        elif self.choice in ['groupedquery', 'groupedqueryattention', 'gqa']:\n",
    "            return l(x, start=start, inference=inference)\n",
    "        else:\n",
    "            return l(x, start=start, inference=inference)\n",
    "\n",
    "class NormSelector(object):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(NormSelector, self).__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "    \n",
    "    def select(self):\n",
    "        choice = (self.config.norm or 'rms').lower()\n",
    "        if choice in ['batchnorm', 'bn', 'batchnormalization']:\n",
    "            return layers.BatchNormalization()\n",
    "        else:\n",
    "            return RMSNorm()\n",
    "\n",
    "class LlamaBlock(tf.keras.Model):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(LlamaBlock, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.norm_selector = NormSelector(config)\n",
    "        self.attention_selector = AttentionSelector(config)\n",
    "        \n",
    "        self.norm_1 = self.norm_selector.select()\n",
    "        self.attention = self.attention_selector.select()\n",
    "        self.norm_2 = self.norm_selector.select()\n",
    "    \n",
    "    def call(self, x, start, inference):\n",
    "        x += self.attention_selector.call(\n",
    "            self.attention,\n",
    "            self.norm_1(x), start=start, inference=inference\n",
    "        )\n",
    "        x = self.norm_2(x)\n",
    "#         x += self.feed_forward(self.norm_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# l = LlamaBlock(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=16,\n",
    "#     cache_size=4\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, metrics\n",
    "from functools import reduce\n",
    "\n",
    "class LlamaModel(tf.keras.Model):\n",
    "    def __init__(self, config, loss_fn, *args, **kwargs):\n",
    "        super(LlamaModel, self).__init__(*args, **kwargs)\n",
    "        # Args\n",
    "        self.config = config\n",
    "\n",
    "        # Model elements\n",
    "        self.embeddings = layers.Embedding(config.vocab_size, config.embed_dims)\n",
    "        self.pos_embeddings = layers.Embedding(config.block_size, config.embed_dims) if config.pos_embeddings else None\n",
    "        self.dec_blocks = [LlamaBlock(config) for _ in range(config.decoders)]\n",
    "\n",
    "        self.head = layers.Dense(config.vocab_size, use_bias=False)\n",
    "\n",
    "        # Loss \n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        # Metrics\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "    \n",
    "    def alternating_reduce(self, fn, blocks, x):\n",
    "        for b_index, block in enumerate(blocks):\n",
    "            # Pick the sub-block for transformer branch\n",
    "            pivot = b_index % self.config.alternate_blocks\n",
    "\n",
    "            # Split input into subblocks\n",
    "            xs = tf.split(x, self.config.alternate_blocks, axis=-1)\n",
    "\n",
    "            # Process picked block\n",
    "            x = fn(xs[pivot], block)\n",
    "\n",
    "            # Re-compose\n",
    "            x = tf.concat([*xs[:pivot], x, *xs[(pivot + 1):]], axis=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        x_pos_embed = self.pos_embeddings(tf.range(T)) if self.pos_embeddings else None\n",
    "        x_embed = self.embeddings(x)\n",
    "        \n",
    "        x = x_embed + x_pos_embed if self.pos_embeddings else x_embed\n",
    "        \n",
    "        reduce_fn = self.alternating_reduce if self.config.alternate_blocks else reduce\n",
    "        \n",
    "        x = reduce_fn(\n",
    "            lambda y,dec_block: dec_block(y, start=start, inference=inference),\n",
    "            self.dec_blocks,\n",
    "            x\n",
    "        )\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        y = tf.cast(y, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X)\n",
    "            loss = self.loss_fn(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return self.record(loss, y, logits)\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        X, y = data\n",
    "\n",
    "        logits = self(X)\n",
    "        loss = self.loss_fn(y, logits)\n",
    "    \n",
    "        return self.record(loss, y, logits)\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "    \n",
    "    def record(self, loss, y, logits):\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=len(model_vocab),\n",
    "    \n",
    "    # Model Size and Prediction Capacity\n",
    "    decoders=4,\n",
    "    block_size=64,\n",
    "    embed_dims=512,\n",
    "    dims=128,\n",
    "    \n",
    "    # Attention Params\n",
    "    attention='swa',\n",
    "    heads=8,\n",
    "    kv_heads=8,\n",
    "    \n",
    "    # KV Cache\n",
    "    cache_size=2,\n",
    "    \n",
    "    # Sliding Window Attention\n",
    "    window_size=3,\n",
    "    \n",
    "    # Features\n",
    "    pos_embeddings=False,\n",
    "    alternate_blocks=4,\n",
    ")\n",
    "\n",
    "llama = LlamaModel(config=model_config, loss_fn=loss_fn)\n",
    "llama(tf.random.uniform((2, llama.config.block_size), minval=0, maxval=llama.config.vocab_size, dtype=tf.int32))\n",
    "llama.compile(optimizer=optimizer, loss=loss_fn)\n",
    "llama.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama = LlamaModel(config=model_config, loss_fn=loss_fn)\n",
    "\n",
    "# config_data = model_config._asdict()\n",
    "# config_data['kv_heads'] = config_data['kv_heads']//grouping_factor\n",
    "\n",
    "# grouped_config = ModelConfig(**config_data)\n",
    "\n",
    "# llama_grouped.compile(optimizer=optimizer, loss=loss_fn)\n",
    "# llama_grouped(tf.random.uniform((2, 5), minval=0, maxval=llama_grouped.vocab_size, dtype=tf.int32))\n",
    "# llama_grouped.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_stats(text, tokenizer, fns=[]):\n",
    "    def stats_fn(token):\n",
    "        return np.array([token.lower_] + [fn(token) for fn in fns])\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    stats = np.stack([stats_fn(token) for token in tokens])\n",
    "\n",
    "    return stats\n",
    "\n",
    "def random_token_from_logits(logits, samples=1):\n",
    "    return tf.random.categorical(logits, samples, dtype=tf.int32)\n",
    "\n",
    "def argmax_token(logits, samples=1):\n",
    "    return tf.expand_dims(\n",
    "        tf.math.argmax(tf.math.softmax(logits), axis=-1, output_type=tf.int32),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "def generate(model, token_idx, tokens, randomize=True):\n",
    "    sequence = token_idx\n",
    "    for start in range(tokens - 1):\n",
    "        logits = model(token_idx, start=start, inference=True)[:, -1, :]\n",
    "#         token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        token_idx = random_token_from_logits(logits) if randomize else argmax_token(logits)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def generate_no_cache(model, token_idx, tokens, randomize=True):\n",
    "    sequence = token_idx\n",
    "    for _ in range(tokens - 1):\n",
    "        logits = model(sequence[:, -model.block_size:])[:, -1, :]\n",
    "#         token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        token_idx = random_token_from_logits(logits) if randomize else argmax_token(logits)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def generate_and_evaluate(model_vocab, generator, model, tokens=50, randomize=True):\n",
    "    idx_to_char = {i:c for c,i in model_vocab.items()}\n",
    "    decoder = lambda x: ''.join([idx_to_char[i] for i in x])\n",
    "\n",
    "    starter = tf.constant([model_vocab[' ']], shape=(1, 1), dtype=tf.int32)\n",
    "    generated_text = decoder(\n",
    "        generator(\n",
    "            model,\n",
    "            starter,\n",
    "            tokens=tokens,\n",
    "            randomize=randomize,\n",
    "        )[0].numpy()\n",
    "    )\n",
    "\n",
    "    print(f'{generator.__name__}:: {generated_text=}')\n",
    "\n",
    "# word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "# print(\n",
    "#     f'\\nVocab: {list(word_vocab.keys())[:5]}'\n",
    "# )\n",
    "# generate_and_evaluate(model_vocab, generate, llama, randomize=False, tokens=min(50, llama.block_size))\n",
    "# generate_and_evaluate(model_vocab, generate_no_cache, llama, randomize=False, tokens=min(50, llama.block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_layers(model, exceptions=[]):\n",
    "    def fn(l):\n",
    "        if hasattr(l, 'layers'):\n",
    "            return list(itertools.chain(*map(fn, l.layers)))\n",
    "        elif l.name in exceptions:\n",
    "            return [None]\n",
    "        else:\n",
    "            return [l]\n",
    "    \n",
    "    return list(filter(lambda x: x is not None, fn(model)))\n",
    "\n",
    "\n",
    "def apply_groupings(src_model, target_model, factor=2):\n",
    "    def grouped_apply(l1, l2):\n",
    "        grouped_w1s = []\n",
    "\n",
    "        for w1, w2 in zip(l1.get_weights(), l2.get_weights()):\n",
    "            w1_groups = tf.split(w1, factor, axis=-1)\n",
    "            print(f'\\nSplit {src_model.name}.{l1.name}({w1.shape}) --> {factor}x{w1_groups[0].shape}')\n",
    "\n",
    "            w1_grouped = tf.math.add(*w1_groups) / factor\n",
    "            print(f'Average {factor}x{w1_groups[0].shape} --> {w1_grouped.shape}')\n",
    "\n",
    "            grouped_w1s.append(w1_grouped)\n",
    "        \n",
    "        # Apply grouped weights to the target\n",
    "        l2.set_weights(grouped_w1s)\n",
    "        print(f'Copied? {np.array_equal(grouped_w1s, l2.get_weights())}')\n",
    "\n",
    "    src_attention = src_model.dec_blocks[0].attention\n",
    "    target_attention = target_model.dec_blocks[0].attention\n",
    "\n",
    "    src_other_layers = get_layers(\n",
    "        src_model,\n",
    "        exceptions=list(map(lambda x: x.name, [src_attention.key, src_attention.value]))\n",
    "    )\n",
    "\n",
    "    target_other_layers = get_layers(\n",
    "        target_model,\n",
    "        exceptions=list(map(lambda x: x.name, [target_attention.key, target_attention.value]))\n",
    "    )\n",
    "\n",
    "    for src_layer, target_layer in zip(src_other_layers, target_other_layers):\n",
    "        target_layer.set_weights(src_layer.get_weights())\n",
    "    \n",
    "    # Group keys and values.\n",
    "    print(f'\\n Grouping Layers ({factor=})\\n============================')\n",
    "    grouped_apply(src_attention.key, target_attention.key)\n",
    "    grouped_apply(src_attention.value, target_attention.value)\n",
    "\n",
    "def compare_weights(one, two):\n",
    "    one_layers = get_layers(one)\n",
    "    two_layers = get_layers(two)\n",
    "\n",
    "    print('\\n Comparison Status\\n===================')\n",
    "\n",
    "    for ol, tl in filter(lambda x: x[0].get_weights(),  zip(one_layers, two_layers)):\n",
    "        if np.array_equal(ol.get_weights(), tl.get_weights()):\n",
    "            print(f'{ol.name} == {tl.name}')\n",
    "        else:\n",
    "            print(f'\\t{ol.name} != {tl.name}')\n",
    "\n",
    "# apply_groupings(llama, llama_grouped)\n",
    "# compare_weights(llama, llama_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Grouping Layers (factor=2)\n",
      "============================\n",
      "\n",
      "Split llama_model_37.dense_384((128, 128)) --> 2x(128, 64)\n",
      "Average 2x(128, 64) --> (128, 64)\n",
      "Copied? True\n",
      "\n",
      "Split llama_model_37.dense_385((128, 128)) --> 2x(128, 64)\n",
      "Average 2x(128, 64) --> (128, 64)\n",
      "Copied? True\n",
      "fn:: generated_text=' sssawaksedreerederedoubexfoctleereere:rere:reree:'\n"
     ]
    }
   ],
   "source": [
    "def get_grouped_generator(grouped_model, grouping_factor, base_fn=generate_no_cache):\n",
    "    def fn(model, *args, **kwargs):\n",
    "        apply_groupings(model, grouped_model)\n",
    "        return base_fn(grouped_model, *args, **kwargs)\n",
    "    \n",
    "    return fn\n",
    "\n",
    "generator_grouped = get_grouped_generator(llama_grouped, grouping_factor=grouping_factor)\n",
    "generator_grouped_cached = get_grouped_generator(llama_grouped, grouping_factor=grouping_factor, base_fn=generate)\n",
    "\n",
    "generate_and_evaluate(model_vocab, generator, llama, randomize=False, tokens=min(50, llama.block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei(1)::False [[[21.616919 43.966854]]] [[[21.616917 43.966858]]] (1, 1, 2) (1, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# llama.layers[2].layers[1].attn_layers[0].logs['cache']['k']\n",
    "cache_logs = llama.layers[2].layers[1].attn_layers[0].logs['cache']\n",
    "no_cache_logs = llama.layers[2].layers[1].attn_layers[0].logs['no-cache']\n",
    "\n",
    "log_items = len(cache_logs['k'])\n",
    "\n",
    "# for key in ['in_x', 'k', 'q', 'v', 'rope_k', 'rope_q', 'wei', 'out']:\n",
    "for key in ['wei']:\n",
    "    for idx in [1]:\n",
    "        print(\n",
    "            f'{key}({idx})::{tf.math.reduce_all(cache_logs[key][idx] == no_cache_logs[key][idx+1][..., idx:, :])}'\n",
    "            f' {cache_logs[key][idx]} {no_cache_logs[key][idx+1][..., idx:, :]}'\n",
    "            f' {cache_logs[key][idx].shape} {no_cache_logs[key][idx+1][..., idx:, :].shape}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True \n",
      "True \n",
      "cache_logs['wei'][index]=<tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[21.616919, 43.966854]]], dtype=float32)>\n",
      "no_cache_logs['wei'][index+1]=<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
      "array([[[29.863405 ,  7.2788296],\n",
      "        [21.616917 , 43.966858 ]]], dtype=float32)>\n",
      "cache_logs['wei'][index]=<tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[21.616919, 43.966854]]], dtype=float32)>\n",
      "no_cache_logs['wei'][index+1]=<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
      "array([[[29.863405 ,  7.2788296],\n",
      "        [21.616917 , 43.966858 ]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# tf.math.reduce_all(cache_logs['cache_k'][1] == no_cache_logs['rope_k'][2])\n",
    "index = 1\n",
    "print(\n",
    "    f\"{tf.math.reduce_all(cache_logs['wei'][index] == no_cache_logs['wei'][index+1])}\"\n",
    "    # f\"\\n{tf.math.reduce_all(cache_logs['cache_v'][1] == no_cache_logs['v'][2])}\"\n",
    "    f\"\\n{tf.math.reduce_all(cache_logs['cache_k'][index] == no_cache_logs['rope_k'][index+1])}\",\n",
    "    # f\"\\n{tf.math.reduce_all(cache_logs['cache_v'][index] == no_cache_logs['v'][index+1][:, 1:, :])}\",\n",
    "    f\"\\n{tf.math.reduce_all(cache_logs['rope_q'][index] == no_cache_logs['rope_q'][index+1][:, 1:, :])}\",\n",
    "    # f\"\\n{cache_logs['rope_q'][index]=}\"\n",
    "    # f\"\\n{no_cache_logs['rope_q'][index+1]=}\"\n",
    "    # f\"\\n{cache_logs['cache_k'][index]=}\"\n",
    "    # f\"\\n{no_cache_logs['rope_k'][index+1]=}\"\n",
    "    f\"\\n{cache_logs['wei'][index]=}\"\n",
    "    f\"\\n{no_cache_logs['wei'][index+1]=}\"\n",
    "    f\"\\n{cache_logs['wei'][index]=}\"\n",
    "    f\"\\n{no_cache_logs['wei'][index+1]=}\"\n",
    ")\n",
    "# cache_logs['out'][1], no_cache_logs['out'][2][..., 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 16), dtype=float32, numpy=\n",
       " array([[[ 2.697421  , -2.4562492 , -1.5515298 ,  0.14776081,\n",
       "           0.37695628, -1.5206785 ,  0.8845265 ,  0.17617644,\n",
       "          -2.1475816 ,  1.1095126 , -0.10803113, -4.174893  ,\n",
       "           0.12987702,  0.36463377, -4.113693  , -0.7377639 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2, 16), dtype=float32, numpy=\n",
       " array([[[ 4.474342  , -1.4330535 , -4.977524  , -4.9114738 ,\n",
       "          -0.23111431,  1.3314574 ,  0.03826439,  1.3454266 ,\n",
       "           1.7946535 ,  0.05863512, -1.4367882 , -2.3322222 ,\n",
       "           0.93547916, -2.7148142 ,  0.902055  ,  0.88373613],\n",
       "         [ 1.8138299 , -2.927385  , -3.3529627 , -4.1700516 ,\n",
       "          -0.463976  , -2.7048278 , -1.8990085 ,  1.7266546 ,\n",
       "           1.0458257 , -0.78157973, -0.0542576 , -2.8697302 ,\n",
       "           2.9209538 , -0.50924057, -3.9430103 ,  0.7489088 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[21.616919, 43.966854]]], dtype=float32)>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_q = cache_logs['rope_q'][index]\n",
    "c_k = cache_logs['cache_k'][index]\n",
    "\n",
    "c_q, c_k, c_q @ tf.transpose(c_k, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2, 16), dtype=float32, numpy=\n",
       " array([[[-0.23348725, -1.2534161 , -3.0547209 , -0.6355625 ,\n",
       "          -0.08479708,  1.8937602 ,  1.8187456 , -1.0931925 ,\n",
       "          -0.7623807 ,  2.3252077 , -0.7686442 , -1.8549696 ,\n",
       "           0.79256994, -1.0165763 ,  1.2038559 ,  0.9727511 ],\n",
       "         [ 2.697421  , -2.4562492 , -1.5515298 ,  0.14776081,\n",
       "           0.37695628, -1.5206785 ,  0.8845265 ,  0.17617644,\n",
       "          -2.1475816 ,  1.1095126 , -0.10803113, -4.174893  ,\n",
       "           0.12987702,  0.36463377, -4.113693  , -0.7377639 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2, 16), dtype=float32, numpy=\n",
       " array([[[ 4.474342  , -1.4330535 , -4.977524  , -4.9114738 ,\n",
       "          -0.23111431,  1.3314574 ,  0.03826439,  1.3454266 ,\n",
       "           1.7946535 ,  0.05863512, -1.4367882 , -2.3322222 ,\n",
       "           0.93547916, -2.7148142 ,  0.902055  ,  0.88373613],\n",
       "         [ 1.8138299 , -2.927385  , -3.3529627 , -4.1700516 ,\n",
       "          -0.463976  , -2.7048278 , -1.8990085 ,  1.7266546 ,\n",
       "           1.0458257 , -0.78157973, -0.0542576 , -2.8697302 ,\n",
       "           2.9209538 , -0.50924057, -3.9430103 ,  0.7489088 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
       " array([[[29.863405 ,  7.2788296],\n",
       "         [21.616917 , 43.966858 ]]], dtype=float32)>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_q = no_cache_logs['rope_q'][index + 1]\n",
    "nc_k = no_cache_logs['rope_k'][index + 1]\n",
    "\n",
    "nc_q, nc_k, nc_q @ tf.transpose(nc_k, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
      "array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
      "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
      "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
      "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
      "         44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
      "         55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
      "         66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
      "         77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
      "         88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
      "         99.],\n",
      "       [100., 101., 102., 103., 104., 105., 106., 107., 108., 109., 110.,\n",
      "        111., 112., 113., 114., 115., 116., 117., 118., 119., 120., 121.,\n",
      "        122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,\n",
      "        133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,\n",
      "        144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,\n",
      "        155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,\n",
      "        166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176.,\n",
      "        177., 178., 179., 180., 181., 182., 183., 184., 185., 186., 187.,\n",
      "        188., 189., 190., 191., 192., 193., 194., 195., 196., 197., 198.,\n",
      "        199.]], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
       "array([[ 61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250.],\n",
       "       [311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250.]], dtype=float32)>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "K = 2\n",
    "\n",
    "def alternating_reduce(fn, blocks, x):\n",
    "    for b_index, block in enumerate(blocks):\n",
    "        # Pick the sub-block for transformer branch\n",
    "        pivot = b_index % K\n",
    "\n",
    "        # Split input into subblocks\n",
    "        xs = tf.split(x, K, axis=-1)\n",
    "\n",
    "        # Process picked block\n",
    "        x = block(xs[pivot])\n",
    "\n",
    "        # Re-compose\n",
    "        x = tf.concat([*xs[:pivot], x, *xs[(pivot + 1):]], axis=-1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "x = tf.reshape(tf.cast(tf.range(2*100), dtype=tf.float32), (2, 100))\n",
    "\n",
    "print(f'{x=}')\n",
    "alternating_reduce(\n",
    "    lambda y,dec_block: dec_block(y),\n",
    "    [\n",
    "        # layers.Dense(100, use_bias=False),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "    ],\n",
    "    x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AltUpBlockSequence(tf.keras.Model):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(AltUpBlockSequence, self).__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.blocks = [LlamaBlock(config) for _ in range(config.decoders)]\n",
    "    \n",
    "    def build(self, shape):\n",
    "        self.prediction_scalars = self.add_weight(\n",
    "            \"prediction_scalars\",\n",
    "            shape=(self.config.alternate_blocks,)*2,\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.correction_scalars = self.add_weight(\n",
    "            \"correction_scalars\",\n",
    "            shape=(self.config.alternate_blocks,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "    \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        for b_index, block in enumerate(self.blocks):\n",
    "            # Pick the sub-block for transformer branch\n",
    "            pivot = b_index % self.config.alternate_blocks\n",
    "\n",
    "            # Split input into subblocks\n",
    "            xs = tf.split(x, self.config.alternate_blocks, axis=-1)\n",
    "\n",
    "            # Prediction with a linear map\n",
    "            xs_hat = tf.unstack(\n",
    "                tf.einsum('aa,axyz->axyz', self.prediction_scalars, tf.stack(xs, axis=0)),\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            # Process picked block\n",
    "            x = block(xs[pivot], start=start, inference=inference)\n",
    "\n",
    "            # Correction\n",
    "            xs = list(map(\n",
    "                lambda idx: xs_hat[idx] + self.correction_scalars[idx]*(x - xs_hat[pivot]),\n",
    "                range(self.config.alternate_blocks),\n",
    "            ))\n",
    "\n",
    "            # Re-compose\n",
    "            # x = tf.concat([*xs[:pivot], x, *xs[(pivot + 1):]], axis=-1)\n",
    "            x = tf.concat(xs, axis=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "l = AltUpBlockSequence(model_config)\n",
    "x = tf.random.uniform((2, model_config.block_size, model_config.embed_dims))\n",
    "y = l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=TensorShape([8, 32]) y.shape=TensorShape([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.7486548>,\n",
       "  'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>},\n",
       " None,\n",
       " {'loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       "  'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, metrics, losses, optimizers\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = layers.Dense(10)\n",
    "\n",
    "        self.trackers = {\n",
    "            'loss': metrics.Mean(name=\"loss\"),\n",
    "            'val_loss': metrics.Mean(name=\"val_loss\"),\n",
    "        }\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.dense(x)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X, training=True)\n",
    "            loss = self.compute_loss(y=y, y_pred=logits)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        return self.record('loss', loss)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        logits = self(X, training=True)\n",
    "        loss = self.compute_loss(y=y, y_pred=logits)\n",
    "\n",
    "        return self.record('val_loss', loss)\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return list(self.trackers.values())\n",
    "\n",
    "    def record(self, name, loss):\n",
    "        self.trackers[name].update_state(loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = optimizers.AdamW()\n",
    "model = CustomModel()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "B, C = 8, 32\n",
    "classes = 10\n",
    "X = tf.random.normal((B, C))\n",
    "y = tf.random.uniform((B,), minval=0, maxval=classes, dtype=tf.int32)\n",
    "print(f'{X.shape=} {y.shape=}')\n",
    "\n",
    "model.train_step((X, y)), model.reset_metrics(), model.get_metrics_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
