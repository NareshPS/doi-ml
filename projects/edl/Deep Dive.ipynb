{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen', ':', '\\n', 'before']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_word_1gram_frequencies(data, tokenizer):\n",
    "    words = map(lambda x:x.lower_, itertools.chain(*map(tokenizer, data)))\n",
    "    return Counter(words)\n",
    "\n",
    "word_tokenizer = spacy.blank(\"en\")\n",
    "word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "\n",
    "print(\n",
    "    f'{list(word_vocab.keys())[:5]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1], dtype=object)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = nlp('and to')\n",
    "# ! pip install levenshtein\n",
    "\n",
    "from Levenshtein import distance as edit_distance\n",
    "\n",
    "def get_proximity_with_vocab(text, tokenizer, vocab):\n",
    "    def proximity_fn(word):\n",
    "        if word in vocab:\n",
    "            distance = 0\n",
    "        else:\n",
    "            distance = min(*map(\n",
    "                lambda vocab_word: edit_distance(word, vocab_word),\n",
    "                vocab.keys()\n",
    "            ))\n",
    "        \n",
    "        return distance\n",
    "\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    distance = reduce(lambda y,token: y+proximity_fn(str(token)), tokens, 0)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def get_all_proximity_with_vocab(texts, tokenizer, vocab):\n",
    "    return np.fromiter(\n",
    "        map(\n",
    "            lambda text: get_proximity_with_vocab(text, tokenizer, vocab),\n",
    "            texts,\n",
    "        ),\n",
    "        dtype=object\n",
    "    )\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "texts = [\n",
    "    'hello, how are you doing?',\n",
    "    'hello, how are you doing?',\n",
    "]\n",
    "\n",
    "get_all_proximity_with_vocab(texts, nlp, word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n"
     ]
    }
   ],
   "source": [
    "! head input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, optimizers\n",
    "\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = optimizers.legacy.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* [X] loss and accuracy metrics.\n",
    "* [X] training, validation and inference flags.\n",
    "* Match dictionary with embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model_vocab = OrderedDict([('F', 0), ('i', 1), ('r', 2), ('s', 3), ('t', 4), (' ', 5), ('C', 6), ('z', 7), ('e', 8), ('n', 9), (':', 10), ('\\n', 11), ('B', 12), ('f', 13), ('o', 14), ('w', 15), ('p', 16), ('c', 17), ('d', 18), ('a', 19), ('y', 20), ('u', 21), ('h', 22), (',', 23), ('m', 24), ('k', 25), ('.', 26), ('A', 27), ('l', 28), ('S', 29), ('Y', 30), ('v', 31), ('?', 32), ('R', 33), ('M', 34), ('W', 35), (\"'\", 36), ('L', 37), ('I', 38), ('N', 39), ('g', 40), (';', 41), ('b', 42), ('!', 43), ('O', 44), ('j', 45), ('V', 46), ('-', 47), ('T', 48), ('H', 49), ('E', 50), ('U', 51), ('D', 52), ('P', 53), ('q', 54), ('x', 55), ('J', 56), ('G', 57), ('K', 58), ('Q', 59), ('&', 60), ('Z', 61), ('X', 62), ('3', 63), ('$', 64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "datapath = Path('./input.txt')\n",
    "\n",
    "with open(datapath) as f:\n",
    "    tiny_shakespere = list(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = ''.join(tiny_shakespere)\n",
    "\n",
    "all_data_length = len(all_data)\n",
    "train_data_length = int(all_data_length*.9)\n",
    "valid_data_length = all_data_length - train_data_length\n",
    "\n",
    "train_data = all_data[:train_data_length]\n",
    "valid_data = all_data[-valid_data_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Set\n",
      "==============\n",
      "X.shape=TensorShape([2, 128]) y.shape=TensorShape([2, 128]) [[ 0  1  2  3  4  5  6  1  4  1  7  8  9 10 11 12  8 13 14  2  8  5 15  8\n",
      "   5 16  2 14 17  8  8 18  5 19  9 20  5 13 21  2  4 22  8  2 23  5 22  8\n",
      "  19  2  5 24  8  5  3 16  8 19 25 26 11 11 27 28 28 10 11 29 16  8 19 25\n",
      "  23  5  3 16  8 19 25 26 11 11  0  1  2  3  4  5  6  1  4  1  7  8  9 10\n",
      "  11 30 14 21  5 19  2  8  5 19 28 28  5  2  8  3 14 28 31  8 18  5  2 19\n",
      "   4 22  8  2  5  4 14  5]\n",
      " [ 1  2  3  4  5  6  1  4  1  7  8  9 10 11 12  8 13 14  2  8  5 15  8  5\n",
      "  16  2 14 17  8  8 18  5 19  9 20  5 13 21  2  4 22  8  2 23  5 22  8 19\n",
      "   2  5 24  8  5  3 16  8 19 25 26 11 11 27 28 28 10 11 29 16  8 19 25 23\n",
      "   5  3 16  8 19 25 26 11 11  0  1  2  3  4  5  6  1  4  1  7  8  9 10 11\n",
      "  30 14 21  5 19  2  8  5 19 28 28  5  2  8  3 14 28 31  8 18  5  2 19  4\n",
      "  22  8  2  5  4 14  5 18]] [[ 1  2  3  4  5  6  1  4  1  7  8  9 10 11 12  8 13 14  2  8  5 15  8  5\n",
      "  16  2 14 17  8  8 18  5 19  9 20  5 13 21  2  4 22  8  2 23  5 22  8 19\n",
      "   2  5 24  8  5  3 16  8 19 25 26 11 11 27 28 28 10 11 29 16  8 19 25 23\n",
      "   5  3 16  8 19 25 26 11 11  0  1  2  3  4  5  6  1  4  1  7  8  9 10 11\n",
      "  30 14 21  5 19  2  8  5 19 28 28  5  2  8  3 14 28 31  8 18  5  2 19  4\n",
      "  22  8  2  5  4 14  5 18]\n",
      " [ 2  3  4  5  6  1  4  1  7  8  9 10 11 12  8 13 14  2  8  5 15  8  5 16\n",
      "   2 14 17  8  8 18  5 19  9 20  5 13 21  2  4 22  8  2 23  5 22  8 19  2\n",
      "   5 24  8  5  3 16  8 19 25 26 11 11 27 28 28 10 11 29 16  8 19 25 23  5\n",
      "   3 16  8 19 25 26 11 11  0  1  2  3  4  5  6  1  4  1  7  8  9 10 11 30\n",
      "  14 21  5 19  2  8  5 19 28 28  5  2  8  3 14 28 31  8 18  5  2 19  4 22\n",
      "   8  2  5  4 14  5 18  1]]\n",
      "X.numpy()=array([[ 0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,\n",
      "         8, 13, 14,  2,  8,  5, 15,  8,  5, 16,  2, 14, 17,  8,  8, 18,\n",
      "         5, 19,  9, 20,  5, 13, 21,  2,  4, 22,  8,  2, 23,  5, 22,  8,\n",
      "        19,  2,  5, 24,  8,  5,  3, 16,  8, 19, 25, 26, 11, 11, 27, 28,\n",
      "        28, 10, 11, 29, 16,  8, 19, 25, 23,  5,  3, 16,  8, 19, 25, 26,\n",
      "        11, 11,  0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10,\n",
      "        11, 30, 14, 21,  5, 19,  2,  8,  5, 19, 28, 28,  5,  2,  8,  3,\n",
      "        14, 28, 31,  8, 18,  5,  2, 19,  4, 22,  8,  2,  5,  4, 14,  5],\n",
      "       [ 1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,  8,\n",
      "        13, 14,  2,  8,  5, 15,  8,  5, 16,  2, 14, 17,  8,  8, 18,  5,\n",
      "        19,  9, 20,  5, 13, 21,  2,  4, 22,  8,  2, 23,  5, 22,  8, 19,\n",
      "         2,  5, 24,  8,  5,  3, 16,  8, 19, 25, 26, 11, 11, 27, 28, 28,\n",
      "        10, 11, 29, 16,  8, 19, 25, 23,  5,  3, 16,  8, 19, 25, 26, 11,\n",
      "        11,  0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11,\n",
      "        30, 14, 21,  5, 19,  2,  8,  5, 19, 28, 28,  5,  2,  8,  3, 14,\n",
      "        28, 31,  8, 18,  5,  2, 19,  4, 22,  8,  2,  5,  4, 14,  5, 18]]) y.numpy()=array([[ 1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,  8,\n",
      "        13, 14,  2,  8,  5, 15,  8,  5, 16,  2, 14, 17,  8,  8, 18,  5,\n",
      "        19,  9, 20,  5, 13, 21,  2,  4, 22,  8,  2, 23,  5, 22,  8, 19,\n",
      "         2,  5, 24,  8,  5,  3, 16,  8, 19, 25, 26, 11, 11, 27, 28, 28,\n",
      "        10, 11, 29, 16,  8, 19, 25, 23,  5,  3, 16,  8, 19, 25, 26, 11,\n",
      "        11,  0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11,\n",
      "        30, 14, 21,  5, 19,  2,  8,  5, 19, 28, 28,  5,  2,  8,  3, 14,\n",
      "        28, 31,  8, 18,  5,  2, 19,  4, 22,  8,  2,  5,  4, 14,  5, 18],\n",
      "       [ 2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 12,  8, 13,\n",
      "        14,  2,  8,  5, 15,  8,  5, 16,  2, 14, 17,  8,  8, 18,  5, 19,\n",
      "         9, 20,  5, 13, 21,  2,  4, 22,  8,  2, 23,  5, 22,  8, 19,  2,\n",
      "         5, 24,  8,  5,  3, 16,  8, 19, 25, 26, 11, 11, 27, 28, 28, 10,\n",
      "        11, 29, 16,  8, 19, 25, 23,  5,  3, 16,  8, 19, 25, 26, 11, 11,\n",
      "         0,  1,  2,  3,  4,  5,  6,  1,  4,  1,  7,  8,  9, 10, 11, 30,\n",
      "        14, 21,  5, 19,  2,  8,  5, 19, 28, 28,  5,  2,  8,  3, 14, 28,\n",
      "        31,  8, 18,  5,  2, 19,  4, 22,  8,  2,  5,  4, 14,  5, 18,  1]])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def make_dataset(data, block_size, vocab):\n",
    "    def indices(phrase):\n",
    "        return list(map(vocab.get, phrase))\n",
    "\n",
    "    @tf.numpy_function(Tout=(tf.int64, tf.int64))\n",
    "    def get_numpy_item(index):\n",
    "        X, y = data[index:index+block_size], data[index+1:index+block_size+1]\n",
    "        return indices(X), indices(y)\n",
    "\n",
    "    def get_item(index):\n",
    "        X, y = get_numpy_item(index)\n",
    "        return tf.reshape(X, [block_size]), tf.reshape(y, [block_size])\n",
    "    \n",
    "    return tf.data.Dataset.range(len(data) - block_size).map(get_item)\n",
    "\n",
    "ds = make_dataset(train_data, block_size, model_vocab).batch(2)\n",
    "X, y = next(iter(ds))\n",
    "\n",
    "print(\n",
    "    f'\\n\\nTrain Set\\n=============='\n",
    "    f'\\n{X.shape=} {y.shape=} {X} {y}'\n",
    "    f'\\n{X.numpy()=} {y.numpy()=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "fields = (\n",
    "    'vocab_size', \n",
    "    \n",
    "    # Model Size and Prediction Capacity\n",
    "    'decoders', 'block_size', 'embed_dims', 'dims',\n",
    "    \n",
    "    # Attention Params\n",
    "    'attention', 'heads', 'kv_heads',\n",
    "    \n",
    "    # KV Cache\n",
    "    'cache_size',\n",
    "    \n",
    "    # Sliding Window Attention\n",
    "    'window_size', \n",
    "    \n",
    "    # Features\n",
    "    'pos_embeddings', 'norm',\n",
    "    \n",
    "    # Alternating Updates\n",
    "    'alternate_blocks'\n",
    ")\n",
    "ModelConfig = namedtuple('ModelConfig', fields, defaults=(None, )*len(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"llama_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  33280     \n",
      "                                                                 \n",
      " llama_block_1 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_2 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 16 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_99 (Dense)          multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_100 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_101 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_13 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_3 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " llama_block_2 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_4 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 17 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_102 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_103 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_104 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_14 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_5 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " llama_block_3 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_6 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 18 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_105 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_106 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_107 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_15 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_7 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " llama_block_4 (LlamaBlock)  multiple                  49408     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_8 (RMSNorm)       multiple                  128      |\n",
      "|                                                               |\n",
      "| sliding_window_attention_  multiple                  49152    |\n",
      "| 19 (SlidingWindowAttentio                                     |\n",
      "| n)                                                            |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| dense_108 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_109 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| dense_110 (Dense)         multiple                  16384   ||\n",
      "||                                                             ||\n",
      "|| rotary_positional_encodi  multiple                  0       ||\n",
      "|| ngs_16 (RotaryPositional                                    ||\n",
      "|| Encodings)                                                  ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| rms_norm_9 (RMSNorm)       multiple                  128      |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " dense_111 (Dense)           multiple                  33280     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264194 (1.01 MB)\n",
      "Trainable params: 264192 (1.01 MB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RotaryPositionalEncodings(layers.Layer):\n",
    "    def __init__(self, block_size, dims, base=10000.):\n",
    "        super(RotaryPositionalEncodings, self).__init__()\n",
    "        \n",
    "        self.dims = dims\n",
    "\n",
    "        # θ = 1 / (base^(2(i - 1) / dims)), i = [1, dims/2]\n",
    "        self.theta = 1. / (base ** (tf.range(0, dims, 2, dtype=tf.float32) / dims))\n",
    "        self.positions = tf.range(block_size, dtype=tf.float32)\n",
    "        \n",
    "        self.mtheta = tf.concat([tf.einsum('n,d->nd', self.positions, self.theta)]*2, axis=-1)\n",
    "\n",
    "        self.cos_mtheta = tf.reshape(\n",
    "            tf.math.cos(self.mtheta),\n",
    "            [1, 1, *self.mtheta.shape],\n",
    "        )\n",
    "        self.sin_mtheta =  tf.reshape(\n",
    "            tf.math.sin(self.mtheta),\n",
    "            [1, 1, *self.mtheta.shape],\n",
    "        )\n",
    "\n",
    "    def call(self, x, start):\n",
    "        \"\"\"Compute positional encodings\n",
    "\n",
    "        Arguments:\n",
    "            x: A tensor of shape (B, _, T, _)\n",
    "            start: Token start position\n",
    "        \n",
    "        Returns:\n",
    "            An position-encoded tensor of shape (B, _, T, _)\n",
    "        \"\"\"\n",
    "        B, _, T, _ = x.shape\n",
    "\n",
    "        # (B, _, T, dims)*(1, 1, T, dims) -> (B, _, T, dims)\n",
    "        x_real = x*self.cos_mtheta[..., start:start+T, :]\n",
    "\n",
    "        # (B, _, T, dims)*(1, 1, T, dims) -> (B, _, T, dims)\n",
    "        x_img = tf.concat(\n",
    "            [-x[..., self.dims//2:], x[..., :self.dims//2]],\n",
    "            axis=-1\n",
    "        )*self.sin_mtheta[..., start:start+T, :]\n",
    "\n",
    "        # (B, _, T, dims) + (B, _, T, dims) -> (B, _, T, dims)\n",
    "        output = x_real + x_img\n",
    "        return output\n",
    "\n",
    "# B, T, H, D = 2, 8, 4, 4\n",
    "# B_i, T_i, H_i, D_i = 0, 1, 0, 0\n",
    "# x = tf.random.uniform((B, H, T, D))\n",
    "# l = RotaryPositionalEncodings(T, D)\n",
    "# output = l(x, start=0)\n",
    "# # tf.print(f'{l.theta=} {l.theta.shape}')\n",
    "# # tf.print(f'{l.positions=} {l.positions.shape}')\n",
    "# tf.print(f'{x[B_i]=}')\n",
    "# tf.print(f'{l.cos_mtheta=}')\n",
    "# tf.print(f'{l.sin_mtheta=}')\n",
    "# tf.print(f'{output[B_i]=}\\n{output[B_i]=} {output.shape}')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class KVCache(object):\n",
    "    def __init__(self, *shape):\n",
    "        super(KVCache, self).__init__()\n",
    "\n",
    "        with tf.device('/device:CPU:0'):\n",
    "            self.cache_k = tf.Variable(tf.zeros(shape), trainable=False)\n",
    "            self.cache_v = tf.Variable(tf.zeros(shape), trainable=False)\n",
    "\n",
    "    def update(self, start, xk, xv):\n",
    "        B, T, _, _ = xk.shape\n",
    "\n",
    "        # print(\n",
    "        #     f'KVCache.update({B}, {T}):: {xk.shape=} {xv.shape=}'\n",
    "        # )\n",
    "\n",
    "        self.cache_k[:B, start:start+T].assign(xk)\n",
    "        self.cache_v[:B, start:start+T].assign(xv)\n",
    "\n",
    "    def get(self, batch_size, start, seqlen):\n",
    "        # print(\n",
    "        #     f'KVCache.get({batch_size}, {start}, {seqlen})'\n",
    "        #     f'{self.cache_k.shape=} {self.cache_v.shape=}'\n",
    "        # )\n",
    "\n",
    "        keys = self.cache_k[:batch_size, :start+seqlen]\n",
    "        values = self.cache_v[:batch_size, :start+seqlen]\n",
    "\n",
    "        return keys, values\n",
    "\n",
    "def update_and_show(cache, batch_size, start, data_shape, msg, debug=False):\n",
    "    xk = np.random.rand(batch_size, 1, *data_shape)\n",
    "    xv = np.random.rand(batch_size, 1, *data_shape)\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            f'\\n{msg}::xk:\\n{xk}'\n",
    "            f'\\n{msg}::xv:\\n{xv}'\n",
    "        )\n",
    "\n",
    "    cache.update(start, xk, xv)\n",
    "    print(\n",
    "        f'\\n{msg}::key:\\n{cache.cache_k}'\n",
    "        f'\\n{msg}::value:\\n{cache.cache_v}'\n",
    "    )\n",
    "\n",
    "# cache_size, block_size, heads, head_dims = 1, 8, 2, 4\n",
    "# cache = KVCache(cache_size, block_size, heads, head_dims)\n",
    "# data_shape = (head_dims,)\n",
    "\n",
    "# print(f'\\nInitial:\\n{cache.cache_k}')\n",
    "\n",
    "# # update_and_show(cache, cache_size, 2, 4, data_shape, 'InUpdate(2, 4)')\n",
    "# # update_and_show(cache, cache_size, 4, 4, data_shape, 'EndUpdate(4, 4)')\n",
    "# # update_and_show(cache, cache_size, 6, 4, data_shape, 'SpilledUpdate(6, 4)', debug=True)\n",
    "# update_and_show(cache, cache_size, 0, data_shape, 'Update(0)', debug=True)\n",
    "# print(\n",
    "#     # f'\\nInQuery:\\n{cache.get(cache_size, 0, 4)[0]}'\n",
    "#     # f'\\n\\nEndQuery:\\n{cache.get(cache_size, 4, 4)[0]}'\n",
    "#     f'\\n\\nQuery::key:\\n{cache.get(cache_size, 0, 2)[0]}'\n",
    "#     f'\\n\\nQuery::value:\\n{cache.get(cache_size, 0, 2)[1]}'\n",
    "# )\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class GroupedQueryAttention(tf.keras.Model):\n",
    "    def __init__(self, cache_size, block_size, heads, kv_heads, dims):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.head_size = dims // heads\n",
    "        self.kv_heads = kv_heads or heads\n",
    "        \n",
    "        self.query = tf.keras.layers.Dense(dims, use_bias=False)\n",
    "        self.key = tf.keras.layers.Dense(kv_heads*self.head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(kv_heads*self.head_size, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(cache_size, block_size, self.kv_heads, self.head_size)\n",
    "        self.rope = RotaryPositionalEncodings(block_size, self.head_size)\n",
    "        \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        shape = tf.shape(x)\n",
    "        B, T = shape[0], shape[1]\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # (B, T, dims) -> (B, T, heads/kv_heads, head_dims)\n",
    "        q = tf.reshape(q, [B, T, self.heads, self.head_size])\n",
    "        k = tf.reshape(k, [B, T, self.kv_heads, self.head_size])\n",
    "        v = tf.reshape(v, [B, T, self.kv_heads, self.head_size])\n",
    "        \n",
    "        # RoPE expects inputs in (B, heads/kv_heads, T, head_size) format.\n",
    "        # Transpose(B, T, heads/kv_heads, head_size) -> (B, heads/kv_heads, T, head_size)\n",
    "        q = self.rope(tf.transpose(q, perm=[0, 2, 1, 3]), start=start)\n",
    "        k = self.rope(tf.transpose(k, perm=[0, 2, 1, 3]), start=start)\n",
    "        \n",
    "        if inference:\n",
    "            # Update KV cache\n",
    "            # KV cache expects inputs in (B, T, heads/kv_heads, head_size) format.\n",
    "            self.cache.update(\n",
    "                start,\n",
    "                tf.transpose(k, perm=[0, 2, 1, 3]),\n",
    "                v,\n",
    "            )\n",
    "            \n",
    "            # Get prefix context from the cache.\n",
    "            # (B, start+T, ...)\n",
    "            k, v = self.cache.get(B, start, T)\n",
    "            \n",
    "            ## Replicate KV heads to match query heads.\n",
    "            # (B, start+T, heads/kv_heads, head_size) -> (B, start+T, heads, head_size)\n",
    "            k = tf.tile(k, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "            \n",
    "            # (B, T, heads/kv_heads, head_dims) -> (B, T, heads, head_dims)\n",
    "            v = tf.tile(v, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "            \n",
    "            # (B, heads/kv_heads, T, head_size) @ (B, heads/kv_heads, head_size, start+T)\n",
    "            # -> (B, heads/kv_heads, T, start+T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 2, 3, 1])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "            wei = tf.nn.softmax(wei)\n",
    "        else:\n",
    "            assert start == 0\n",
    "            \n",
    "            ## Replicate KV heads to match query heads.\n",
    "            # (B, heads/kv_heads, T, head_size) -> (B, heads, T, head_size)\n",
    "            k = tf.tile(k, multiples=(1, self.heads//self.kv_heads, 1, 1))\n",
    "            \n",
    "            # (B, T, heads/kv_heads, head_dims) -> (B, T, heads, head_dims)\n",
    "            v = tf.tile(v, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "            \n",
    "            # (B, heads/kv_heads, T, head_size) @ (B, heads/kv_heads, head_size, start+T)\n",
    "            # -> (B, heads/kv_heads, T, start+T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 1, 3, 2])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "            tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "            wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "\n",
    "        # (B, heads/kv_heads, T, start+T) @ (B, heads/kv_heads, start+T, head_dims)\n",
    "        # -> (B, heads/kv_heads, T, head_dims)\n",
    "        out = wei @ tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # (B, heads/kv_heads, T, head_dims) -> (B, T, heads/kv_heads, head_dims)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return tf.reshape(out, shape=(B, T, -1))\n",
    "\n",
    "# l = GroupedQueryAttention(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=16,\n",
    "#     cache_size=4\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C), (B, T, C))\n",
    "# output = l(x, start)\n",
    "\n",
    "# B_i, T_i, C_i = 0, 0, 3\n",
    "# H_i, F_i = 0, C_i\n",
    "\n",
    "# print(\n",
    "#     f'{x[B_i]=}'\n",
    "#     f'\\n{l.cache.cache_k.shape=}\\n{l.cache.cache_k[B_i, :start+T]=}'\n",
    "#     f'\\n{l.cache.cache_v[B_i, :start+T]=}'\n",
    "# )\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SlidingWindowAttention(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.head_size = config.dims // config.heads\n",
    "        \n",
    "        self.query = tf.keras.layers.Dense(config.dims, use_bias=False)\n",
    "        self.key = tf.keras.layers.Dense(config.kv_heads*self.head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(config.kv_heads*self.head_size, use_bias=False)\n",
    "        \n",
    "        self.rope = RotaryPositionalEncodings(config.block_size, self.head_size)\n",
    "    \n",
    "    def as_strided(self, x):\n",
    "        shape = tf.shape(x)\n",
    "        B, T = shape[0], shape[2]\n",
    "\n",
    "        # (B, heads/kv_heads, T, head_size) -> (B, heads/kv_heads, T+2*window_size, head_size)\n",
    "        padded_x = tf.pad(x, [[0, 0], [0, 0], [self.config.window_size]*2, [0, 0]])\n",
    "\n",
    "        # indices.shape = (T, window_size + 1)\n",
    "        indices = tf.tile(tf.reshape(tf.range(T), (-1, 1)), [1, self.config.window_size + 1])\n",
    "        indices = indices + tf.expand_dims(tf.range(self.config.window_size + 1), axis=0)\n",
    "\n",
    "        # (B, heads/kv_heads, T, window_size + 1)\n",
    "        strided_x = tf.gather(padded_x, indices, axis=2)\n",
    "\n",
    "#         print(\n",
    "#             f'{x.shape=}'\n",
    "#             f'\\n{indices=}'\n",
    "#             f'\\n{padded_x.shape=}'\n",
    "#             f'{strided_x=}'\n",
    "#             f'\\n{x=}'\n",
    "#         )\n",
    "\n",
    "        return strided_x\n",
    "    \n",
    "    def as_grid(self, x):\n",
    "        # (B, heads, T, window_size+1) -> (B, heads, T, T)\n",
    "        diagonals = tf.transpose(x, perm=[0, 1, 3, 2])[..., ::-1, :]\n",
    "        grid_x = tf.linalg.diag(diagonals, k=(-self.config.window_size, 0), align='RIGHT_RIGHT')\n",
    "\n",
    "        # print(\n",
    "        #     f'\\n{x.shape=} {diagonals.shape=}'\n",
    "        #     f'\\n{grid_x.shape=}'\n",
    "        #     f'\\n{x[0, 0]=}'\n",
    "        #     # f'\\n{x[0, 0, -2:]=}'\n",
    "        #     f'\\n{grid_x[0, 0]=}'\n",
    "        #     # f'\\n{grid_x[0, 0, -2:]=}'\n",
    "        #     f'\\n{tf.math.reduce_sum(x[..., 0, 0])=}'\n",
    "        #     f'\\n{tf.reduce_all(x[..., 1, :] == grid_x[..., 1, :2])=}'\n",
    "        #     f'\\n{tf.reduce_all(x[..., -1, :] == grid_x[..., -1, -2:])=}'\n",
    "        # )\n",
    "\n",
    "        return grid_x\n",
    "        \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        shape = tf.shape(x)\n",
    "        B, T = shape[0], shape[1]\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # (B, T, dims) -> (B, T, heads/kv_heads, head_dims)\n",
    "        q = tf.reshape(q, [B, T, self.config.heads, self.head_size])\n",
    "        k = tf.reshape(k, [B, T, self.config.kv_heads, self.head_size])\n",
    "        v = tf.reshape(v, [B, T, self.config.kv_heads, self.head_size])\n",
    "        \n",
    "        # RoPE expects inputs in (B, heads/kv_heads, T, head_size) format.\n",
    "        # Transpose(B, T, heads/kv_heads, head_size) -> (B, heads/kv_heads, T, head_size)\n",
    "        q = self.rope(tf.transpose(q, perm=[0, 2, 1, 3]), start=start)\n",
    "        k = self.rope(tf.transpose(k, perm=[0, 2, 1, 3]), start=start)\n",
    "            \n",
    "        ## Replicate KV heads to match query heads.\n",
    "        # (B, heads/kv_heads, T, head_size) -> (B, heads, T, head_size)\n",
    "        k = tf.tile(k, multiples=(1, self.config.heads//self.config.kv_heads, 1, 1))\n",
    "        \n",
    "        # (B, T, heads/kv_heads, head_dims) -> (B, T, heads, head_dims)\n",
    "        v = tf.tile(v, multiples=(1, 1, self.config.heads//self.config.kv_heads, 1))\n",
    "\n",
    "        # Compose strided keys to mimic the sliding window. \n",
    "        # (B, heads, T, head_size) -> (B, heads, T, window_size+1, head_size)\n",
    "        strided_k = self.as_strided(k)\n",
    "\n",
    "        # (B, heads, T, head_size) @ (B, heads, T, window_size+1, head_size)\n",
    "        # -> (B, heads, T, window_size+1)\n",
    "        wei = tf.einsum('bhtd,bhtxd->bhtx', q, strided_k)\n",
    "\n",
    "        # (B, heads, T, window_size+1) -> (B, heads, T, T)\n",
    "        wei = self.as_grid(wei)\n",
    "        wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "        tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "\n",
    "        # (B, heads, T, T) @ (B, heads, T, head_dims)\n",
    "        # -> (B, heads, T, head_dims)\n",
    "        out = wei @ tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # (B, heads, T, head_dims) -> (B, T, heads, head_dims)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return tf.reshape(out, shape=(B, T, -1))\n",
    "\n",
    "# l = SlidingWindowAttention(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=32,\n",
    "#     window_size=2\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 32\n",
    "# x = tf.cast(tf.reshape(tf.range(B*T*C), (B, T, C)), dtype=tf.float32)\n",
    "# output = l(x)\n",
    "\n",
    "# B_i, T_i, C_i = 0, 0, 3\n",
    "# H_i, F_i = 0, C_i\n",
    "\n",
    "# print(\n",
    "#     f'{x[B_i]=}'\n",
    "#     f'{output.shape=}'\n",
    "# )\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, cache_size, block_size, head_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        # Input args\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(cache_size, block_size, head_size)\n",
    "        self.rope = RotaryPositionalEncodings(block_size, head_size)\n",
    "\n",
    "    def call(self, x, start=0, inference=False):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q = self.rope(q, start=start)\n",
    "        k = self.rope(k, start=start)\n",
    "        \n",
    "        if inference:\n",
    "            # Update KV cache\n",
    "            self.cache.update(start, k, v)\n",
    "            \n",
    "            # Get prefix context from the cache.\n",
    "            # (B, start+T, head_dims)\n",
    "            k, v = self.cache.get(B, start, T)\n",
    "            \n",
    "            # (B, T, head_size) @ (B, head_size, start+T) --> (B, T, start+T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 2, 1])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "            wei = tf.nn.softmax(wei)\n",
    "        else:\n",
    "            assert start == 0\n",
    "            \n",
    "            # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "            wei = q @ tf.transpose(k, perm=[0, 2, 1])\n",
    "            wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "            tril = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "            wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "            \n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, cache_size, block_size, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.attn_layers = [SelfAttentionLayer(cache_size, block_size, head_size) for i in range(num_heads)]\n",
    "\n",
    "    def call(self, x, start=0, inference=False):\n",
    "        return tf.concat([\n",
    "            attn_layer(x, start=start, inference=inference) for attn_layer in self.attn_layers\n",
    "        ], axis=-1)\n",
    "\n",
    "# cache_size, block_size, num_heads, head_size = 2, 16, 1, 16\n",
    "# B, T, C = 2, 2, num_heads*head_size\n",
    "\n",
    "# l = MultiHeadAttentionLayer(cache_size, block_size, num_heads, head_size)\n",
    "\n",
    "# for start in range(T):\n",
    "#     print(\n",
    "#         f'\\n{start=}::{l(tf.random.uniform((B, 1, C)), start=start, inference=True).shape}\\n'\n",
    "#     )\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RMSNorm(layers.Layer):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self, shape):\n",
    "        self.w = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=(1, shape[-1]),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "    \n",
    "    def norm(self, x):\n",
    "        return x*tf.math.rsqrt(tf.math.reduce_mean(x**2, axis=-1, keepdims=True) + self.eps)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.w * self.norm(x)\n",
    "\n",
    "# data = tf.constant(np.arange(20).reshape(5, 2, 2) * 10, dtype=tf.float32)\n",
    "\n",
    "# l=RMSNorm()\n",
    "# l.build(data.shape)\n",
    "# l(data)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, dims):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.hidden_dims = 4*dims\n",
    "        self.linear_1 = layers.Dense(self.hidden_dims, use_bias=False, activation='swish')\n",
    "        self.linear_2 = layers.Dense(self.hidden_dims, use_bias=False)\n",
    "        self.linear_3 = layers.Dense(dims, use_bias=False)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # (..., dims) -> (..., hidden_dims)\n",
    "        x = self.linear_1(x)*self.linear_2(x)\n",
    "        \n",
    "        # (..., hidden_dims) -> (..., dims)\n",
    "        x = self.linear_3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# l = FeedForward(C)\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class AttentionSelector(object):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(AttentionSelector, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.config = config\n",
    "    \n",
    "    @property\n",
    "    def choice(self):\n",
    "        return (self.config.attention or 'msa').lower()\n",
    "    \n",
    "    def select(self):\n",
    "        if self.choice in ['slidingwindow', 'slidingwindowattention', 'swa']:\n",
    "            return SlidingWindowAttention(self.config)\n",
    "        elif self.choice in ['groupedquery', 'groupedqueryattention', 'gqa']:\n",
    "            return GroupedQueryAttention(self.config)\n",
    "        else:\n",
    "            return MultiHeadAttentionLayer(self.config)\n",
    "    \n",
    "    def call(self, l, x, start=0, inference=False):\n",
    "        if self.choice in ['slidingwindow', 'slidingwindowattention', 'swa']:\n",
    "            return l(x)\n",
    "        elif self.choice in ['groupedquery', 'groupedqueryattention', 'gqa']:\n",
    "            return l(x, start=start, inference=inference)\n",
    "        else:\n",
    "            return l(x, start=start, inference=inference)\n",
    "\n",
    "class NormSelector(object):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(NormSelector, self).__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "    \n",
    "    def select(self):\n",
    "        choice = (self.config.norm or 'rms').lower()\n",
    "        if choice in ['batchnorm', 'bn', 'batchnormalization']:\n",
    "            return layers.BatchNormalization()\n",
    "        else:\n",
    "            return RMSNorm()\n",
    "\n",
    "class LlamaBlock(tf.keras.Model):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(LlamaBlock, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.norm_selector = NormSelector(config)\n",
    "        self.attention_selector = AttentionSelector(config)\n",
    "        \n",
    "        self.norm_1 = self.norm_selector.select()\n",
    "        self.attention = self.attention_selector.select()\n",
    "        self.norm_2 = self.norm_selector.select()\n",
    "    \n",
    "    def call(self, x, start, inference):\n",
    "        x += self.attention_selector.call(\n",
    "            self.attention,\n",
    "            self.norm_1(x), start=start, inference=inference\n",
    "        )\n",
    "        x = self.norm_2(x)\n",
    "#         x += self.feed_forward(self.norm_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# l = LlamaBlock(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=16,\n",
    "#     cache_size=4\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, metrics\n",
    "from functools import reduce\n",
    "\n",
    "class LlamaModel(tf.keras.Model):\n",
    "    def __init__(self, config, loss_fn, *args, **kwargs):\n",
    "        super(LlamaModel, self).__init__(*args, **kwargs)\n",
    "        # Args\n",
    "        self.config = config\n",
    "\n",
    "        # Model elements\n",
    "        self.embeddings = layers.Embedding(config.vocab_size, config.embed_dims)\n",
    "        self.pos_embeddings = layers.Embedding(config.block_size, config.embed_dims) if config.pos_embeddings else None\n",
    "        self.dec_blocks = [LlamaBlock(config) for _ in range(config.decoders)]\n",
    "\n",
    "        self.head = layers.Dense(config.vocab_size, use_bias=False)\n",
    "\n",
    "        # Loss \n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        # Metrics\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "    \n",
    "    def alternating_reduce(self, fn, blocks, x):\n",
    "        for b_index, block in enumerate(blocks):\n",
    "            # Pick the sub-block for transformer branch\n",
    "            pivot = b_index % self.config.alternate_blocks\n",
    "\n",
    "            # Split input into subblocks\n",
    "            xs = tf.split(x, self.config.alternate_blocks, axis=-1)\n",
    "\n",
    "            # Process picked block\n",
    "            x = fn(xs[pivot], block)\n",
    "\n",
    "            # Re-compose\n",
    "            x = tf.concat([*xs[:pivot], x, *xs[(pivot + 1):]], axis=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        x_pos_embed = self.pos_embeddings(tf.range(T)) if self.pos_embeddings else None\n",
    "        x_embed = self.embeddings(x)\n",
    "        \n",
    "        x = x_embed + x_pos_embed if self.pos_embeddings else x_embed\n",
    "        \n",
    "        reduce_fn = self.alternating_reduce if self.config.alternate_blocks else reduce\n",
    "        \n",
    "        x = reduce_fn(\n",
    "            lambda y,dec_block: dec_block(y, start=start, inference=inference),\n",
    "            self.dec_blocks,\n",
    "            x\n",
    "        )\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        y = tf.cast(y, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X)\n",
    "            loss = self.loss_fn(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return self.record(loss, y, logits)\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        X, y = data\n",
    "\n",
    "        logits = self(X)\n",
    "        loss = self.loss_fn(y, logits)\n",
    "    \n",
    "        return self.record(loss, y, logits)\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "    \n",
    "    def record(self, loss, y, logits):\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=len(model_vocab),\n",
    "    \n",
    "    # Model Size and Prediction Capacity\n",
    "    decoders=4,\n",
    "    block_size=64,\n",
    "    embed_dims=512,\n",
    "    dims=128,\n",
    "    \n",
    "    # Attention Params\n",
    "    attention='swa',\n",
    "    heads=8,\n",
    "    kv_heads=8,\n",
    "    \n",
    "    # KV Cache\n",
    "    cache_size=2,\n",
    "    \n",
    "    # Sliding Window Attention\n",
    "    window_size=3,\n",
    "    \n",
    "    # Features\n",
    "    pos_embeddings=False,\n",
    "    alternate_blocks=4,\n",
    ")\n",
    "\n",
    "llama = LlamaModel(config=model_config, loss_fn=loss_fn)\n",
    "llama(tf.random.uniform((2, llama.config.block_size), minval=0, maxval=llama.config.vocab_size, dtype=tf.int32))\n",
    "llama.compile(optimizer=optimizer, loss=loss_fn)\n",
    "llama.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama = LlamaModel(config=model_config, loss_fn=loss_fn)\n",
    "\n",
    "# config_data = model_config._asdict()\n",
    "# config_data['kv_heads'] = config_data['kv_heads']//grouping_factor\n",
    "\n",
    "# grouped_config = ModelConfig(**config_data)\n",
    "\n",
    "# llama_grouped.compile(optimizer=optimizer, loss=loss_fn)\n",
    "# llama_grouped(tf.random.uniform((2, 5), minval=0, maxval=llama_grouped.vocab_size, dtype=tf.int32))\n",
    "# llama_grouped.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_stats(text, tokenizer, fns=[]):\n",
    "    def stats_fn(token):\n",
    "        return np.array([token.lower_] + [fn(token) for fn in fns])\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    stats = np.stack([stats_fn(token) for token in tokens])\n",
    "\n",
    "    return stats\n",
    "\n",
    "def random_token_from_logits(logits, samples=1):\n",
    "    return tf.random.categorical(logits, samples, dtype=tf.int32)\n",
    "\n",
    "def argmax_token(logits, samples=1):\n",
    "    return tf.expand_dims(\n",
    "        tf.math.argmax(tf.math.softmax(logits), axis=-1, output_type=tf.int32),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "def generate(model, token_idx, tokens, randomize=True):\n",
    "    sequence = token_idx\n",
    "    for start in range(tokens - 1):\n",
    "        logits = model(token_idx, start=start, inference=True)[:, -1, :]\n",
    "#         token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        token_idx = random_token_from_logits(logits) if randomize else argmax_token(logits)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def generate_no_cache(model, token_idx, tokens, randomize=True):\n",
    "    sequence = token_idx\n",
    "    for _ in range(tokens - 1):\n",
    "        logits = model(sequence[:, -model.block_size:])[:, -1, :]\n",
    "#         token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        token_idx = random_token_from_logits(logits) if randomize else argmax_token(logits)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def generate_and_evaluate(model_vocab, generator, model, tokens=50, randomize=True):\n",
    "    idx_to_char = {i:c for c,i in model_vocab.items()}\n",
    "    decoder = lambda x: ''.join([idx_to_char[i] for i in x])\n",
    "\n",
    "    starter = tf.constant([model_vocab[' ']], shape=(1, 1), dtype=tf.int32)\n",
    "    generated_text = decoder(\n",
    "        generator(\n",
    "            model,\n",
    "            starter,\n",
    "            tokens=tokens,\n",
    "            randomize=randomize,\n",
    "        )[0].numpy()\n",
    "    )\n",
    "\n",
    "    print(f'{generator.__name__}:: {generated_text=}')\n",
    "\n",
    "# word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "# print(\n",
    "#     f'\\nVocab: {list(word_vocab.keys())[:5]}'\n",
    "# )\n",
    "# generate_and_evaluate(model_vocab, generate, llama, randomize=False, tokens=min(50, llama.block_size))\n",
    "# generate_and_evaluate(model_vocab, generate_no_cache, llama, randomize=False, tokens=min(50, llama.block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_layers(model, exceptions=[]):\n",
    "    def fn(l):\n",
    "        if hasattr(l, 'layers'):\n",
    "            return list(itertools.chain(*map(fn, l.layers)))\n",
    "        elif l.name in exceptions:\n",
    "            return [None]\n",
    "        else:\n",
    "            return [l]\n",
    "    \n",
    "    return list(filter(lambda x: x is not None, fn(model)))\n",
    "\n",
    "\n",
    "def apply_groupings(src_model, target_model, factor=2):\n",
    "    def grouped_apply(l1, l2):\n",
    "        grouped_w1s = []\n",
    "\n",
    "        for w1, w2 in zip(l1.get_weights(), l2.get_weights()):\n",
    "            w1_groups = tf.split(w1, factor, axis=-1)\n",
    "            print(f'\\nSplit {src_model.name}.{l1.name}({w1.shape}) --> {factor}x{w1_groups[0].shape}')\n",
    "\n",
    "            w1_grouped = tf.math.add(*w1_groups) / factor\n",
    "            print(f'Average {factor}x{w1_groups[0].shape} --> {w1_grouped.shape}')\n",
    "\n",
    "            grouped_w1s.append(w1_grouped)\n",
    "        \n",
    "        # Apply grouped weights to the target\n",
    "        l2.set_weights(grouped_w1s)\n",
    "        print(f'Copied? {np.array_equal(grouped_w1s, l2.get_weights())}')\n",
    "\n",
    "    src_attention = src_model.dec_blocks[0].attention\n",
    "    target_attention = target_model.dec_blocks[0].attention\n",
    "\n",
    "    src_other_layers = get_layers(\n",
    "        src_model,\n",
    "        exceptions=list(map(lambda x: x.name, [src_attention.key, src_attention.value]))\n",
    "    )\n",
    "\n",
    "    target_other_layers = get_layers(\n",
    "        target_model,\n",
    "        exceptions=list(map(lambda x: x.name, [target_attention.key, target_attention.value]))\n",
    "    )\n",
    "\n",
    "    for src_layer, target_layer in zip(src_other_layers, target_other_layers):\n",
    "        target_layer.set_weights(src_layer.get_weights())\n",
    "    \n",
    "    # Group keys and values.\n",
    "    print(f'\\n Grouping Layers ({factor=})\\n============================')\n",
    "    grouped_apply(src_attention.key, target_attention.key)\n",
    "    grouped_apply(src_attention.value, target_attention.value)\n",
    "\n",
    "def compare_weights(one, two):\n",
    "    one_layers = get_layers(one)\n",
    "    two_layers = get_layers(two)\n",
    "\n",
    "    print('\\n Comparison Status\\n===================')\n",
    "\n",
    "    for ol, tl in filter(lambda x: x[0].get_weights(),  zip(one_layers, two_layers)):\n",
    "        if np.array_equal(ol.get_weights(), tl.get_weights()):\n",
    "            print(f'{ol.name} == {tl.name}')\n",
    "        else:\n",
    "            print(f'\\t{ol.name} != {tl.name}')\n",
    "\n",
    "# apply_groupings(llama, llama_grouped)\n",
    "# compare_weights(llama, llama_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Grouping Layers (factor=2)\n",
      "============================\n",
      "\n",
      "Split llama_model_37.dense_384((128, 128)) --> 2x(128, 64)\n",
      "Average 2x(128, 64) --> (128, 64)\n",
      "Copied? True\n",
      "\n",
      "Split llama_model_37.dense_385((128, 128)) --> 2x(128, 64)\n",
      "Average 2x(128, 64) --> (128, 64)\n",
      "Copied? True\n",
      "fn:: generated_text=' sssawaksedreerederedoubexfoctleereere:rere:reree:'\n"
     ]
    }
   ],
   "source": [
    "def get_grouped_generator(grouped_model, grouping_factor, base_fn=generate_no_cache):\n",
    "    def fn(model, *args, **kwargs):\n",
    "        apply_groupings(model, grouped_model)\n",
    "        return base_fn(grouped_model, *args, **kwargs)\n",
    "    \n",
    "    return fn\n",
    "\n",
    "generator_grouped = get_grouped_generator(llama_grouped, grouping_factor=grouping_factor)\n",
    "generator_grouped_cached = get_grouped_generator(llama_grouped, grouping_factor=grouping_factor, base_fn=generate)\n",
    "\n",
    "generate_and_evaluate(model_vocab, generator, llama, randomize=False, tokens=min(50, llama.block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei(1)::False [[[21.616919 43.966854]]] [[[21.616917 43.966858]]] (1, 1, 2) (1, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# llama.layers[2].layers[1].attn_layers[0].logs['cache']['k']\n",
    "cache_logs = llama.layers[2].layers[1].attn_layers[0].logs['cache']\n",
    "no_cache_logs = llama.layers[2].layers[1].attn_layers[0].logs['no-cache']\n",
    "\n",
    "log_items = len(cache_logs['k'])\n",
    "\n",
    "# for key in ['in_x', 'k', 'q', 'v', 'rope_k', 'rope_q', 'wei', 'out']:\n",
    "for key in ['wei']:\n",
    "    for idx in [1]:\n",
    "        print(\n",
    "            f'{key}({idx})::{tf.math.reduce_all(cache_logs[key][idx] == no_cache_logs[key][idx+1][..., idx:, :])}'\n",
    "            f' {cache_logs[key][idx]} {no_cache_logs[key][idx+1][..., idx:, :]}'\n",
    "            f' {cache_logs[key][idx].shape} {no_cache_logs[key][idx+1][..., idx:, :].shape}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True \n",
      "True \n",
      "cache_logs['wei'][index]=<tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[21.616919, 43.966854]]], dtype=float32)>\n",
      "no_cache_logs['wei'][index+1]=<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
      "array([[[29.863405 ,  7.2788296],\n",
      "        [21.616917 , 43.966858 ]]], dtype=float32)>\n",
      "cache_logs['wei'][index]=<tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[21.616919, 43.966854]]], dtype=float32)>\n",
      "no_cache_logs['wei'][index+1]=<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
      "array([[[29.863405 ,  7.2788296],\n",
      "        [21.616917 , 43.966858 ]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# tf.math.reduce_all(cache_logs['cache_k'][1] == no_cache_logs['rope_k'][2])\n",
    "index = 1\n",
    "print(\n",
    "    f\"{tf.math.reduce_all(cache_logs['wei'][index] == no_cache_logs['wei'][index+1])}\"\n",
    "    # f\"\\n{tf.math.reduce_all(cache_logs['cache_v'][1] == no_cache_logs['v'][2])}\"\n",
    "    f\"\\n{tf.math.reduce_all(cache_logs['cache_k'][index] == no_cache_logs['rope_k'][index+1])}\",\n",
    "    # f\"\\n{tf.math.reduce_all(cache_logs['cache_v'][index] == no_cache_logs['v'][index+1][:, 1:, :])}\",\n",
    "    f\"\\n{tf.math.reduce_all(cache_logs['rope_q'][index] == no_cache_logs['rope_q'][index+1][:, 1:, :])}\",\n",
    "    # f\"\\n{cache_logs['rope_q'][index]=}\"\n",
    "    # f\"\\n{no_cache_logs['rope_q'][index+1]=}\"\n",
    "    # f\"\\n{cache_logs['cache_k'][index]=}\"\n",
    "    # f\"\\n{no_cache_logs['rope_k'][index+1]=}\"\n",
    "    f\"\\n{cache_logs['wei'][index]=}\"\n",
    "    f\"\\n{no_cache_logs['wei'][index+1]=}\"\n",
    "    f\"\\n{cache_logs['wei'][index]=}\"\n",
    "    f\"\\n{no_cache_logs['wei'][index+1]=}\"\n",
    ")\n",
    "# cache_logs['out'][1], no_cache_logs['out'][2][..., 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 16), dtype=float32, numpy=\n",
       " array([[[ 2.697421  , -2.4562492 , -1.5515298 ,  0.14776081,\n",
       "           0.37695628, -1.5206785 ,  0.8845265 ,  0.17617644,\n",
       "          -2.1475816 ,  1.1095126 , -0.10803113, -4.174893  ,\n",
       "           0.12987702,  0.36463377, -4.113693  , -0.7377639 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2, 16), dtype=float32, numpy=\n",
       " array([[[ 4.474342  , -1.4330535 , -4.977524  , -4.9114738 ,\n",
       "          -0.23111431,  1.3314574 ,  0.03826439,  1.3454266 ,\n",
       "           1.7946535 ,  0.05863512, -1.4367882 , -2.3322222 ,\n",
       "           0.93547916, -2.7148142 ,  0.902055  ,  0.88373613],\n",
       "         [ 1.8138299 , -2.927385  , -3.3529627 , -4.1700516 ,\n",
       "          -0.463976  , -2.7048278 , -1.8990085 ,  1.7266546 ,\n",
       "           1.0458257 , -0.78157973, -0.0542576 , -2.8697302 ,\n",
       "           2.9209538 , -0.50924057, -3.9430103 ,  0.7489088 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[21.616919, 43.966854]]], dtype=float32)>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_q = cache_logs['rope_q'][index]\n",
    "c_k = cache_logs['cache_k'][index]\n",
    "\n",
    "c_q, c_k, c_q @ tf.transpose(c_k, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2, 16), dtype=float32, numpy=\n",
       " array([[[-0.23348725, -1.2534161 , -3.0547209 , -0.6355625 ,\n",
       "          -0.08479708,  1.8937602 ,  1.8187456 , -1.0931925 ,\n",
       "          -0.7623807 ,  2.3252077 , -0.7686442 , -1.8549696 ,\n",
       "           0.79256994, -1.0165763 ,  1.2038559 ,  0.9727511 ],\n",
       "         [ 2.697421  , -2.4562492 , -1.5515298 ,  0.14776081,\n",
       "           0.37695628, -1.5206785 ,  0.8845265 ,  0.17617644,\n",
       "          -2.1475816 ,  1.1095126 , -0.10803113, -4.174893  ,\n",
       "           0.12987702,  0.36463377, -4.113693  , -0.7377639 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2, 16), dtype=float32, numpy=\n",
       " array([[[ 4.474342  , -1.4330535 , -4.977524  , -4.9114738 ,\n",
       "          -0.23111431,  1.3314574 ,  0.03826439,  1.3454266 ,\n",
       "           1.7946535 ,  0.05863512, -1.4367882 , -2.3322222 ,\n",
       "           0.93547916, -2.7148142 ,  0.902055  ,  0.88373613],\n",
       "         [ 1.8138299 , -2.927385  , -3.3529627 , -4.1700516 ,\n",
       "          -0.463976  , -2.7048278 , -1.8990085 ,  1.7266546 ,\n",
       "           1.0458257 , -0.78157973, -0.0542576 , -2.8697302 ,\n",
       "           2.9209538 , -0.50924057, -3.9430103 ,  0.7489088 ]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\n",
       " array([[[29.863405 ,  7.2788296],\n",
       "         [21.616917 , 43.966858 ]]], dtype=float32)>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_q = no_cache_logs['rope_q'][index + 1]\n",
    "nc_k = no_cache_logs['rope_k'][index + 1]\n",
    "\n",
    "nc_q, nc_k, nc_q @ tf.transpose(nc_k, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
      "array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
      "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
      "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
      "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
      "         44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
      "         55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
      "         66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
      "         77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
      "         88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
      "         99.],\n",
      "       [100., 101., 102., 103., 104., 105., 106., 107., 108., 109., 110.,\n",
      "        111., 112., 113., 114., 115., 116., 117., 118., 119., 120., 121.,\n",
      "        122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,\n",
      "        133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,\n",
      "        144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,\n",
      "        155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,\n",
      "        166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176.,\n",
      "        177., 178., 179., 180., 181., 182., 183., 184., 185., 186., 187.,\n",
      "        188., 189., 190., 191., 192., 193., 194., 195., 196., 197., 198.,\n",
      "        199.]], dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
       "array([[ 61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250.,  61250.,  61250.,  61250.,  61250.,  61250.,  61250.,\n",
       "         61250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250., 186250., 186250., 186250., 186250., 186250.,\n",
       "        186250., 186250.],\n",
       "       [311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 311250., 311250., 311250., 311250., 311250., 311250.,\n",
       "        311250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250., 436250., 436250., 436250., 436250., 436250.,\n",
       "        436250., 436250.]], dtype=float32)>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "K = 2\n",
    "\n",
    "def alternating_reduce(fn, blocks, x):\n",
    "    for b_index, block in enumerate(blocks):\n",
    "        # Pick the sub-block for transformer branch\n",
    "        pivot = b_index % K\n",
    "\n",
    "        # Split input into subblocks\n",
    "        xs = tf.split(x, K, axis=-1)\n",
    "\n",
    "        # Process picked block\n",
    "        x = block(xs[pivot])\n",
    "\n",
    "        # Re-compose\n",
    "        x = tf.concat([*xs[:pivot], x, *xs[(pivot + 1):]], axis=-1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "x = tf.reshape(tf.cast(tf.range(2*100), dtype=tf.float32), (2, 100))\n",
    "\n",
    "print(f'{x=}')\n",
    "alternating_reduce(\n",
    "    lambda y,dec_block: dec_block(y),\n",
    "    [\n",
    "        # layers.Dense(100, use_bias=False),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "        layers.Dense(50, use_bias=False, kernel_initializer='ones'),\n",
    "    ],\n",
    "    x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AltUpBlockSequence(tf.keras.Model):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(AltUpBlockSequence, self).__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.blocks = [LlamaBlock(config) for _ in range(config.decoders)]\n",
    "    \n",
    "    def build(self, shape):\n",
    "        self.prediction_scalars = self.add_weight(\n",
    "            \"prediction_scalars\",\n",
    "            shape=(self.config.alternate_blocks,)*2,\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.correction_scalars = self.add_weight(\n",
    "            \"correction_scalars\",\n",
    "            shape=(self.config.alternate_blocks,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "    \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        for b_index, block in enumerate(self.blocks):\n",
    "            # Pick the sub-block for transformer branch\n",
    "            pivot = b_index % self.config.alternate_blocks\n",
    "\n",
    "            # Split input into subblocks\n",
    "            xs = tf.split(x, self.config.alternate_blocks, axis=-1)\n",
    "\n",
    "            # Prediction with a linear map\n",
    "            xs_hat = tf.unstack(\n",
    "                tf.einsum('aa,axyz->axyz', self.prediction_scalars, tf.stack(xs, axis=0)),\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            # Process picked block\n",
    "            x = block(xs[pivot], start=start, inference=inference)\n",
    "\n",
    "            # Correction\n",
    "            xs = list(map(\n",
    "                lambda idx: xs_hat[idx] + self.correction_scalars[idx]*(x - xs_hat[pivot]),\n",
    "                range(self.config.alternate_blocks),\n",
    "            ))\n",
    "\n",
    "            # Re-compose\n",
    "            # x = tf.concat([*xs[:pivot], x, *xs[(pivot + 1):]], axis=-1)\n",
    "            x = tf.concat(xs, axis=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "l = AltUpBlockSequence(model_config)\n",
    "x = tf.random.uniform((2, model_config.block_size, model_config.embed_dims))\n",
    "y = l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 512), dtype=float32, numpy=\n",
       " array([[0.31179297, 0.8263413 , 0.6849456 , ..., 0.79634094, 0.16588712,\n",
       "         0.7652644 ],\n",
       "        [0.9043931 , 0.09195149, 0.25456262, ..., 0.10521233, 0.29894042,\n",
       "         0.96700084],\n",
       "        [0.18097997, 0.90534747, 0.6530881 , ..., 0.810333  , 0.8480791 ,\n",
       "         0.68864477],\n",
       "        ...,\n",
       "        [0.9133718 , 0.3668282 , 0.8784882 , ..., 0.57490706, 0.8577874 ,\n",
       "         0.02953506],\n",
       "        [0.06336856, 0.5252453 , 0.8964504 , ..., 0.12895262, 0.16139305,\n",
       "         0.5827892 ],\n",
       "        [0.5203084 , 0.2430867 , 0.10670376, ..., 0.82163894, 0.05010164,\n",
       "         0.4061122 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 512), dtype=float32, numpy=\n",
       " array([[-1.1757655 ,  0.22772452,  0.39361706, ...,  2.4383278 ,\n",
       "          0.5172272 , -0.91794693],\n",
       "        [-0.616339  , -0.24007252, -0.29521257, ...,  2.067838  ,\n",
       "          0.48718598, -0.9977868 ],\n",
       "        [-1.4486234 ,  0.33871815,  0.46358114, ...,  2.3470387 ,\n",
       "          0.48138243, -1.7389158 ],\n",
       "        ...,\n",
       "        [ 0.13750303,  0.6198484 ,  0.06040925, ...,  1.8629272 ,\n",
       "          0.68603754, -2.2597256 ],\n",
       "        [-0.55783045,  0.5971597 ,  0.08331817, ...,  1.381442  ,\n",
       "          0.23401473, -2.1203701 ],\n",
       "        [ 0.00863039,  0.27567112, -0.8299949 , ...,  1.8471062 ,\n",
       "          0.1796828 , -2.1189559 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 64, 128), dtype=float32, numpy=\n",
       "array([[[-0.36719808,  0.97809   , -0.03038533, ..., -0.30581978,\n",
       "         -0.1215362 ,  3.1138353 ],\n",
       "        [-0.8910546 ,  0.52530986,  0.37287   , ..., -0.5833475 ,\n",
       "          0.08942268,  3.0151086 ],\n",
       "        [-0.7777609 ,  0.37908828,  0.09426907, ..., -0.5609133 ,\n",
       "          0.11016361,  2.9316347 ],\n",
       "        ...,\n",
       "        [-0.71178305,  1.202099  ,  0.05243267, ..., -1.1422229 ,\n",
       "          0.4896912 ,  2.347373  ],\n",
       "        [-0.8180739 ,  0.9634706 ,  0.15713139, ..., -0.98911667,\n",
       "          0.5352649 ,  2.310747  ],\n",
       "        [-0.7443944 ,  0.99957556,  0.16659923, ..., -1.024197  ,\n",
       "          0.6366435 ,  2.0893376 ]],\n",
       "\n",
       "       [[-0.2841137 ,  1.5303981 ,  0.10613061, ..., -0.74273384,\n",
       "         -0.68495226,  2.1344197 ],\n",
       "        [-0.3547677 ,  1.3389255 ,  0.5335099 , ..., -0.65516317,\n",
       "         -0.28303394,  2.4078557 ],\n",
       "        [-0.3474855 ,  1.1259456 ,  0.2917909 , ..., -0.86325204,\n",
       "         -0.06728553,  2.332008  ],\n",
       "        ...,\n",
       "        [-0.57965225,  1.1753275 , -0.29459953, ..., -0.7792972 ,\n",
       "          0.3598435 ,  2.2636282 ],\n",
       "        [-0.7353044 ,  0.87159604, -0.04712372, ..., -0.9922649 ,\n",
       "          0.38433793,  2.4579532 ],\n",
       "        [-0.5458765 ,  1.1233443 , -0.12663522, ..., -0.8972338 ,\n",
       "          0.36012524,  2.258087  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class BlockSequence(tf.keras.Model):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(BlockSequence, self).__init__(*args, **kwargs)\n",
    "        self.config = config\n",
    "\n",
    "        self.blocks = [LlamaBlock(config) for _ in range(config.decoders)]\n",
    "    \n",
    "    def call(self, x, start=0, inference=False):\n",
    "        return reduce(\n",
    "            lambda y,block: block(y, start=start, inference=inference),\n",
    "            self.blocks,\n",
    "            x\n",
    "        )\n",
    "\n",
    "# config_data = model_config._asdict()\n",
    "# config_data['']\n",
    "\n",
    "# altup_config = ModelConfig(**config_data)\n",
    "\n",
    "l = BlockSequence(model_config)\n",
    "l(tf.random.uniform((2, model_config.block_size, model_config.dims)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
