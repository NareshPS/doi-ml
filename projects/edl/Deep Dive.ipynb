{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14], dtype=object)"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = nlp('and to')\n",
    "# ! pip install levenshtein\n",
    "\n",
    "from Levenshtein import distance as edit_distance\n",
    "\n",
    "def get_proximity_with_vocab(text, tokenizer, vocab):\n",
    "    def proximity_fn(word):\n",
    "        if word in vocab:\n",
    "            distance = 0\n",
    "        else:\n",
    "            distance = min(*map(\n",
    "                lambda vocab_word: edit_distance(word, vocab_word),\n",
    "                vocab.keys()\n",
    "            ))\n",
    "        \n",
    "        return distance\n",
    "\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    distance = reduce(lambda y,token: y+proximity_fn(str(token)), tokens, 0)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def get_all_proximity_with_vocab(texts, tokenizer, vocab):\n",
    "    return np.fromiter(\n",
    "        map(\n",
    "            lambda text: get_proximity_with_vocab(text, tokenizer, vocab),\n",
    "            texts,\n",
    "        ),\n",
    "        dtype=object\n",
    "    )\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "texts = [\n",
    "    'hello, how are you doing?',\n",
    "    'hello, how are you doing?',\n",
    "]\n",
    "\n",
    "get_all_proximity_with_vocab(texts, nlp, word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n"
     ]
    }
   ],
   "source": [
    "! head input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Set\n",
      "==============\n",
      "X.shape=TensorShape([5]) y.shape=TensorShape([5])\n",
      "X.numpy()=array([22,  8, 28, 28, 14]) y.numpy()=array([ 8, 28, 28, 14, 25])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "data = 'helloksdhglsadjgalsdhglaksddd'\n",
    "block_size = 5\n",
    "\n",
    "# ds = tf.data.Dataset.range(len(data) - block_size - 1).map(\n",
    "#     # lambda index: data[index:index+block_size], data[index+1:index+block_size+1]\n",
    "#     lambda index: data[index:index+block_size]\n",
    "# )\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "with open('input.txt') as f:\n",
    "    tiny_shakespere = list(f.readlines())\n",
    "\n",
    "char_frequencies = Counter(chain(*tiny_shakespere))\n",
    "char_vocab = dict(zip(char_frequencies.keys(), range(len(char_frequencies))))\n",
    "\n",
    "def make_dataset(data, block_size, vocab):\n",
    "    def indices(phrase):\n",
    "        return list(map(char_vocab.get, phrase))\n",
    "\n",
    "    @tf.numpy_function(Tout=(tf.int64, tf.int64))\n",
    "    def get_item(index):\n",
    "        X = data[index:index+block_size]\n",
    "        y = data[index+1:index+block_size+1]\n",
    "        return indices(X), indices(y) \n",
    "    \n",
    "    return tf.data.Dataset.range(len(data) - block_size).map(get_item)\n",
    "\n",
    "ds = make_dataset(data, block_size, char_vocab)\n",
    "X, y = next(iter(ds))\n",
    "\n",
    "print(\n",
    "    f'\\n\\nTrain Set\\n=============='\n",
    "    f'\\n{X.shape=} {y.shape=}'\n",
    "    f'\\n{X.numpy()=} {y.numpy()=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[B_i]=<tf.Tensor: shape=(8, 4, 4), dtype=float32, numpy=\n",
      "array([[[0.31179297, 0.8263413 , 0.6849456 , 0.0067091 ],\n",
      "        [0.78749514, 0.3906511 , 0.29263055, 0.99216926],\n",
      "        [0.95810425, 0.55623317, 0.16466296, 0.13445711],\n",
      "        [0.13229859, 0.5348098 , 0.57090175, 0.50970507]],\n",
      "\n",
      "       [[0.48252344, 0.15580535, 0.3703227 , 0.49210668],\n",
      "        [0.567016  , 0.2077086 , 0.18223882, 0.99883735],\n",
      "        [0.36950588, 0.37927854, 0.7723117 , 0.68211746],\n",
      "        [0.39932835, 0.7840713 , 0.67880154, 0.73395896]],\n",
      "\n",
      "       [[0.5520444 , 0.10948515, 0.6487982 , 0.9890779 ],\n",
      "        [0.8203654 , 0.70470357, 0.9578625 , 0.02297425],\n",
      "        [0.93598676, 0.6513264 , 0.31663585, 0.00111556],\n",
      "        [0.9212191 , 0.3822806 , 0.77246034, 0.91514194]],\n",
      "\n",
      "       [[0.5751133 , 0.793342  , 0.4289763 , 0.19118965],\n",
      "        [0.7452506 , 0.41762006, 0.8173511 , 0.20117116],\n",
      "        [0.6457157 , 0.16484237, 0.4484123 , 0.6057888 ],\n",
      "        [0.816115  , 0.4129653 , 0.2632984 , 0.19087589]],\n",
      "\n",
      "       [[0.7631861 , 0.8340243 , 0.49447727, 0.6866319 ],\n",
      "        [0.300259  , 0.26729417, 0.83389175, 0.62988555],\n",
      "        [0.15143108, 0.47044265, 0.69402504, 0.9605911 ],\n",
      "        [0.93813527, 0.5299927 , 0.21156156, 0.3940668 ]],\n",
      "\n",
      "       [[0.16456759, 0.42329657, 0.6732943 , 0.28540838],\n",
      "        [0.41686988, 0.74725735, 0.8982729 , 0.95173836],\n",
      "        [0.09668875, 0.94335854, 0.6924689 , 0.98699486],\n",
      "        [0.9925091 , 0.20401764, 0.26398885, 0.79402816]],\n",
      "\n",
      "       [[0.71974635, 0.47845697, 0.21366036, 0.78412795],\n",
      "        [0.93207085, 0.5625777 , 0.47580373, 0.30543637],\n",
      "        [0.5387846 , 0.06375635, 0.634938  , 0.6435729 ],\n",
      "        [0.3422103 , 0.8459996 , 0.21559799, 0.10748649]],\n",
      "\n",
      "       [[0.1337657 , 0.95617914, 0.6482675 , 0.55495536],\n",
      "        [0.02367401, 0.74084485, 0.35598373, 0.29914415],\n",
      "        [0.6900625 , 0.21108961, 0.00885725, 0.42452395],\n",
      "        [0.08308113, 0.40762496, 0.5765996 , 0.49922466]]], dtype=float32)>\n",
      "l.cos_mtheta=<tf.Tensor: shape=(64, 4), dtype=float32, numpy=\n",
      "array([[ 1.        ,  1.        ,  1.        ,  1.        ],\n",
      "       [ 0.5403023 ,  0.99995   ,  0.5403023 ,  0.99995   ],\n",
      "       [-0.4161468 ,  0.9998    , -0.4161468 ,  0.9998    ],\n",
      "       [-0.9899925 ,  0.99955004, -0.9899925 ,  0.99955004],\n",
      "       [-0.6536436 ,  0.9992001 , -0.6536436 ,  0.9992001 ],\n",
      "       [ 0.28366217,  0.99875027,  0.28366217,  0.99875027],\n",
      "       [ 0.96017027,  0.99820054,  0.96017027,  0.99820054],\n",
      "       [ 0.75390226,  0.997551  ,  0.75390226,  0.997551  ],\n",
      "       [-0.14550003,  0.99680173, -0.14550003,  0.99680173],\n",
      "       [-0.91113025,  0.9959527 , -0.91113025,  0.9959527 ],\n",
      "       [-0.8390715 ,  0.9950042 , -0.8390715 ,  0.9950042 ],\n",
      "       [ 0.0044257 ,  0.9939561 ,  0.0044257 ,  0.9939561 ],\n",
      "       [ 0.84385395,  0.99280864,  0.84385395,  0.99280864],\n",
      "       [ 0.9074468 ,  0.9915619 ,  0.9074468 ,  0.9915619 ],\n",
      "       [ 0.13673723,  0.990216  ,  0.13673723,  0.990216  ],\n",
      "       [-0.7596879 ,  0.9887711 , -0.7596879 ,  0.9887711 ],\n",
      "       [-0.9576595 ,  0.98722726, -0.9576595 ,  0.98722726],\n",
      "       [-0.27516335,  0.9855848 , -0.27516335,  0.9855848 ],\n",
      "       [ 0.6603167 ,  0.9838437 ,  0.6603167 ,  0.9838437 ],\n",
      "       [ 0.9887046 ,  0.9820042 ,  0.9887046 ,  0.9820042 ],\n",
      "       [ 0.40808207,  0.9800666 ,  0.40808207,  0.9800666 ],\n",
      "       [-0.54772925,  0.9780309 , -0.54772925,  0.9780309 ],\n",
      "       [-0.99996084,  0.97589743, -0.99996084,  0.97589743],\n",
      "       [-0.53283304,  0.97366637, -0.53283304,  0.97366637],\n",
      "       [ 0.42417902,  0.971338  ,  0.42417902,  0.971338  ],\n",
      "       [ 0.99120283,  0.9689124 ,  0.99120283,  0.9689124 ],\n",
      "       [ 0.64691937,  0.96638995,  0.64691937,  0.96638995],\n",
      "       [-0.29213881,  0.9637709 , -0.29213881,  0.9637709 ],\n",
      "       [-0.9626059 ,  0.96105546, -0.9626059 ,  0.96105546],\n",
      "       [-0.74805754,  0.9582439 , -0.74805754,  0.9582439 ],\n",
      "       [ 0.15425146,  0.9553365 ,  0.15425146,  0.9553365 ],\n",
      "       [ 0.91474235,  0.95233357,  0.91474235,  0.95233357],\n",
      "       [ 0.83422333,  0.94923544,  0.83422333,  0.94923544],\n",
      "       [-0.01327675,  0.94604236, -0.01327675,  0.94604236],\n",
      "       [-0.8485703 ,  0.9427547 , -0.8485703 ,  0.9427547 ],\n",
      "       [-0.9036922 ,  0.9393727 , -0.9036922 ,  0.9393727 ],\n",
      "       [-0.12796369,  0.9358968 , -0.12796369,  0.9358968 ],\n",
      "       [ 0.76541406,  0.93232733,  0.76541406,  0.93232733],\n",
      "       [ 0.95507365,  0.9286646 ,  0.95507365,  0.9286646 ],\n",
      "       [ 0.26664293,  0.92490906,  0.26664293,  0.92490906],\n",
      "       [-0.66693807,  0.921061  , -0.66693807,  0.921061  ],\n",
      "       [-0.98733926,  0.9171208 , -0.98733926,  0.9171208 ],\n",
      "       [-0.3999853 ,  0.9130889 , -0.3999853 ,  0.9130889 ],\n",
      "       [ 0.5551133 ,  0.90896577,  0.5551133 ,  0.90896577],\n",
      "       [ 0.9998433 ,  0.90475166,  0.9998433 ,  0.90475166],\n",
      "       [ 0.525322  ,  0.90044713,  0.525322  ,  0.90044713],\n",
      "       [-0.43217796,  0.8960525 , -0.43217796,  0.8960525 ],\n",
      "       [-0.9923355 ,  0.8915683 , -0.9923355 ,  0.8915683 ],\n",
      "       [-0.64014435,  0.8869949 , -0.64014435,  0.8869949 ],\n",
      "       [ 0.30059254,  0.88233286,  0.30059254,  0.88233286],\n",
      "       [ 0.964966  ,  0.87758255,  0.964966  ,  0.87758255],\n",
      "       [ 0.74215424,  0.8727445 ,  0.74215424,  0.8727445 ],\n",
      "       [-0.16299078,  0.8678192 , -0.16299078,  0.8678192 ],\n",
      "       [-0.9182828 ,  0.8628071 , -0.9182828 ,  0.8628071 ],\n",
      "       [-0.8293098 ,  0.8577087 , -0.8293098 ,  0.8577087 ],\n",
      "       [ 0.02212675,  0.8525245 ,  0.02212675,  0.8525245 ],\n",
      "       [ 0.8532201 ,  0.8472551 ,  0.8532201 ,  0.8472551 ],\n",
      "       [ 0.8998668 ,  0.84190094,  0.8998668 ,  0.84190094],\n",
      "       [ 0.11918014,  0.8364627 ,  0.11918014,  0.8364627 ],\n",
      "       [-0.77108026,  0.83094066, -0.77108026,  0.83094066],\n",
      "       [-0.95241296,  0.8253356 , -0.95241296,  0.8253356 ],\n",
      "       [-0.25810164,  0.819648  , -0.25810164,  0.819648  ],\n",
      "       [ 0.67350715,  0.8138785 ,  0.67350715,  0.8138785 ],\n",
      "       [ 0.9858966 ,  0.8080275 ,  0.9858966 ,  0.8080275 ]],\n",
      "      dtype=float32)>\n",
      "l.sin_mtheta=<tf.Tensor: shape=(64, 4), dtype=float32, numpy=\n",
      "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.841471  ,  0.00999983,  0.841471  ,  0.00999983],\n",
      "       [ 0.9092974 ,  0.01999867,  0.9092974 ,  0.01999867],\n",
      "       [ 0.14112   ,  0.0299955 ,  0.14112   ,  0.0299955 ],\n",
      "       [-0.7568025 ,  0.03998933, -0.7568025 ,  0.03998933],\n",
      "       [-0.9589243 ,  0.04997917, -0.9589243 ,  0.04997917],\n",
      "       [-0.2794155 ,  0.059964  , -0.2794155 ,  0.059964  ],\n",
      "       [ 0.6569866 ,  0.06994285,  0.6569866 ,  0.06994285],\n",
      "       [ 0.98935825,  0.07991469,  0.98935825,  0.07991469],\n",
      "       [ 0.41211846,  0.08987854,  0.41211846,  0.08987854],\n",
      "       [-0.54402107,  0.09983341, -0.54402107,  0.09983341],\n",
      "       [-0.9999902 ,  0.1097783 , -0.9999902 ,  0.1097783 ],\n",
      "       [-0.53657293,  0.1197122 , -0.53657293,  0.1197122 ],\n",
      "       [ 0.42016703,  0.12963414,  0.42016703,  0.12963414],\n",
      "       [ 0.9906074 ,  0.13954312,  0.9906074 ,  0.13954312],\n",
      "       [ 0.65028787,  0.14943813,  0.65028787,  0.14943813],\n",
      "       [-0.28790334,  0.15931821, -0.28790334,  0.15931821],\n",
      "       [-0.96139747,  0.16918235, -0.96139747,  0.16918235],\n",
      "       [-0.75098723,  0.17902957, -0.75098723,  0.17902957],\n",
      "       [ 0.1498772 ,  0.1888589 ,  0.1498772 ,  0.1888589 ],\n",
      "       [ 0.9129453 ,  0.19866931,  0.9129453 ,  0.19866931],\n",
      "       [ 0.8366556 ,  0.2084599 ,  0.8366556 ,  0.2084599 ],\n",
      "       [-0.00885131,  0.21822962, -0.00885131,  0.21822962],\n",
      "       [-0.8462204 ,  0.22797751, -0.8462204 ,  0.22797751],\n",
      "       [-0.9055784 ,  0.23770262, -0.9055784 ,  0.23770262],\n",
      "       [-0.13235176,  0.24740396, -0.13235176,  0.24740396],\n",
      "       [ 0.7625584 ,  0.25708055,  0.7625584 ,  0.25708055],\n",
      "       [ 0.95637596,  0.2667314 ,  0.95637596,  0.2667314 ],\n",
      "       [ 0.2709058 ,  0.27635565,  0.2709058 ,  0.27635565],\n",
      "       [-0.6636339 ,  0.2859522 , -0.6636339 ,  0.2859522 ],\n",
      "       [-0.9880316 ,  0.2955202 , -0.9880316 ,  0.2955202 ],\n",
      "       [-0.40403765,  0.30505863, -0.40403765,  0.30505863],\n",
      "       [ 0.5514267 ,  0.31456655,  0.5514267 ,  0.31456655],\n",
      "       [ 0.99991184,  0.324043  ,  0.99991184,  0.324043  ],\n",
      "       [ 0.5290827 ,  0.3334871 ,  0.5290827 ,  0.3334871 ],\n",
      "       [-0.42818266,  0.3428978 , -0.42818266,  0.3428978 ],\n",
      "       [-0.99177885,  0.3522742 , -0.99177885,  0.3522742 ],\n",
      "       [-0.6435381 ,  0.36161545, -0.6435381 ,  0.36161545],\n",
      "       [ 0.29636857,  0.37092045,  0.29636857,  0.37092045],\n",
      "       [ 0.96379536,  0.3801884 ,  0.96379536,  0.3801884 ],\n",
      "       [ 0.7451132 ,  0.38941833,  0.7451132 ,  0.38941833],\n",
      "       [-0.15862267,  0.3986093 , -0.15862267,  0.3986093 ],\n",
      "       [-0.91652155,  0.40776044, -0.91652155,  0.40776044],\n",
      "       [-0.8317747 ,  0.41687077, -0.8317747 ,  0.41687077],\n",
      "       [ 0.01770193,  0.42593947,  0.01770193,  0.42593947],\n",
      "       [ 0.8509035 ,  0.43496552,  0.8509035 ,  0.43496552],\n",
      "       [ 0.90178835,  0.4439481 ,  0.90178835,  0.4439481 ],\n",
      "       [ 0.12357312,  0.45288628,  0.12357312,  0.45288628],\n",
      "       [-0.76825464,  0.46177918, -0.76825464,  0.46177918],\n",
      "       [-0.95375264,  0.47062588, -0.95375264,  0.47062588],\n",
      "       [-0.26237485,  0.47942555, -0.26237485,  0.47942555],\n",
      "       [ 0.67022914,  0.48817724,  0.67022914,  0.48817724],\n",
      "       [ 0.9866276 ,  0.4968801 ,  0.9866276 ,  0.4968801 ],\n",
      "       [ 0.39592513,  0.50553334,  0.39592513,  0.50553334],\n",
      "       [-0.5587891 ,  0.51413596, -0.5587891 ,  0.51413596],\n",
      "       [-0.99975514,  0.52268726, -0.99975514,  0.52268726],\n",
      "       [-0.521551  ,  0.5311862 , -0.521551  ,  0.5311862 ],\n",
      "       [ 0.43616477,  0.539632  ,  0.43616477,  0.539632  ],\n",
      "       [ 0.99287266,  0.54802394,  0.99287266,  0.54802394],\n",
      "       [ 0.636738  ,  0.556361  ,  0.636738  ,  0.556361  ],\n",
      "       [-0.30481064,  0.5646424 , -0.30481064,  0.5646424 ],\n",
      "       [-0.96611774,  0.57286745, -0.96611774,  0.57286745],\n",
      "       [-0.7391807 ,  0.5810352 , -0.7391807 ,  0.5810352 ],\n",
      "       [ 0.1673557 ,  0.58914477,  0.1673557 ,  0.58914477]],\n",
      "      dtype=float32)>\n",
      "output[B_i]=<tf.Tensor: shape=(8, 4, 4), dtype=float32, numpy=\n",
      "array([[[ 0.31179297,  0.8263413 ,  0.6849456 ,  0.0067091 ],\n",
      "        [ 0.78749514,  0.3906511 ,  0.29263055,  0.99216926],\n",
      "        [ 0.95810425,  0.55623317,  0.16466296,  0.13445711],\n",
      "        [ 0.13229859,  0.5348098 ,  0.57090175,  0.50970507]],\n",
      "\n",
      "       [[-0.05090731,  0.15087657,  0.6061157 ,  0.4936401 ],\n",
      "        [ 0.15301135,  0.19771   ,  0.57559156,  1.0008645 ],\n",
      "        [-0.45023304,  0.37243852,  0.7282103 ,  0.6858761 ],\n",
      "        [-0.35543382,  0.7766926 ,  0.70278126,  0.7417628 ]],\n",
      "\n",
      "       [[-0.81968206,  0.08968301,  0.23197725,  0.99106973],\n",
      "        [-1.2123743 ,  0.70410323,  0.34734476,  0.03706279],\n",
      "        [-0.6774241 ,  0.6511739 ,  0.7193233 ,  0.014141  ],\n",
      "        [-1.0857587 ,  0.36390254,  0.51620525,  0.9226041 ]],\n",
      "\n",
      "       [[-0.62989503,  0.7872502 , -0.34352332,  0.21490031],\n",
      "        [-0.8531371 ,  0.41139793, -0.70400167,  0.21360736],\n",
      "        [-0.70253366,  0.14659725, -0.3528014 ,  0.61046076],\n",
      "        [-0.84510446,  0.40705407, -0.14549327,  0.2031771 ]],\n",
      "\n",
      "       [[-0.12463009,  0.8058992 , -0.9007931 ,  0.71943474],\n",
      "        [ 0.434829  ,  0.24189167, -0.7723048 ,  0.6400706 ],\n",
      "        [ 0.4262579 ,  0.43165293, -0.56824845,  0.97863543],\n",
      "        [-0.45309582,  0.5138103 , -0.848269  ,  0.41494566]],\n",
      "\n",
      "       [[ 0.69231987,  0.4085031 ,  0.03318027,  0.30620772],\n",
      "        [ 0.9796259 ,  0.6987564 , -0.14494061,  0.98789626],\n",
      "        [ 0.6914522 ,  0.89285046,  0.10371004,  1.0329096 ],\n",
      "        [ 0.53468263,  0.1640778 , -0.87685746,  0.8032325 ]],\n",
      "\n",
      "       [[ 0.75077903,  0.43057656,  0.00404204,  0.81140715],\n",
      "        [ 1.0278937 ,  0.5432502 ,  0.19641754,  0.33862117],\n",
      "        [ 0.6947365 ,  0.02505041,  0.45910382,  0.64623797],\n",
      "        [ 0.38882154,  0.8380319 ,  0.11139192,  0.1580226 ]],\n",
      "\n",
      "       [[-0.3250568 ,  0.9150223 ,  0.5766126 ,  0.6204742 ],\n",
      "        [-0.21602865,  0.7181075 ,  0.28393045,  0.35022834],\n",
      "        [ 0.5144206 ,  0.18088025,  0.46003935,  0.43824852],\n",
      "        [-0.31618315,  0.3717095 ,  0.48928294,  0.5265125 ]]],\n",
      "      dtype=float32)>\n",
      "output[B_i]=<tf.Tensor: shape=(8, 4, 4), dtype=float32, numpy=\n",
      "array([[[ 0.31179297,  0.8263413 ,  0.6849456 ,  0.0067091 ],\n",
      "        [ 0.78749514,  0.3906511 ,  0.29263055,  0.99216926],\n",
      "        [ 0.95810425,  0.55623317,  0.16466296,  0.13445711],\n",
      "        [ 0.13229859,  0.5348098 ,  0.57090175,  0.50970507]],\n",
      "\n",
      "       [[-0.05090731,  0.15087657,  0.6061157 ,  0.4936401 ],\n",
      "        [ 0.15301135,  0.19771   ,  0.57559156,  1.0008645 ],\n",
      "        [-0.45023304,  0.37243852,  0.7282103 ,  0.6858761 ],\n",
      "        [-0.35543382,  0.7766926 ,  0.70278126,  0.7417628 ]],\n",
      "\n",
      "       [[-0.81968206,  0.08968301,  0.23197725,  0.99106973],\n",
      "        [-1.2123743 ,  0.70410323,  0.34734476,  0.03706279],\n",
      "        [-0.6774241 ,  0.6511739 ,  0.7193233 ,  0.014141  ],\n",
      "        [-1.0857587 ,  0.36390254,  0.51620525,  0.9226041 ]],\n",
      "\n",
      "       [[-0.62989503,  0.7872502 , -0.34352332,  0.21490031],\n",
      "        [-0.8531371 ,  0.41139793, -0.70400167,  0.21360736],\n",
      "        [-0.70253366,  0.14659725, -0.3528014 ,  0.61046076],\n",
      "        [-0.84510446,  0.40705407, -0.14549327,  0.2031771 ]],\n",
      "\n",
      "       [[-0.12463009,  0.8058992 , -0.9007931 ,  0.71943474],\n",
      "        [ 0.434829  ,  0.24189167, -0.7723048 ,  0.6400706 ],\n",
      "        [ 0.4262579 ,  0.43165293, -0.56824845,  0.97863543],\n",
      "        [-0.45309582,  0.5138103 , -0.848269  ,  0.41494566]],\n",
      "\n",
      "       [[ 0.69231987,  0.4085031 ,  0.03318027,  0.30620772],\n",
      "        [ 0.9796259 ,  0.6987564 , -0.14494061,  0.98789626],\n",
      "        [ 0.6914522 ,  0.89285046,  0.10371004,  1.0329096 ],\n",
      "        [ 0.53468263,  0.1640778 , -0.87685746,  0.8032325 ]],\n",
      "\n",
      "       [[ 0.75077903,  0.43057656,  0.00404204,  0.81140715],\n",
      "        [ 1.0278937 ,  0.5432502 ,  0.19641754,  0.33862117],\n",
      "        [ 0.6947365 ,  0.02505041,  0.45910382,  0.64623797],\n",
      "        [ 0.38882154,  0.8380319 ,  0.11139192,  0.1580226 ]],\n",
      "\n",
      "       [[-0.3250568 ,  0.9150223 ,  0.5766126 ,  0.6204742 ],\n",
      "        [-0.21602865,  0.7181075 ,  0.28393045,  0.35022834],\n",
      "        [ 0.5144206 ,  0.18088025,  0.46003935,  0.43824852],\n",
      "        [-0.31618315,  0.3717095 ,  0.48928294,  0.5265125 ]]],\n",
      "      dtype=float32)> (2, 8, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RotaryPositionalEncodings(layers.Layer):\n",
    "    def __init__(self, dims, block_size, base=10000.):\n",
    "        super(RotaryPositionalEncodings, self).__init__()\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.base = base\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.theta = 1. / (self.base ** (tf.range(0, self.dims, 2, dtype=tf.float32) / self.dims))\n",
    "        self.positions = tf.range(self.block_size, dtype=tf.float32)\n",
    "        self.mtheta = self.positions[..., None]*self.theta[None, ...]\n",
    "        self.mtheta_paired = tf.concat([self.mtheta]*2, axis=-1)\n",
    "\n",
    "        self.cos_mtheta = tf.math.cos(self.mtheta_paired)\n",
    "        self.sin_mtheta = tf.math.sin(self.mtheta_paired)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "\n",
    "        x_real = x*self.cos_mtheta[None, :T, None, ...]\n",
    "        # print(f'{x_real=}\\n{self.cos_mtheta[:T]=}')\n",
    "        x_img = tf.concat(\n",
    "            [-x[..., self.dims//2:], x[..., :self.dims//2]],\n",
    "            axis=-1\n",
    "        )*self.sin_mtheta[None, :T, None, ...]\n",
    "        # print(f'{x_img=}\\n{self.sin_mtheta[:T]=}')\n",
    "        output = x_real + x_img\n",
    "        return output\n",
    "\n",
    "B, T, H, D = 2, 8, 4, 4\n",
    "B_i, T_i, H_i, D_i = 0, 1, 0, 0\n",
    "x = tf.random.uniform((B, T, H, D))\n",
    "l = RotaryPositionalEncodings(4, 64)\n",
    "output = l(x)\n",
    "# tf.print(f'{l.theta=} {l.theta.shape}')\n",
    "# tf.print(f'{l.positions=} {l.positions.shape}')\n",
    "tf.print(f'{x[B_i]=}')\n",
    "tf.print(f'{l.cos_mtheta=}')\n",
    "tf.print(f'{l.sin_mtheta=}')\n",
    "tf.print(f'{output[B_i]=}\\n{output[B_i]=} {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[B_i]=<tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n",
      "      dtype=int32)>\n",
      "l.cache.cache_k.shape=(4, 16, 2, 4)\n",
      "l.cache.cache_k[B_i, :start+T]=array([[[  0.       ,   0.       ,   0.       ,   0.       ],\n",
      "        [  0.       ,   0.       ,   0.       ,   0.       ]],\n",
      "\n",
      "       [[ 11.398402 ,  11.38393  ,  -5.348648 ,  12.098706 ],\n",
      "        [  1.0817871, -16.598928 ,  -1.4157329,  14.4951935]],\n",
      "\n",
      "       [[ 23.552362 ,  21.765985 ,   8.511203 ,  37.022823 ],\n",
      "        [  8.715925 , -41.078644 , -12.075365 ,  37.266632 ]]],\n",
      "      dtype=float32)\n",
      "l.cache.cache_v[B_i, :start+T]=array([[[  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "        [  0.        ,   0.        ,   0.        ,   0.        ]],\n",
      "\n",
      "       [[  3.5393999 ,   2.8608413 ,  -3.3473425 ,  -0.45976973],\n",
      "        [-10.767759  , -10.359337  ,   5.055772  ,  10.362978  ]],\n",
      "\n",
      "       [[ 19.419163  ,  10.644936  ,  -8.607593  ,  -3.0411987 ],\n",
      "        [-38.65007   , -26.748064  ,   7.108481  ,  42.319984  ]]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RotaryPositionalEncodings(layers.Layer):\n",
    "    def __init__(self, dims, block_size, base=10000.):\n",
    "        super(RotaryPositionalEncodings, self).__init__()\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.base = base\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.theta = 1. / (self.base ** (tf.range(0, self.dims, 2, dtype=tf.float32) / self.dims))\n",
    "        self.positions = tf.range(self.block_size, dtype=tf.float32)\n",
    "        self.mtheta = self.positions[..., None]*self.theta[None, ...]\n",
    "        self.mtheta_paired = tf.concat([self.mtheta]*2, axis=-1)\n",
    "\n",
    "        self.cos_mtheta = tf.math.cos(self.mtheta_paired)\n",
    "        self.sin_mtheta = tf.math.sin(self.mtheta_paired)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "\n",
    "        x_real = x*self.cos_mtheta[None, :T, None, ...]\n",
    "        # print(f'{x_real=}\\n{self.cos_mtheta[:T]=}')\n",
    "        x_img = tf.concat(\n",
    "            [-x[..., self.dims//2:], x[..., :self.dims//2]],\n",
    "            axis=-1\n",
    "        )*self.sin_mtheta[None, :T, None, ...]\n",
    "        # print(f'{x_img=}\\n{self.sin_mtheta[:T]=}')\n",
    "        output = x_real + x_img\n",
    "        return output\n",
    "\n",
    "class KVCache(object):\n",
    "    def __init__(self, cache_size, block_size, heads, head_dims):\n",
    "        super(KVCache, self).__init__()\n",
    "\n",
    "        cache_shape = (cache_size, block_size, heads, head_dims)\n",
    "        self.cache_k = np.zeros(cache_shape, dtype='float32')\n",
    "        self.cache_v = np.zeros(cache_shape, dtype='float32')\n",
    "\n",
    "    def update(self, start, xk, xv):\n",
    "        batch_size, seq_len = xk.shape[:2]\n",
    "        # self.cache_k[:batch_size, start:start+seq_len] = xk.numpy()\n",
    "        # self.cache_v[:batch_size, start:start+seq_len] = xv.numpy()\n",
    "        self.cache_k[:batch_size, start:start+seq_len] = xk\n",
    "        self.cache_v[:batch_size, start:start+seq_len] = xv\n",
    "\n",
    "    def get(self, batch_size, seq_len, start):\n",
    "        keys = tf.constant(self.cache_k[:batch_size, :start+seq_len])\n",
    "        values = tf.constant(self.cache_v[:batch_size, :start+seq_len])\n",
    "        return keys, values\n",
    "\n",
    "class GroupedQueryAttention(layers.Layer):\n",
    "    def __init__(self, block_size, heads, kv_heads, dims, cache_size):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dims = dims\n",
    "        self.head_dims = dims // self.heads\n",
    "        self.kv_heads = kv_heads if kv_heads else heads\n",
    "        self.block_size = block_size\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        self.wq = layers.Dense(self.dims, use_bias=False)\n",
    "        self.wk = layers.Dense(self.kv_heads * self.head_dims, use_bias=False)\n",
    "        self.wv = layers.Dense(self.kv_heads * self.head_dims, use_bias=False)\n",
    "        self.wo = layers.Dense(self.dims, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(self.cache_size, self.block_size, self.kv_heads, self.head_dims)\n",
    "        self.rope = RotaryPositionalEncodings(self.head_dims, self.block_size)\n",
    "    \n",
    "    def call(self, x, start):\n",
    "        # print(f'{x=}\\n{start=}')\n",
    "        shape = tf.shape(x)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "\n",
    "        # (B, T, dims)\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # (B, T, heads/kv_heads, head_dims)\n",
    "        xq = tf.reshape(q, (B, T, self.heads, self.head_dims))\n",
    "        xk = tf.reshape(k, (B, T, self.kv_heads, self.head_dims))\n",
    "        xv = tf.reshape(v, (B, T, self.kv_heads, self.head_dims))\n",
    "\n",
    "        # Apply RoPE\n",
    "        # (B, T, heads/kv_heads, head_dims)\n",
    "        xq = self.rope(xq)\n",
    "        xk = self.rope(xk)\n",
    "\n",
    "        # Update KV cache\n",
    "        self.cache.update(start, xk, xv)\n",
    "        \n",
    "        # Get prefix context from the cache.\n",
    "        # (B, start+T, kv_heads, head_dims)\n",
    "        keys, values = self.cache.get(B, T, start)\n",
    "\n",
    "        # Expand kv_heads to heads\n",
    "        # (B, start+T, heads, head_dims)\n",
    "        # print(f'{keys.shape=} {values.shape=} {keys.dtype=}')\n",
    "        keys = tf.tile(keys, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "        values = tf.tile(values, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "        \n",
    "        # Transpose xq, keys and values to (B, heads, T/start+T, head_dims)\n",
    "        xq = tf.transpose(xq, perm=[0, 2, 1, 3])\n",
    "        xk = tf.transpose(keys, perm=[0, 2, 1, 3])\n",
    "        xv = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Multiply xq and xk to compute attention matrix.\n",
    "        # (B, heads, T, head_dims) @ (B, heads, head_dims, start+T) -> (B, heads, T, start+T)\n",
    "        xa = xq @ tf.transpose(xk, perm=[0, 1, 3, 2]) / math.sqrt(self.head_dims)\n",
    "        scores = tf.nn.softmax(xa)\n",
    "\n",
    "        # Scale Values and compute output\n",
    "        # (B, heads, T, start+T) @ (B, heads, start+T, head_dims) -> (B, heads, T, head_dims)\n",
    "        output = scores @ xv\n",
    "        \n",
    "        # Reshape output to (B, T, dims)\n",
    "        output = tf.reshape(\n",
    "            tf.transpose(output, perm=[0, 2, 1, 3]),\n",
    "            shape=(B, T, -1)\n",
    "        )\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "l = GroupedQueryAttention(\n",
    "    block_size=16,\n",
    "    heads=4,\n",
    "    kv_heads=2,\n",
    "    dims=16,\n",
    "    cache_size=4\n",
    ")\n",
    "\n",
    "B, T, C = 2, 2, 16\n",
    "start = 1\n",
    "x = tf.reshape(tf.range(B*T*C), (B, T, C))\n",
    "output = l(x, start)\n",
    "\n",
    "B_i, T_i, C_i = 0, 0, 3\n",
    "H_i, F_i = 0, C_i\n",
    "\n",
    "print(\n",
    "    f'{x[B_i]=}'\n",
    "    f'\\n{l.cache.cache_k.shape=}\\n{l.cache.cache_k[B_i, :start+T]=}'\n",
    "    f'\\n{l.cache.cache_v[B_i, :start+T]=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=TensorShape([5, 9, 7]) result.shape=TensorShape([5, 9, 7])x[:2]=<tf.Tensor: shape=(2, 9, 7), dtype=float32, numpy=\n",
      "array([[[0.31179297, 0.8263413 , 0.6849456 , 0.0067091 , 0.78749514,\n",
      "         0.3906511 , 0.29263055],\n",
      "        [0.99216926, 0.95810425, 0.55623317, 0.16466296, 0.13445711,\n",
      "         0.13229859, 0.5348098 ],\n",
      "        [0.57090175, 0.50970507, 0.48252344, 0.15580535, 0.3703227 ,\n",
      "         0.49210668, 0.567016  ],\n",
      "        [0.2077086 , 0.18223882, 0.99883735, 0.36950588, 0.37927854,\n",
      "         0.7723117 , 0.68211746],\n",
      "        [0.39932835, 0.7840713 , 0.67880154, 0.73395896, 0.5520444 ,\n",
      "         0.10948515, 0.6487982 ],\n",
      "        [0.9890779 , 0.8203654 , 0.70470357, 0.9578625 , 0.02297425,\n",
      "         0.93598676, 0.6513264 ],\n",
      "        [0.31663585, 0.00111556, 0.9212191 , 0.3822806 , 0.77246034,\n",
      "         0.91514194, 0.5751133 ],\n",
      "        [0.793342  , 0.4289763 , 0.19118965, 0.7452506 , 0.41762006,\n",
      "         0.8173511 , 0.20117116],\n",
      "        [0.6457157 , 0.16484237, 0.4484123 , 0.6057888 , 0.816115  ,\n",
      "         0.4129653 , 0.2632984 ]],\n",
      "\n",
      "       [[0.19087589, 0.7631861 , 0.8340243 , 0.49447727, 0.6866319 ,\n",
      "         0.300259  , 0.26729417],\n",
      "        [0.83389175, 0.62988555, 0.15143108, 0.47044265, 0.69402504,\n",
      "         0.9605911 , 0.93813527],\n",
      "        [0.5299927 , 0.21156156, 0.3940668 , 0.16456759, 0.42329657,\n",
      "         0.6732943 , 0.28540838],\n",
      "        [0.41686988, 0.74725735, 0.8982729 , 0.95173836, 0.09668875,\n",
      "         0.94335854, 0.6924689 ],\n",
      "        [0.98699486, 0.9925091 , 0.20401764, 0.26398885, 0.79402816,\n",
      "         0.71974635, 0.47845697],\n",
      "        [0.21366036, 0.78412795, 0.93207085, 0.5625777 , 0.47580373,\n",
      "         0.30543637, 0.5387846 ],\n",
      "        [0.06375635, 0.634938  , 0.6435729 , 0.3422103 , 0.8459996 ,\n",
      "         0.21559799, 0.10748649],\n",
      "        [0.1337657 , 0.95617914, 0.6482675 , 0.55495536, 0.02367401,\n",
      "         0.74084485, 0.35598373],\n",
      "        [0.29914415, 0.6900625 , 0.21108961, 0.00885725, 0.42452395,\n",
      "         0.08308113, 0.40762496]]], dtype=float32)> result[:2]=<tf.Tensor: shape=(2, 9, 7), dtype=float32, numpy=\n",
      "array([[[5.6822085e-01, 1.5059490e+00, 1.2482653e+00, 1.2226862e-02,\n",
      "         1.4351547e+00, 7.1193427e-01, 5.3329867e-01],\n",
      "        [1.6404932e+00, 1.5841687e+00, 9.1969866e-01, 2.7226046e-01,\n",
      "         2.2231688e-01, 2.1874788e-01, 8.8427639e-01],\n",
      "        [1.2157279e+00, 1.0854104e+00, 1.0275275e+00, 3.3178547e-01,\n",
      "         7.8859740e-01, 1.0479348e+00, 1.2074533e+00],\n",
      "        [3.5354125e-01, 3.1018910e-01, 1.7001232e+00, 6.2893677e-01,\n",
      "         6.4557081e-01, 1.3145534e+00, 1.1610336e+00],\n",
      "        [6.6668886e-01, 1.3090271e+00, 1.1332765e+00, 1.2253633e+00,\n",
      "         9.2165220e-01, 1.8278825e-01, 1.0831852e+00],\n",
      "        [1.2523547e+00, 1.0387336e+00, 8.9228445e-01, 1.2128303e+00,\n",
      "         2.9089633e-02, 1.1851315e+00, 8.2469916e-01],\n",
      "        [4.9536783e-01, 1.7452629e-03, 1.4412212e+00, 5.9806716e-01,\n",
      "         1.2084924e+00, 1.4317137e+00, 8.9974850e-01],\n",
      "        [1.3875629e+00, 7.5028372e-01, 3.3439255e-01, 1.3034505e+00,\n",
      "         7.3042154e-01, 1.4295551e+00, 3.5185030e-01],\n",
      "        [1.2335490e+00, 3.1490815e-01, 8.5662860e-01, 1.1572742e+00,\n",
      "         1.5590729e+00, 7.8891206e-01, 5.0299454e-01]],\n",
      "\n",
      "       [[3.4109417e-01, 1.3638093e+00, 1.4903969e+00, 8.8362813e-01,\n",
      "         1.2270074e+00, 5.3656113e-01, 4.7765321e-01],\n",
      "        [1.1594485e+00, 8.7579697e-01, 2.1055076e-01, 6.5410650e-01,\n",
      "         9.6497691e-01, 1.3356121e+00, 1.3043894e+00],\n",
      "        [1.2684867e+00, 5.0635237e-01, 9.4316119e-01, 3.9387676e-01,\n",
      "         1.0131198e+00, 1.6114655e+00, 6.8309760e-01],\n",
      "        [5.6403995e-01, 1.0110661e+00, 1.2153955e+00, 1.2877362e+00,\n",
      "         1.3082334e-01, 1.2763979e+00, 9.3693525e-01],\n",
      "        [1.4063109e+00, 1.4141679e+00, 2.9069272e-01, 3.7614217e-01,\n",
      "         1.1313640e+00, 1.0255243e+00, 6.8172520e-01],\n",
      "        [3.6073062e-01, 1.3238720e+00, 1.5736493e+00, 9.4982058e-01,\n",
      "         8.0331689e-01, 5.1567942e-01, 9.0964979e-01],\n",
      "        [1.2891042e-01, 1.2837958e+00, 1.3012550e+00, 6.9192290e-01,\n",
      "         1.7105461e+00, 4.3592253e-01, 2.1732941e-01],\n",
      "        [2.3153023e-01, 1.6550161e+00, 1.1220629e+00, 9.6055228e-01,\n",
      "         4.0976495e-02, 1.2823017e+00, 6.1615944e-01],\n",
      "        [8.0627453e-01, 1.8599055e+00, 5.6894374e-01, 2.3872690e-02,\n",
      "         1.1442071e+00, 2.2392616e-01, 1.0986598e+00]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RMSNorm(layers.Layer):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[int(input_shape[-1])],\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "        )\n",
    "    \n",
    "    def norm(self, x):\n",
    "        return x*tf.math.rsqrt(tf.math.reduce_mean(x**2, axis=-1, keepdims=True) + self.eps)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.w * self.norm(inputs)\n",
    "\n",
    "\n",
    "batch, seqlen, dims = 5, 9, 7\n",
    "x = tf.random.uniform((batch, seqlen, dims))\n",
    "l = RMSNorm()\n",
    "l.build(x.shape)\n",
    "result = l(x)\n",
    "\n",
    "print(\n",
    "    f'{x.shape=} {result.shape=}'\n",
    "    f'{x[:2]=} {result[:2]=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 16), dtype=float32, numpy=\n",
       "array([[[-2.0513205e+01,  1.0806817e+01, -9.1668015e+00, -7.4922695e+00,\n",
       "         -1.3153091e+01,  5.2315617e-01,  1.2746422e+01, -3.4128265e+01,\n",
       "          6.7470369e+00,  2.7175690e+01, -4.5521870e+00,  1.6494555e+01,\n",
       "          1.8180428e+01,  1.5493259e+00, -1.4829735e+01,  3.4519196e+01],\n",
       "        [-1.0848410e+02,  4.9710754e+01,  1.2151041e+01, -6.9968826e+01,\n",
       "         -6.3353088e+01, -8.1262497e+01,  2.8162182e+01, -2.5223247e+02,\n",
       "         -6.3216888e+01,  1.3744359e+02, -4.6534172e+01,  2.7248346e+02,\n",
       "          2.6686646e+02, -1.2055109e+02, -2.0656288e+02,  1.3071808e+02]],\n",
       "\n",
       "       [[-2.6512726e+02,  4.4408295e+01,  2.8791975e+01, -1.2816547e+02,\n",
       "         -1.7421616e+02, -2.4323286e+02,  1.2617584e+01, -6.8494702e+02,\n",
       "         -1.9408469e+02,  3.7911090e+02, -1.4601923e+02,  8.5417902e+02,\n",
       "          7.5281537e+02, -4.2752676e+02, -5.7983594e+02,  2.5046817e+02],\n",
       "        [-4.8799042e+02,  7.7549438e+00,  5.0849304e+01, -1.8859167e+02,\n",
       "         -3.4041486e+02, -4.9665674e+02, -2.6662445e+01, -1.3386355e+03,\n",
       "         -3.9203278e+02,  7.4468665e+02, -2.9587506e+02,  1.7522783e+03,\n",
       "          1.4785563e+03, -9.1033948e+02, -1.1339839e+03,  3.9625031e+02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, dims):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.hidden_dims = 4*dims\n",
    "        self.linear_1 = layers.Dense(self.hidden_dims, use_bias=False, activation='swish')\n",
    "        self.linear_2 = layers.Dense(self.hidden_dims, use_bias=False)\n",
    "        self.linear_3 = layers.Dense(dims, use_bias=False)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # (..., dims) -> (..., hidden_dims)\n",
    "        x = self.linear_1(x)*self.linear_2(x)\n",
    "        \n",
    "        # (..., hidden_dims) -> (..., dims)\n",
    "        x = self.linear_3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "B, T, C = 2, 2, 16\n",
    "l = FeedForward(C)\n",
    "x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 16), dtype=float32, numpy=\n",
       "array([[[ 1.5042548 , -0.06997496, -0.17430654,  0.83612585,\n",
       "          4.4509845 ,  5.4747915 ,  5.0120664 ,  5.330117  ,\n",
       "          8.416934  ,  9.06529   , 10.056999  , 10.308799  ,\n",
       "         11.04062   , 14.062305  , 14.283695  , 13.84579   ],\n",
       "        [17.730944  , 16.037317  , 15.690351  , 17.138454  ,\n",
       "         20.40212   , 21.468744  , 21.2813    , 21.50022   ,\n",
       "         24.659027  , 24.916517  , 26.150116  , 26.664133  ,\n",
       "         27.088156  , 30.176422  , 29.790104  , 30.220867  ]],\n",
       "\n",
       "       [[33.831486  , 31.854721  , 31.671326  , 33.34517   ,\n",
       "         36.32003   , 37.277905  , 37.55028   , 37.49979   ,\n",
       "         40.585087  , 40.70484   , 42.30504   , 42.776077  ,\n",
       "         43.12911   , 46.41141   , 45.518208  , 46.99089   ],\n",
       "        [49.839653  , 47.869476  , 47.66023   , 49.371593  ,\n",
       "         52.30976   , 53.274467  , 53.573917  , 53.524033  ,\n",
       "         56.58298   , 56.68282   , 58.309742  , 58.80591   ,\n",
       "         59.12888   , 62.410606  , 61.477734  , 63.022484  ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class LlamaBlock(tf.keras.Model):\n",
    "    def __init__(self, block_size, heads, kv_heads, dims, cache_size):\n",
    "        super(LlamaBlock, self).__init__()\n",
    "        \n",
    "        self.rms_1 = RMSNorm()\n",
    "        self.attention = GroupedQueryAttention(block_size, heads, kv_heads, dims, cache_size)\n",
    "        self.rms_2 = RMSNorm()\n",
    "        self.feed_forward = FeedForward(dims)\n",
    "    \n",
    "    def call(self, x, start=0):\n",
    "        x += self.attention(self.rms_1(x), start=start)\n",
    "        x += self.feed_forward(self.rms_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# l = LlamaBlock(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=16,\n",
    "#     cache_size=4\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C, dtype='float32'), (B, T, C))\n",
    "# l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 1000), dtype=float32, numpy=\n",
       "array([[[-0.1239534 , -0.0607941 , -0.26741952, ..., -0.17009711,\n",
       "          0.08234538, -0.09490468],\n",
       "        [-0.06256978, -0.01165929, -0.35023674, ..., -0.11410779,\n",
       "         -0.01988001, -0.10182261]],\n",
       "\n",
       "       [[-0.00436667,  0.2219006 , -0.09742606, ...,  0.06465681,\n",
       "         -0.02983708, -0.02597765],\n",
       "        [-0.02063455,  0.18581536, -0.16156226, ...,  0.05736311,\n",
       "          0.00983013, -0.05446468]]], dtype=float32)>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from functools import reduce\n",
    "\n",
    "class LlamaModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, encoders, dims,\n",
    "        block_size, heads, kv_heads, cache_size,\n",
    "    ):\n",
    "        super(LlamaModel, self).__init__()\n",
    "        self.embeddings = layers.Embedding(vocab_size, dims)\n",
    "        self.enc_blocks = [\n",
    "            LlamaBlock(block_size, heads, kv_heads, dims, cache_size)\n",
    "            for enc_id in range(encoders)\n",
    "        ]\n",
    "        self.rms = RMSNorm()\n",
    "        self.head = layers.Dense(vocab_size, use_bias=False)\n",
    "    \n",
    "    def call(self, x, start=0):\n",
    "        B, T = x.shape\n",
    "        x_embed = self.embeddings(x)\n",
    "        x = reduce(\n",
    "            lambda y,enc_block: enc_block(y, start=start),\n",
    "            self.enc_blocks,\n",
    "            x_embed\n",
    "        )\n",
    "        x = self.rms(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X)\n",
    "            loss = self.compute_loss(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return compute_metrics(loss, y, logits)\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        X, y = data\n",
    "\n",
    "        logits = self(X)\n",
    "        loss = self.compute_loss(y, logits)\n",
    "        \n",
    "        return compute_metrics(loss, y, logits)\n",
    "    \n",
    "    def compute_metrics(self, loss, y, logits):\n",
    "        for metric in self.metrics:\n",
    "            if metric.name == \"loss\":\n",
    "                metric.update_state(loss)\n",
    "            else:\n",
    "                metric.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "m = LlamaModel(\n",
    "    vocab_size=1000,\n",
    "    encoders=2,\n",
    "    block_size=16,\n",
    "    heads=4,\n",
    "    kv_heads=2,\n",
    "    dims=16,\n",
    "    cache_size=4\n",
    ")\n",
    "\n",
    "# m.compile()\n",
    "\n",
    "B, T = 2, 2\n",
    "start = 1\n",
    "x = tf.reshape(tf.range(B*T), (B, T))\n",
    "m(x)\n",
    "# m.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, optimizers\n",
    "\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = optimizers.legacy.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* [X] loss and accuracy metrics.\n",
    "* [X] training, validation and inference flags.\n",
    "* Match dictionary with embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1707970084.179407       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - ETA: 0s - loss: 4.3404 - accuracy: 0.0045"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1707970096.618879       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 22s 5s/step - loss: 4.3404 - accuracy: 0.0045 - val_loss: 4.0718 - val_accuracy: 0.0432\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, metrics\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class KVCache(object):\n",
    "    def __init__(self, cache_size, block_size, heads, head_dims):\n",
    "        super(KVCache, self).__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        cache_shape = (cache_size, block_size, heads, head_dims)\n",
    "\n",
    "        with tf.device('/device:CPU:0'):\n",
    "            self.cache_k = tf.Variable(tf.zeros(cache_shape), shape=cache_shape, trainable=False)\n",
    "            self.cache_v = tf.Variable(tf.zeros(cache_shape), shape=cache_shape, trainable=False)\n",
    "\n",
    "    def update(self, start, xk, xv):\n",
    "        shape = tf.shape(xk)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "\n",
    "        # Calculate update start and end positions\n",
    "        start = start%self.block_size\n",
    "        end = (start + T)%(self.block_size + 1)\n",
    "\n",
    "        # start < end: It is a single cache update.\n",
    "        # end > start: It is a split cache update.\n",
    "        if start < end:\n",
    "            self.cache_k[:B, start:start+T].assign(xk)\n",
    "            self.cache_v[:B, start:start+T].assign(xv)\n",
    "        else:\n",
    "            # Update cache with partial sequence that fits towards the end.\n",
    "            self.cache_k[:B, start:].assign(xk[:, :-(end+1)])\n",
    "            self.cache_v[:B, start:].assign(xv[:, :-(end+1)])\n",
    "\n",
    "            # Splillover sequence is cached towards the front of the cache.\n",
    "            self.cache_k[:B, :end+1].assign(xk[:, -(end+1):])\n",
    "            self.cache_v[:B, :end+1].assign(xv[:, -(end+1):])\n",
    "\n",
    "    # TODO:: Update the callers to reflect the args order change.\n",
    "    def get(self, batch_size, start, seq_len):\n",
    "        # Calculate update start and end positions\n",
    "        start = start%self.block_size\n",
    "        end = (start + seq_len)%(self.block_size + 1)\n",
    "\n",
    "        # start < end: It is a single cache fetch.\n",
    "        # end > start: It is a split cache fetch.\n",
    "        if start < end:\n",
    "            keys = self.cache_k[:batch_size, :start+seq_len]\n",
    "            values = self.cache_v[:batch_size, :start+seq_len]\n",
    "        else:\n",
    "            # Fetch sequence prefix\n",
    "            keys_1 = self.cache_k[:, (end+1):]\n",
    "            values_1 = self.cache_k[:, (end+1):]\n",
    "\n",
    "            # Fetch sequence suffix\n",
    "            keys_2 = self.cache_k[:, :(end+1):]\n",
    "            values_2 = self.cache_k[:, :(end+1):]\n",
    "\n",
    "            # Compose the whole sequence\n",
    "            keys = tf.concat([keys_1, keys_2], axis=1)\n",
    "            values = tf.concat([values_1, values_2], axis=1)\n",
    "\n",
    "        return keys, values\n",
    "\n",
    "def update_and_show(cache, batch_size, start, seqlen, data_shape, msg, debug=False):\n",
    "    xk = tf.random.uniform((batch_size, seqlen, *data_shape))\n",
    "    xv = tf.random.uniform((batch_size, seqlen, *data_shape))\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            f'\\n{msg}::xk:\\n{xk}'\n",
    "        )\n",
    "\n",
    "    cache.update(start, xk, xv)\n",
    "    print(\n",
    "        f'\\n{msg}:\\n{cache.cache_k}'\n",
    "    )\n",
    "\n",
    "# cache_size, block_size, heads, head_dims = 1, 8, 1, 4\n",
    "# cache = KVCache(cache_size, block_size, heads, head_dims)\n",
    "# data_shape = (heads, head_dims)\n",
    "\n",
    "# # print(f'\\nInitial:\\n{cache.cache_k}')\n",
    "\n",
    "# # update_and_show(cache, cache_size, 2, 4, data_shape, 'InUpdate(2, 4)')\n",
    "# # update_and_show(cache, cache_size, 4, 4, data_shape, 'EndUpdate(4, 4)')\n",
    "# # update_and_show(cache, cache_size, 6, 4, data_shape, 'SpilledUpdate(6, 4)', debug=True)\n",
    "# update_and_show(cache, cache_size, 6, 8, data_shape, 'Fill(6, 8)')\n",
    "# print(\n",
    "#     # f'\\nInQuery:\\n{cache.get(cache_size, 0, 4)[0]}'\n",
    "#     # f'\\n\\nEndQuery:\\n{cache.get(cache_size, 4, 4)[0]}'\n",
    "#     f'\\n\\nSpilledQuery:\\n{cache.get(cache_size, 6, 2)[0]}'\n",
    "# )\n",
    "\n",
    "class GroupedQueryAttention(tf.keras.Model):\n",
    "    def __init__(self, block_size, heads, kv_heads, dims, cache_size):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dims = dims\n",
    "        self.head_dims = dims // self.heads\n",
    "        self.kv_heads = kv_heads or heads\n",
    "        self.block_size = block_size\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        self.wq = layers.Dense(self.dims, use_bias=False)\n",
    "        self.wk = layers.Dense(self.kv_heads * self.head_dims, use_bias=False)\n",
    "        self.wv = layers.Dense(self.kv_heads * self.head_dims, use_bias=False)\n",
    "        self.wo = layers.Dense(self.dims, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(self.cache_size, self.block_size, self.kv_heads, self.head_dims)\n",
    "        self.rope = RotaryPositionalEncodings(self.head_dims, self.block_size)\n",
    "    \n",
    "    def call(self, x, start=0, use_cache=False):\n",
    "        # print(f'{x=}\\n{start=}')\n",
    "        shape = tf.shape(x)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "\n",
    "        # (B, T, dims)\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # (B, T, heads/kv_heads, head_dims)\n",
    "        xq = tf.reshape(q, (B, T, self.heads, self.head_dims))\n",
    "        xk = tf.reshape(k, (B, T, self.kv_heads, self.head_dims))\n",
    "        xv = tf.reshape(v, (B, T, self.kv_heads, self.head_dims))\n",
    "\n",
    "        # Apply RoPE\n",
    "        # (B, T, heads/kv_heads, head_dims)\n",
    "        xq = self.rope(xq)\n",
    "        xk = self.rope(xk)\n",
    "\n",
    "        if use_cache:\n",
    "            # Update KV cache\n",
    "            self.cache.update(start, xk, xv)\n",
    "            \n",
    "            # Get prefix context from the cache.\n",
    "            # (B, start+T, kv_heads, head_dims)\n",
    "            keys, values = self.cache.get(B, start, T)\n",
    "        else:\n",
    "            assert start == 0\n",
    "            keys, values = xk, xv\n",
    "\n",
    "        # Expand kv_heads to heads\n",
    "        # (B, start+T, heads, head_dims)\n",
    "        # print(f'{keys.shape=} {values.shape=} {keys.dtype=}')\n",
    "        keys = tf.tile(keys, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "        values = tf.tile(values, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "        \n",
    "        # Transpose xq, keys and values to (B, heads, T/start+T, head_dims)\n",
    "        xq = tf.transpose(xq, perm=[0, 2, 1, 3])\n",
    "        xk = tf.transpose(keys, perm=[0, 2, 1, 3])\n",
    "        xv = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Multiply xq and xk to compute attention matrix.\n",
    "        # (B, heads, T, head_dims) @ (B, heads, head_dims, start+T) -> (B, heads, T, start+T)\n",
    "        xa = xq @ tf.transpose(xk, perm=[0, 1, 3, 2]) / math.sqrt(self.head_dims)\n",
    "\n",
    "        # Compute softmax scores.\n",
    "        if use_cache:\n",
    "            scores = tf.nn.softmax(xa)\n",
    "        else:\n",
    "            # If cache is not used, apply auto-regressive mask to block forward looking\n",
    "            tril = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)\n",
    "            scores = tf.math.softmax(tf.where(tril > 0.0, xa, float('-inf')))\n",
    "\n",
    "        # Scale Values and compute output\n",
    "        # (B, heads, T, start+T) @ (B, heads, start+T, head_dims) -> (B, heads, T, head_dims)\n",
    "        output = scores @ xv\n",
    "        \n",
    "        # Reshape output to (B, T, dims)\n",
    "        output = tf.reshape(\n",
    "            tf.transpose(output, perm=[0, 2, 1, 3]),\n",
    "            shape=(B, T, self.dims)\n",
    "        )\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "class LlamaBlock(tf.keras.Model):\n",
    "    def __init__(self, block_size, heads, kv_heads, dims, cache_size, *args, **kwargs):\n",
    "        super(LlamaBlock, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.rms_1 = RMSNorm()\n",
    "        self.attention = GroupedQueryAttention(block_size, heads, kv_heads, dims, cache_size)\n",
    "        self.rms_2 = RMSNorm()\n",
    "        self.feed_forward = FeedForward(dims)\n",
    "    \n",
    "    def call(self, x, start=0, use_cache=False):\n",
    "        x += self.attention(self.rms_1(x), start=start, use_cache=use_cache)\n",
    "        x += self.feed_forward(self.rms_2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class LlamaModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, decoders, dims,\n",
    "        block_size, heads, kv_heads, cache_size,\n",
    "        loss_fn,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super(LlamaModel, self).__init__(*args, **kwargs)\n",
    "        # Args\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # Model elements\n",
    "        self.embeddings = layers.Embedding(vocab_size, dims)\n",
    "        self.dec_blocks = [\n",
    "            LlamaBlock(block_size, heads, kv_heads, dims, cache_size)\n",
    "            for _ in range(decoders)\n",
    "        ]\n",
    "        self.rms = RMSNorm()\n",
    "        self.head = layers.Dense(vocab_size, use_bias=False)\n",
    "\n",
    "        # Loss \n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        # Metrics\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "    \n",
    "    def call(self, x, start=0, use_cache=False):\n",
    "        x_embed = self.embeddings(x)\n",
    "        x = reduce(\n",
    "            lambda y,dec_block: dec_block(y, start=start, use_cache=use_cache),\n",
    "            self.dec_blocks,\n",
    "            x_embed\n",
    "        )\n",
    "        x = self.rms(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        y = tf.cast(y, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(X)\n",
    "            loss = loss_fn(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return self.record(loss, y, logits)\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        X, y = data\n",
    "\n",
    "        logits = self(X)\n",
    "        loss = loss_fn(y, logits)\n",
    "    \n",
    "        return self.record(loss, y, logits)\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]\n",
    "    \n",
    "    def record(self, loss, y, logits):\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(y, tf.math.softmax(logits))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "block_size = 16\n",
    "m = LlamaModel(\n",
    "    vocab_size=len(char_vocab),\n",
    "    decoders=1,\n",
    "    block_size=block_size,\n",
    "    heads=4,\n",
    "    kv_heads=2,\n",
    "    dims=16,\n",
    "    cache_size=4,\n",
    "    loss_fn = loss_fn\n",
    ")\n",
    "\n",
    "m(tf.random.uniform((2, block_size), minval=0, maxval=len(char_vocab), dtype=tf.int32))\n",
    "m.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# # m.summary(expand_nested=True)\n",
    "\n",
    "train_ds = make_dataset(text[:100], block_size, char_vocab).batch(32)\n",
    "valid_ds = make_dataset(text[100:200], block_size, char_vocab).batch(32)\n",
    "# # X, y = next(iter(ds))\n",
    "\n",
    "history = m.fit(train_ds, validation_data=valid_ds)\n",
    "# decode([0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "';L3ETeThypx?WDnnq'"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model, token_idx, tokens, block_size):\n",
    "    sequence = token_idx\n",
    "    for start in range(tokens):\n",
    "        logits = model(token_idx, start=start, use_cache=True)[:, -1, :]\n",
    "        token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "idx_to_str = {i:s for i, s in enumerate(char_vocab.keys())}\n",
    "decoder = lambda x: ''.join([idx_to_str[i] for i in x])\n",
    "\n",
    "decoder(generate(m, tf.random.uniform((1, 1), maxval=m.vocab_size, dtype=tf.int32), tokens=16, block_size=16)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \" K$:EE3Tyy-HriUzsFPQ!kr.c.mah\\n:W'k . qv'zxWr$Lrs'WC\",\n",
       " 'dictwords': {'\\n', ' ', '-', '.', ':'},\n",
       " 'non-dictwords': {'hriuzsfpq!kr.c.mah', 'k$:ee3tyy', \"qv'zxwr$lrs'wc\", \"w'k\"},\n",
       " 'char_match_count': 5,\n",
       " 'char_mismatch_count': 44}"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_text_stats(text, tokenizer, fns=[]):\n",
    "    def stats_fn(token):\n",
    "        return np.array([token.lower_] + [fn(token) for fn in fns])\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    stats = np.stack([stats_fn(token) for token in tokens])\n",
    "\n",
    "    return stats\n",
    "\n",
    "def generate(model, token_idx, tokens):\n",
    "    sequence = token_idx\n",
    "    for start in range(tokens):\n",
    "        logits = model(token_idx, start=start, use_cache=True)[:, -1, :]\n",
    "        token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def generate_and_evaluate(tokenizer, model_vocab, word_vocab, generator, model, tokens=50):\n",
    "    idx_to_char = {i:c for c,i in model_vocab.items()}\n",
    "    decoder = lambda x: ''.join([idx_to_char[i] for i in x])\n",
    "\n",
    "    starter = tf.constant([model_vocab[' ']], shape=(1, 1), dtype=tf.int32)\n",
    "    generated_text = decoder(\n",
    "        generator(\n",
    "            model,\n",
    "            starter,\n",
    "            tokens=tokens,\n",
    "        )[0].numpy()\n",
    "    )\n",
    "\n",
    "    \n",
    "    stats = get_text_stats(\n",
    "        generated_text, tokenizer,\n",
    "        fns=[\n",
    "            lambda token: token.lower_ in word_vocab,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    matches = stats[:, 0][stats[:, 1] == 'True']\n",
    "    mismatches = stats[:, 0][stats[:, 1] == 'False']\n",
    "    char_match_count = np.char.str_len(matches).sum()\n",
    "    char_mismatch_count = np.char.str_len(mismatches).sum()\n",
    "    \n",
    "    return {\n",
    "        'generated_text': generated_text,\n",
    "        'dictwords': set(matches),\n",
    "        'non-dictwords': set(mismatches),\n",
    "        'char_match_count': char_match_count,\n",
    "        'char_mismatch_count': char_mismatch_count,\n",
    "    }\n",
    "\n",
    "word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "generate_and_evaluate(word_tokenizer, char_vocab, word_vocab, generate, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hello', 'False'],\n",
       "       ['and', 'True'],\n",
       "       ['welcome', 'True']], dtype='<U7')"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_stats(\n",
    "    'hello and welcome', word_tokenizer,\n",
    "    fns=[\n",
    "        lambda token: token.lower_ in word_vocab,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "# list(word_vocab.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.73 s ± 21.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "def generate_many(model_vocab, generator, model, examples=4, tokens=50):\n",
    "    starters = tf.random.uniform((examples, 1), maxval=model.vocab_size, dtype=tf.int32)\n",
    "    generator(\n",
    "        model,\n",
    "        starters,\n",
    "        tokens=tokens,\n",
    "    )\n",
    "\n",
    "generate_many(char_vocab, generate, m, examples=m.cache_size, tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14 s ± 74.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "def generate_no_cache(model, token_idx, tokens):\n",
    "    sequence = token_idx\n",
    "    for _ in range(tokens):\n",
    "        logits = model(sequence[:, -model.block_size:])[:, -1, :]\n",
    "        token_idx =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "        sequence = tf.concat([sequence, token_idx], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "generate_many(char_vocab, generate_no_cache, m, examples=m.cache_size, tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1707885216.674606       1 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 13s 4s/step - loss: 2.0317 - accuracy: 0.6607\n",
      "Epoch 2/2\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.8964 - accuracy: 0.6860\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from functools import partial\n",
    "\n",
    "class ProgressEvaluation(callbacks.Callback):\n",
    "    def __init__(self, steps, eval_fn, verbose=False):\n",
    "        super(ProgressEvaluation, self).__init__()\n",
    "        self.steps = steps\n",
    "        self.eval_fn = eval_fn\n",
    "        self.stats = pd.DataFrame(\n",
    "            columns=[\n",
    "                'epoch', 'step', 'loss',\n",
    "                'dictwords', 'non-dictwords',\n",
    "                'char_match_count', 'char_mismatch_count',\n",
    "                'generated_text',\n",
    "            ]\n",
    "        )\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.step = 0\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "\n",
    "        # Log results every 'steps' steps\n",
    "        if self.step % self.steps == 0:\n",
    "            results = self.eval_fn(self.model)\n",
    "            results.update({\n",
    "                'epoch': self.epoch,\n",
    "                'step': self.step,\n",
    "                'loss': logs.get(\"loss\"),\n",
    "            })\n",
    "\n",
    "            self.stats.loc[len(self.stats)] = pd.Series(results)\n",
    "\n",
    "            if self.verbose:\n",
    "                char_match_rate = results['char_match_count'] / (results['char_match_count'] + results['char_mismatch_count'])\n",
    "\n",
    "                print(\n",
    "                    f\"\\nEpoch: {results['epoch']} Step: {results['step']} Loss={results['loss']:.3}\"\n",
    "                    f\"\\nGenerated: {results['generated_text']}\"\n",
    "                    f\"\\n\\nStatistics\\n===============\"\n",
    "                    f\"\\nDictionary Words: {results['dictwords']}\"\n",
    "                    f\"\\nNon-dictionary Words: {results['non-dictwords']}\"\n",
    "                    f\"\\nChar Matches: {results['char_match_count']}\"\n",
    "                    f\"\\nChar Mismatches: {results['char_mismatch_count']}\"\n",
    "                    f\"\\nChar Match Rate: {char_match_rate:.2%}\"\n",
    "                )\n",
    "\n",
    "eval_fn = partial(generate_and_evaluate, word_tokenizer, word_vocab, generate, decoder)\n",
    "# eval_cb = ProgressEvaluation(steps=2000, eval_fn=eval_fn)\n",
    "eval_cb = ProgressEvaluation(steps=1, eval_fn=eval_fn)\n",
    "\n",
    "train_ds = make_dataset(text[:100], block_size, char_vocab).batch(32)\n",
    "\n",
    "m.compile(optimizer=optimizer, loss=loss_fn)\n",
    "history = m.fit(train_ds, callbacks=[eval_cb], epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_cb.stats.to_pickle('train.progress.stats.pickle')\n",
    "m.save_weights('llama.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Set\n",
      "==============\n",
      "X.shape=TensorShape([2, 5]) y.shape=TensorShape([2, 5]) [[22  8 28 28 14]\n",
      " [ 8 28 28 14 25]] [[ 8 28 28 14 25]\n",
      " [28 28 14 25  3]]\n",
      "X.numpy()=array([[22,  8, 28, 28, 14],\n",
      "       [ 8, 28, 28, 14, 25]]) y.numpy()=array([[ 8, 28, 28, 14, 25],\n",
      "       [28, 28, 14, 25,  3]])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "data = 'helloksdhglsadjgalsdhglaksddd'\n",
    "block_size = 5\n",
    "\n",
    "# ds = tf.data.Dataset.range(len(data) - block_size - 1).map(\n",
    "#     # lambda index: data[index:index+block_size], data[index+1:index+block_size+1]\n",
    "#     lambda index: data[index:index+block_size]\n",
    "# )\n",
    "with open('input.txt') as f:\n",
    "    tiny_shakespere = ''.join(f.readlines())\n",
    "\n",
    "char_frequencies = Counter(chain(*tiny_shakespere))\n",
    "char_vocab = dict(zip(char_frequencies.keys(), range(len(char_frequencies))))\n",
    "\n",
    "def make_dataset(data, block_size, vocab):\n",
    "    def indices(phrase):\n",
    "        return list(map(vocab.get, phrase))\n",
    "\n",
    "    @tf.numpy_function(Tout=(tf.int64, tf.int64))\n",
    "    def get_numpy_item(index):\n",
    "        X, y = data[index:index+block_size], data[index+1:index+block_size+1]\n",
    "        return indices(X), indices(y)\n",
    "\n",
    "    def get_item(index):\n",
    "        X, y = get_numpy_item(index)\n",
    "        return tf.reshape(X, [block_size]), tf.reshape(y, [block_size])\n",
    "\n",
    "ds = make_dataset(data, block_size, char_vocab).batch(2)\n",
    "X, y = next(iter(ds))\n",
    "\n",
    "print(\n",
    "    f'\\n\\nTrain Set\\n=============='\n",
    "    f'\\n{X.shape=} {y.shape=} {X} {y}'\n",
    "    f'\\n{X.numpy()=} {y.numpy()=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4, 16, 4, 4)\n",
    "cache = tf.zeros(shape)\n",
    "\n",
    "filled = tf.tensor_scatter_nd_update(\n",
    "    cache, tf.stack(tf.unravel_index(range(4**3*16), dims=shape), axis=-1)[2*16*4*4:3*16*4*4], tf.reshape(tf.random.uniform(shape[1:]), -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 4)\n"
     ]
    }
   ],
   "source": [
    "# tf.unravel_index(range(4**3*16), dims=shape)\n",
    "# filled[2]\n",
    "# tf.stack(tf.unravel_index(range(4**3*16), dims=shape), axis=-1)[2*16*4*4:3*16*4*4]\n",
    "start, seqlen = 2, 8\n",
    "\n",
    "indices = tf.transpose(tf.unravel_index(tf.reshape(tf.reshape(tf.range(4*16*4*4), shape)[:2, start:start+seqlen], (-1)), dims=shape))\n",
    "\n",
    "filled = tf.tensor_scatter_nd_update(\n",
    "    cache, indices, tf.reshape(tf.random.uniform((2*seqlen, 4, 4)), (-1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "char_vocab = OrderedDict(zip(\n",
    "    char_frequencies.keys(),\n",
    "    range(len(char_frequencies)),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen', ':', '\\n', 'before']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_word_1gram_frequencies(data, tokenizer):\n",
    "    words = map(lambda x:x.lower_, itertools.chain(*map(tokenizer, data)))\n",
    "    return Counter(words)\n",
    "\n",
    "word_tokenizer = spacy.blank(\"en\")\n",
    "word_vocab = get_word_1gram_frequencies(tiny_shakespere, word_tokenizer)\n",
    "\n",
    "print(\n",
    "    f'{list(word_vocab.keys())[:5]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, [First, Citizen])"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "word_tokenizer = spacy.blank(\"en\")\n",
    "\n",
    "# words = list(map(lambda x:x.lower_, map(word_tokenizer, tiny_shakespere)))\n",
    "# words[:5]\n",
    "# word_tokenizer(tiny_shakespere[0])\n",
    "tokens = list(itertools.chain(*map(word_tokenizer, tiny_shakespere[:2])))\n",
    "len(tokens), tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 16\n",
    "m = LlamaModel(\n",
    "    vocab_size=len(char_vocab),\n",
    "    encoders=2,\n",
    "    block_size=block_size,\n",
    "    heads=4,\n",
    "    kv_heads=2,\n",
    "    dims=16,\n",
    "    cache_size=4,\n",
    "    loss_fn = loss_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 15:37:30.381841: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2514055488347275762\n",
      "2024-02-13 15:37:30.381913: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6864595417707083459\n",
      "2024-02-13 15:37:30.382219: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15030648477679010439\n",
      "2024-02-13 15:37:30.382254: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7802382919658029917\n",
      "2024-02-13 15:37:30.382278: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12903173487718679601\n",
      "2024-02-13 15:37:30.382297: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11760740079638168607\n",
      "2024-02-13 15:37:30.382310: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6637692130073347793\n",
      "2024-02-13 15:37:30.382323: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14603492633315985271\n",
      "2024-02-13 15:37:30.382330: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15951688805606999901\n",
      "2024-02-13 15:37:30.382344: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 381899899561756459\n",
      "2024-02-13 15:37:30.382355: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2514027840537234603\n",
      "2024-02-13 15:37:30.382367: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3438022172517190801\n",
      "2024-02-13 15:37:30.382375: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 197092890153700665\n",
      "2024-02-13 15:37:30.382380: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4173523409733079095\n",
      "2024-02-13 15:37:30.382393: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9665836595423973697\n",
      "2024-02-13 15:37:30.382401: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 830150852124728019\n",
      "2024-02-13 15:37:30.382411: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12695135116972645173\n",
      "2024-02-13 15:37:30.382423: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17674184855639694479\n",
      "2024-02-13 15:37:30.382434: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13094457564342214015\n",
      "2024-02-13 15:37:30.382441: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11129344426395967339\n",
      "2024-02-13 15:37:30.382455: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4252389173442634047\n",
      "2024-02-13 15:37:30.382471: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2364733028026967255\n",
      "2024-02-13 15:37:30.382475: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8947033755602265559\n",
      "2024-02-13 15:37:30.382481: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15261844553326847343\n",
      "2024-02-13 15:37:30.382490: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13064368233433097353\n",
      "2024-02-13 15:37:30.382501: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1111461634275043397\n",
      "2024-02-13 15:37:30.382511: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14628935742189480359\n",
      "2024-02-13 15:37:30.382515: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14794280897173449631\n",
      "2024-02-13 15:37:30.382524: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2974762685181848817\n",
      "2024-02-13 15:37:30.382536: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5221406357241179621\n",
      "2024-02-13 15:37:30.382543: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15868122080306622129\n",
      "2024-02-13 15:37:30.382553: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 203811043960813687\n",
      "2024-02-13 15:37:30.382561: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4873907337148560133\n",
      "2024-02-13 15:37:30.382565: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8581514927131407373\n",
      "2024-02-13 15:37:30.382571: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9117954867849688811\n",
      "2024-02-13 15:37:30.382582: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8654693644371146183\n",
      "2024-02-13 15:37:30.382594: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9988423997565728645\n",
      "2024-02-13 15:37:30.382605: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16914549742754741479\n",
      "2024-02-13 15:37:30.382611: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8664742996445936959\n",
      "2024-02-13 15:37:30.382628: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16659934675399835317\n",
      "2024-02-13 15:37:30.382634: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4040839841508327377\n",
      "2024-02-13 15:37:30.382671: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10563067841934889124\n",
      "2024-02-13 15:37:30.382688: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2857676523195533870\n",
      "2024-02-13 15:37:30.382697: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10111538548217255414\n",
      "2024-02-13 15:37:30.382709: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 326251628134208964\n",
      "2024-02-13 15:37:30.382713: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11103050601756731518\n",
      "2024-02-13 15:37:30.382716: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12214692318589102712\n",
      "2024-02-13 15:37:30.382728: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9665215515700308940\n",
      "2024-02-13 15:37:30.382740: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5169901217921936366\n",
      "2024-02-13 15:37:30.382750: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15607399778109075976\n",
      "2024-02-13 15:37:30.382762: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17535420950351719844\n",
      "2024-02-13 15:37:30.382768: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3421553381043156850\n",
      "2024-02-13 15:37:30.382776: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5408839190683990310\n",
      "2024-02-13 15:37:30.382789: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10599363125358934484\n",
      "2024-02-13 15:37:30.382795: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13625695197051732602\n",
      "2024-02-13 15:37:30.382799: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16804833522958452832\n",
      "2024-02-13 15:37:30.382810: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11413049869872810782\n",
      "2024-02-13 15:37:30.382826: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1899431046360223072\n",
      "2024-02-13 15:37:30.382834: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9580981997173228578\n",
      "2024-02-13 15:37:30.382848: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11509613329645686762\n",
      "2024-02-13 15:37:30.382852: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10801926379432661442\n",
      "2024-02-13 15:37:30.382857: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10442432732762461330\n",
      "2024-02-13 15:37:30.382868: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16305145690179753700\n",
      "2024-02-13 15:37:30.382878: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7290806338182991488\n",
      "2024-02-13 15:37:30.382885: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12254037608494225942\n",
      "2024-02-13 15:37:30.382889: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17359276179916673472\n",
      "2024-02-13 15:37:30.382895: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1421225622130088780\n",
      "2024-02-13 15:37:30.382920: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17182410178134367524\n",
      "2024-02-13 15:37:30.382932: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10275511998764245464\n",
      "2024-02-13 15:37:30.382945: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8516475552621225750\n",
      "2024-02-13 15:37:30.382949: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15158309868554316338\n",
      "2024-02-13 15:37:30.382956: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4247225630413941812\n",
      "2024-02-13 15:37:30.382962: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14276461473725935202\n",
      "2024-02-13 15:37:30.382969: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4699278737390727598\n",
      "2024-02-13 15:37:30.382976: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10674145053824525430\n",
      "2024-02-13 15:37:30.382986: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13539949860378997538\n",
      "2024-02-13 15:37:30.382991: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5814108912858134094\n",
      "2024-02-13 15:37:30.382999: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14448056444759865800\n",
      "2024-02-13 15:37:30.383003: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16032063718558366708\n",
      "2024-02-13 15:37:30.383007: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10532382451127287056\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 15, in <module>\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 3, in generate\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 5, in generate\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 123, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 67, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 69, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/2638763864.py\", line 22, in update\n\nDetected at node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 15, in <module>\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 3, in generate\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 5, in generate\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 123, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 67, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 69, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/2638763864.py\", line 22, in update\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Cannot broadcast input shape [1,9,2,4] into final shape [1,8,2,4]\n\t [[{{node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign}}]]\n\t [[llama_model_168/llama_block_350/grouped_query_attention_410/ReadVariableOp_34/_240]]\n  (1) INVALID_ARGUMENT:  Cannot broadcast input shape [1,9,2,4] into final shape [1,8,2,4]\n\t [[{{node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_generate_306524]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[615], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m idx_to_str \u001b[38;5;241m=\u001b[39m {i:s \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_vocab\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[1;32m     13\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([idx_to_str[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[0;32m---> 15\u001b[0m decode(\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.venv-tensorflow/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.venv-tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 15, in <module>\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 3, in generate\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 5, in generate\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 123, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 67, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 69, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/2638763864.py\", line 22, in update\n\nDetected at node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 15, in <module>\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 3, in generate\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/851780790.py\", line 5, in generate\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 155, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/1265064723.py\", line 123, in call\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/broxoli/.venv-tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 67, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/200258366.py\", line 69, in call\n\n  File \"/var/folders/8y/5694n0_n42j4fmg6d5j411480000gn/T/ipykernel_13484/2638763864.py\", line 22, in update\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Cannot broadcast input shape [1,9,2,4] into final shape [1,8,2,4]\n\t [[{{node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign}}]]\n\t [[llama_model_168/llama_block_350/grouped_query_attention_410/ReadVariableOp_34/_240]]\n  (1) INVALID_ARGUMENT:  Cannot broadcast input shape [1,9,2,4] into final shape [1,8,2,4]\n\t [[{{node llama_model_168/llama_block_349/grouped_query_attention_409/llama_model_168/llama_block_349/grouped_query_attention_409/strided_slice_69/_assign}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_generate_306524]"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def generate(sequence, tokens, block_size):\n",
    "    for start in range(tokens):\n",
    "        sequence = sequence[:, -block_size:]\n",
    "        logits = m(sequence, start=start, use_cache=True)[:, -1, :]\n",
    "        prediction =  tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "\n",
    "        sequence = tf.concat([sequence, prediction], axis=-1)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "idx_to_str = {i:s for i, s in enumerate(char_vocab.keys())}\n",
    "decode = lambda x: ''.join([idx_to_str[i] for i in x])\n",
    "\n",
    "decode(generate(tf.zeros((1, 1), dtype=tf.int32), tokens=block_size, block_size=16)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fill(6, 8):\n",
      "<tf.Variable 'Variable:0' shape=(1, 8, 1, 4) dtype=float32, numpy=\n",
      "array([[[[0.95810425, 0.55623317, 0.16466296, 0.13445711]],\n",
      "\n",
      "        [[0.13229859, 0.5348098 , 0.57090175, 0.50970507]],\n",
      "\n",
      "        [[0.48252344, 0.15580535, 0.3703227 , 0.49210668]],\n",
      "\n",
      "        [[0.567016  , 0.2077086 , 0.18223882, 0.99883735]],\n",
      "\n",
      "        [[0.36950588, 0.37927854, 0.7723117 , 0.68211746]],\n",
      "\n",
      "        [[0.39932835, 0.7840713 , 0.67880154, 0.73395896]],\n",
      "\n",
      "        [[0.31179297, 0.8263413 , 0.6849456 , 0.0067091 ]],\n",
      "\n",
      "        [[0.78749514, 0.3906511 , 0.29263055, 0.99216926]]]],\n",
      "      dtype=float32)>\n",
      "\n",
      "\n",
      "SpilledQuery:\n",
      "[[[[0.95810425 0.55623317 0.16466296 0.13445711]]\n",
      "\n",
      "  [[0.13229859 0.5348098  0.57090175 0.50970507]]\n",
      "\n",
      "  [[0.48252344 0.15580535 0.3703227  0.49210668]]\n",
      "\n",
      "  [[0.567016   0.2077086  0.18223882 0.99883735]]\n",
      "\n",
      "  [[0.36950588 0.37927854 0.7723117  0.68211746]]\n",
      "\n",
      "  [[0.39932835 0.7840713  0.67880154 0.73395896]]\n",
      "\n",
      "  [[0.31179297 0.8263413  0.6849456  0.0067091 ]]\n",
      "\n",
      "  [[0.78749514 0.3906511  0.29263055 0.99216926]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class KVCache(object):\n",
    "    def __init__(self, cache_size, block_size, heads, head_dims):\n",
    "        super(KVCache, self).__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        cache_shape = (cache_size, block_size, heads, head_dims)\n",
    "\n",
    "        with tf.device('/device:CPU:0'):\n",
    "            self.cache_k = tf.Variable(tf.zeros(cache_shape), shape=cache_shape, trainable=False)\n",
    "            self.cache_v = tf.Variable(tf.zeros(cache_shape), shape=cache_shape, trainable=False)\n",
    "\n",
    "    def update(self, start, xk, xv):\n",
    "        shape = tf.shape(xk)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "\n",
    "        # Calculate update start and end positions\n",
    "        start = start%self.block_size\n",
    "        end = (start + T)%(self.block_size + 1)\n",
    "\n",
    "        # start < end: It is a single cache update.\n",
    "        # end > start: It is a split cache update.\n",
    "        if start < end:\n",
    "            self.cache_k[:B, start:start+T].assign(xk)\n",
    "            self.cache_v[:B, start:start+T].assign(xv)\n",
    "        else:\n",
    "            # Update cache with partial sequence that fits towards the end.\n",
    "            self.cache_k[:B, start:].assign(xk[:, :-(end+1)])\n",
    "            self.cache_v[:B, start:].assign(xv[:, :-(end+1)])\n",
    "\n",
    "            # Splillover sequence is cached towards the front of the cache.\n",
    "            self.cache_k[:B, :end+1].assign(xk[:, -(end+1):])\n",
    "            self.cache_v[:B, :end+1].assign(xv[:, -(end+1):])\n",
    "\n",
    "    # TODO:: Update the callers to reflect the args order change.\n",
    "    def get(self, batch_size, start, seq_len):\n",
    "        # Calculate update start and end positions\n",
    "        start = start%self.block_size\n",
    "        end = (start + seq_len)%(self.block_size + 1)\n",
    "\n",
    "        # start < end: It is a single cache fetch.\n",
    "        # end > start: It is a split cache fetch.\n",
    "        if start < end:\n",
    "            keys = self.cache_k[:batch_size, :start+seq_len]\n",
    "            values = self.cache_v[:batch_size, :start+seq_len]\n",
    "        else:\n",
    "            # Fetch sequence prefix\n",
    "            keys_1 = self.cache_k[:, (end+1):]\n",
    "            values_1 = self.cache_k[:, (end+1):]\n",
    "\n",
    "            # Fetch sequence suffix\n",
    "            keys_2 = self.cache_k[:, :(end+1):]\n",
    "            values_2 = self.cache_k[:, :(end+1):]\n",
    "\n",
    "            # Compose the whole sequence\n",
    "            keys = tf.concat([keys_1, keys_2], axis=1)\n",
    "            values = tf.concat([values_1, values_2], axis=1)\n",
    "\n",
    "        return keys, values\n",
    "\n",
    "def update_and_show(cache, batch_size, start, seqlen, data_shape, msg, debug=False):\n",
    "    xk = tf.random.uniform((batch_size, seqlen, *data_shape))\n",
    "    xv = tf.random.uniform((batch_size, seqlen, *data_shape))\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            f'\\n{msg}::xk:\\n{xk}'\n",
    "        )\n",
    "\n",
    "    cache.update(start, xk, xv)\n",
    "    print(\n",
    "        f'\\n{msg}:\\n{cache.cache_k}'\n",
    "    )\n",
    "\n",
    "cache_size, block_size, heads, head_dims = 1, 8, 1, 4\n",
    "cache = KVCache(cache_size, block_size, heads, head_dims)\n",
    "data_shape = (heads, head_dims)\n",
    "\n",
    "# print(f'\\nInitial:\\n{cache.cache_k}')\n",
    "\n",
    "# update_and_show(cache, cache_size, 2, 4, data_shape, 'InUpdate(2, 4)')\n",
    "# update_and_show(cache, cache_size, 4, 4, data_shape, 'EndUpdate(4, 4)')\n",
    "# update_and_show(cache, cache_size, 6, 4, data_shape, 'SpilledUpdate(6, 4)', debug=True)\n",
    "update_and_show(cache, cache_size, 6, 8, data_shape, 'Fill(6, 8)')\n",
    "print(\n",
    "    # f'\\nInQuery:\\n{cache.get(cache_size, 0, 4)[0]}'\n",
    "    # f'\\n\\nEndQuery:\\n{cache.get(cache_size, 4, 4)[0]}'\n",
    "    f'\\n\\nSpilledQuery:\\n{cache.get(cache_size, 6, 2)[0]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 128), dtype=float32, numpy=\n",
       "array([[[-2.33013868e-01, -2.07698846e+00,  3.48630995e-01,\n",
       "         -5.10134697e-01, -2.54128367e-01,  5.40200472e-01,\n",
       "          3.85206193e-02,  7.73041844e-01, -1.09596026e+00,\n",
       "          1.01041520e+00,  9.91183937e-01, -7.66317129e-01,\n",
       "         -5.28275371e-02, -1.04835615e-01, -3.37260664e-01,\n",
       "         -7.06953108e-01,  6.96793646e-02, -5.24373591e-01,\n",
       "         -1.54647708e+00, -1.07588410e-01, -1.45081893e-01,\n",
       "         -4.36975539e-01,  2.54979879e-01,  7.04379976e-02,\n",
       "         -6.10021055e-01,  2.36156583e-02,  1.18972093e-01,\n",
       "         -4.18260843e-01,  1.39139616e+00,  7.75396824e-01,\n",
       "         -7.95781732e-01,  2.60917783e-01,  1.12052810e+00,\n",
       "         -4.42397982e-01,  9.55890119e-02, -1.09111333e+00,\n",
       "         -4.14958864e-01, -3.14540088e-01,  1.01840663e+00,\n",
       "          9.63069439e-01,  3.75590533e-01,  1.06295928e-01,\n",
       "          9.52532589e-01, -6.47454500e-01, -9.87813771e-02,\n",
       "          3.03195119e-02, -8.26753795e-01,  2.42982864e-01,\n",
       "         -1.02700686e+00, -3.04251909e-01, -9.69981790e-01,\n",
       "         -4.38145548e-02,  3.03879678e-01, -2.43207514e-02,\n",
       "         -1.03126705e-01,  2.16522813e-02,  2.70205796e-01,\n",
       "          1.21611834e-01,  2.31811225e-01,  1.09439647e+00,\n",
       "          5.63949645e-01,  3.55636358e-01,  5.22968888e-01,\n",
       "         -6.23510778e-02,  3.39120179e-02,  2.41752654e-01,\n",
       "          7.28477895e-01, -4.23570722e-02, -8.87252331e-01,\n",
       "          7.26693213e-01,  1.83197355e+00,  1.19015801e+00,\n",
       "          7.37680793e-01, -1.98404777e+00,  4.05928135e-01,\n",
       "          8.56539845e-01, -1.24492240e+00,  8.89181972e-01,\n",
       "          1.00185192e+00,  6.20441616e-01,  2.33951664e+00,\n",
       "         -1.44186378e-01,  3.02072346e-01, -3.21748555e-02,\n",
       "         -3.92948687e-01,  3.95849735e-01, -4.64025974e-01,\n",
       "          1.15963638e+00, -1.04049444e-01, -1.83055329e+00,\n",
       "         -1.12167135e-01,  8.17037165e-01,  1.21699184e-01,\n",
       "         -1.83107674e-01, -3.45000565e-01,  3.07320625e-01,\n",
       "         -1.02961451e-01,  6.15191162e-01,  1.17726848e-01,\n",
       "         -2.63493598e-01, -3.78895521e-01,  5.95973611e-01,\n",
       "         -1.09734309e+00, -1.89008683e-01, -3.67648780e-01,\n",
       "         -2.47987211e-02, -5.34743011e-01, -1.31235075e+00,\n",
       "          1.09628606e+00, -1.36618569e-01,  1.43863726e+00,\n",
       "          1.56310350e-01, -1.05250597e+00, -1.59448934e+00,\n",
       "          5.89379549e-01,  9.52339411e-01, -1.07511282e-02,\n",
       "          1.57201004e+00, -7.04744935e-01, -1.58537352e+00,\n",
       "         -6.61066532e-01,  2.61142582e-01,  3.27916741e-01,\n",
       "         -3.07215959e-01,  7.85151124e-01,  2.51918375e-01,\n",
       "         -1.10201955e+00, -2.39092052e-01],\n",
       "        [-2.14184895e-01, -1.88882172e+00,  5.68711400e-01,\n",
       "         -3.61778080e-01, -1.12958252e-03,  4.08074111e-01,\n",
       "         -1.88109219e-01,  8.31999063e-01, -1.09467721e+00,\n",
       "          7.79281080e-01,  1.32460999e+00, -6.98273540e-01,\n",
       "         -9.70128775e-02,  2.25669473e-01, -5.76772392e-02,\n",
       "         -8.95307779e-01,  1.38712198e-01, -1.84313595e-01,\n",
       "         -1.50417209e+00, -5.22812642e-02, -2.88270228e-02,\n",
       "         -2.89791703e-01,  8.23795438e-01,  1.60441965e-01,\n",
       "         -1.07643366e+00, -1.45032182e-02,  2.10072815e-01,\n",
       "          1.69814870e-01,  1.50518894e+00,  7.39587903e-01,\n",
       "         -5.75225294e-01,  2.76099503e-01,  1.03915215e+00,\n",
       "          2.97937244e-02, -2.15897262e-01, -8.67353201e-01,\n",
       "         -3.54631633e-01, -1.86105818e-01,  1.06359076e+00,\n",
       "          8.02866518e-01,  3.81140381e-01, -2.77664606e-02,\n",
       "          5.89547157e-01, -4.04906869e-01, -3.35873216e-02,\n",
       "         -2.81164702e-02,  2.27670103e-01,  2.20098794e-01,\n",
       "         -1.00217390e+00, -3.83879870e-01, -8.71053696e-01,\n",
       "         -1.12953618e-01,  1.58827439e-01, -3.92658919e-01,\n",
       "         -2.00258970e-01,  2.34971046e-01,  3.88769835e-01,\n",
       "          9.71208364e-02,  3.77810866e-01,  1.28298116e+00,\n",
       "          7.68164337e-01,  2.50841111e-01,  3.40052038e-01,\n",
       "         -1.31294146e-01,  1.42121732e-01,  1.29877880e-01,\n",
       "          7.84314394e-01, -1.48599669e-01, -6.94779515e-01,\n",
       "          4.04597819e-01,  1.88637066e+00,  9.98544931e-01,\n",
       "          6.57332063e-01, -1.78543735e+00,  4.52281684e-01,\n",
       "          9.11153853e-02, -1.35345650e+00,  8.44253302e-01,\n",
       "          5.86064279e-01,  4.99947101e-01,  2.05448818e+00,\n",
       "         -3.03629369e-01,  3.56037557e-01, -4.76312563e-02,\n",
       "         -4.17460263e-01,  4.55630660e-01, -4.99701321e-01,\n",
       "          1.05989289e+00, -1.27470195e-02, -1.43457019e+00,\n",
       "         -4.85921279e-02,  8.33869517e-01, -2.88517475e-01,\n",
       "         -6.62816539e-02, -5.89908808e-02,  1.18262164e-01,\n",
       "          2.42248714e-01,  5.71645081e-01,  9.75425184e-01,\n",
       "          3.06602031e-01, -1.10574758e+00,  4.40440148e-01,\n",
       "         -4.28913653e-01, -1.06195338e-01,  5.87962940e-02,\n",
       "         -8.60541761e-02, -4.21699107e-01, -5.61118245e-01,\n",
       "          1.95983481e+00, -2.49576807e-01,  6.01611733e-01,\n",
       "          4.14976150e-01, -9.61654067e-01, -1.48494446e+00,\n",
       "          5.12074709e-01,  9.68909383e-01,  3.56900632e-01,\n",
       "          1.41958427e+00, -6.42056108e-01, -1.27917361e+00,\n",
       "         -5.92625141e-01,  5.29986560e-01,  3.46081495e-01,\n",
       "         -1.29144624e-01,  9.05519605e-01,  3.98545384e-01,\n",
       "         -7.50033855e-01, -2.65499651e-01]],\n",
       "\n",
       "       [[-5.21975458e-01, -2.20918798e+00, -6.16754889e-02,\n",
       "         -6.10476613e-01,  5.98603487e-01,  3.69387537e-01,\n",
       "          2.17749044e-01,  1.92002758e-01, -9.72190082e-01,\n",
       "         -1.67989910e-01,  1.09873390e+00, -1.01808238e+00,\n",
       "         -4.92182761e-01,  6.42049909e-02,  2.00000256e-01,\n",
       "         -7.78663278e-01,  1.71578944e-01, -3.17830473e-01,\n",
       "         -5.44842839e-01,  4.41054165e-01, -3.79922301e-01,\n",
       "         -1.24254477e+00,  1.14214969e+00,  2.42695987e-01,\n",
       "         -5.40131867e-01, -4.97084677e-01,  5.00694752e-01,\n",
       "         -1.14988744e-01,  9.12439227e-01, -3.25810909e-02,\n",
       "         -1.12315989e+00,  3.23907614e-01,  7.55847931e-01,\n",
       "         -1.53701156e-01, -7.42291510e-02, -1.60808992e+00,\n",
       "         -2.31429964e-01, -9.67645586e-01,  2.41771355e-01,\n",
       "          4.54119653e-01, -8.26646239e-02,  8.57758522e-01,\n",
       "          8.45130503e-01, -8.09810042e-01, -6.26243353e-02,\n",
       "          7.48508930e-01, -2.51691580e-01, -1.64359555e-01,\n",
       "         -1.18720531e+00, -2.64544070e-01, -5.45326531e-01,\n",
       "         -1.21589065e-01, -7.44915128e-01, -1.90845847e-01,\n",
       "          1.51660889e-01, -3.81258011e-01,  1.34299433e+00,\n",
       "          3.34561914e-01,  7.70997167e-01,  1.06384444e+00,\n",
       "          3.57102036e-01,  7.34702229e-01, -6.69335723e-02,\n",
       "         -1.47421062e-02, -1.95913941e-01,  5.97417772e-01,\n",
       "          7.49673784e-01, -1.68695152e-01, -1.32988304e-01,\n",
       "          3.14192265e-01,  1.47723734e+00,  1.44271517e+00,\n",
       "          9.51879501e-01, -1.62437463e+00,  1.01226830e+00,\n",
       "         -3.01296413e-02, -1.36687267e+00,  5.16579747e-01,\n",
       "          1.53260779e+00,  3.84713888e-01,  2.35727119e+00,\n",
       "         -7.36180604e-01,  1.88861579e-01,  1.77470744e-02,\n",
       "         -7.64713883e-01,  7.84181952e-01, -3.08228016e-01,\n",
       "          1.12138736e+00,  3.52444351e-02, -5.01287937e-01,\n",
       "         -7.34621137e-02,  1.19701123e+00, -3.82861197e-02,\n",
       "          5.35379291e-01,  1.07826233e-01,  4.98953938e-01,\n",
       "          4.04294431e-01, -1.18599296e-01,  6.39577806e-01,\n",
       "          6.93605781e-01,  1.14611983e-02,  1.06420174e-01,\n",
       "         -3.65758896e-01, -5.56590557e-02, -3.52192819e-02,\n",
       "          4.24929857e-01, -3.17503750e-01, -1.11218762e+00,\n",
       "          1.80809617e+00, -4.16231036e-01,  5.85706592e-01,\n",
       "          9.04693305e-01, -1.10558605e+00, -1.15909123e+00,\n",
       "         -1.59682408e-01,  6.84089780e-01,  3.39788258e-01,\n",
       "          1.22739935e+00, -5.65304637e-01, -1.03338540e+00,\n",
       "         -3.65314841e-01, -6.66239023e-01,  1.37835610e+00,\n",
       "          2.73522198e-01,  9.27032530e-01,  7.44965076e-01,\n",
       "         -9.98761177e-01, -8.97925377e-01],\n",
       "        [-7.57329106e-01, -1.94322276e+00, -3.09356060e-02,\n",
       "         -4.15606618e-01,  3.75590652e-01,  5.78759789e-01,\n",
       "         -8.33908990e-02,  1.85219318e-01, -1.01454961e+00,\n",
       "         -1.75448865e-01,  1.22354758e+00, -7.20585346e-01,\n",
       "         -6.67517185e-01,  9.74944979e-02,  1.57899261e-02,\n",
       "         -5.97766399e-01,  7.44991899e-02, -1.16617784e-01,\n",
       "         -8.45418394e-01,  3.40804040e-01, -5.95213473e-03,\n",
       "         -9.45147634e-01,  1.21840763e+00,  4.30992097e-01,\n",
       "         -3.25219572e-01, -1.32566929e-01,  4.12920654e-01,\n",
       "         -1.33334070e-01,  1.01280308e+00,  4.82517555e-02,\n",
       "         -1.08231473e+00,  3.04354072e-01,  7.96729922e-01,\n",
       "         -4.36134726e-01,  3.73325527e-01, -1.49085474e+00,\n",
       "         -3.39406669e-01, -8.90040040e-01,  2.01819345e-01,\n",
       "          7.42101610e-01,  1.47022977e-01,  2.05864549e-01,\n",
       "          7.59026647e-01, -5.41927338e-01, -9.26959813e-02,\n",
       "          4.46156204e-01, -6.13792360e-01, -1.21564209e-01,\n",
       "         -9.39486504e-01, -3.04844677e-01, -4.69348669e-01,\n",
       "         -2.77075350e-01, -2.97763795e-02,  1.55088603e-01,\n",
       "         -1.02079324e-02,  4.30526674e-01,  1.16693091e+00,\n",
       "          3.34177017e-01,  6.67824626e-01,  8.57354045e-01,\n",
       "          2.34939903e-01,  8.23432326e-01,  3.53349417e-01,\n",
       "          1.13029025e-01,  9.62423757e-02,  3.88443947e-01,\n",
       "          6.35670185e-01, -5.54486752e-01, -2.49692664e-01,\n",
       "          3.08929622e-01,  1.49199367e+00,  1.22285509e+00,\n",
       "          1.05682981e+00, -1.59931731e+00,  9.09410954e-01,\n",
       "         -1.48895010e-01, -8.70108545e-01,  4.46521610e-01,\n",
       "          1.52672231e+00,  2.91478336e-01,  2.22873592e+00,\n",
       "         -7.62237251e-01,  5.02942860e-01, -2.04146415e-01,\n",
       "         -8.03377986e-01,  4.44790006e-01, -2.78158844e-01,\n",
       "          1.24960268e+00,  8.14462751e-02, -5.59666693e-01,\n",
       "         -1.14989966e-01,  1.14252257e+00, -3.71229649e-01,\n",
       "          4.22233880e-01,  2.56522834e-01,  3.30153704e-01,\n",
       "         -3.40999186e-01,  5.09610832e-01,  5.07335305e-01,\n",
       "          2.77467132e-01, -1.41673580e-01,  6.00716650e-01,\n",
       "         -3.08706284e-01, -7.27990568e-02, -2.00221792e-01,\n",
       "         -7.14544803e-02, -6.05044007e-01, -7.10211933e-01,\n",
       "          1.90947163e+00, -2.23814234e-01,  8.45365345e-01,\n",
       "          4.35462147e-01, -1.18778300e+00, -1.36360168e+00,\n",
       "          6.50706649e-01,  1.03214598e+00,  3.27875912e-01,\n",
       "          1.23132658e+00, -1.24810494e-01, -1.09652102e+00,\n",
       "         -4.79429841e-01, -4.38913167e-01,  9.86326098e-01,\n",
       "         -2.37592727e-01,  1.04119754e+00,  2.65951991e-01,\n",
       "         -7.03522563e-01, -8.47859383e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.key = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "    self.query = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "    self.value = tf.keras.layers.Dense(head_size, use_bias=False)\n",
    "    self.head_size = head_size\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    B, T, C = x.shape\n",
    "\n",
    "    k = self.key(x)\n",
    "    q = self.query(x)\n",
    "    v = self.query(x)\n",
    "\n",
    "    wei = k @ tf.transpose(q, perm=[0, 2, 1]) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "    wei /= tf.cast(tf.math.sqrt(self.head_size * 1.), dtype=x.dtype)\n",
    "\n",
    "    tril = tf.linalg.band_part(\n",
    "        tf.ones((T, T)), -1, 0\n",
    "    )\n",
    "\n",
    "    wei = tf.nn.softmax(tf.where(tril > 0.0, wei, float('-inf')))\n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.attn_layers = [SelfAttentionLayer(head_size) for i in range(num_heads)]\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    return tf.concat([\n",
    "        attn_layer(x) for attn_layer in self.attn_layers\n",
    "    ], axis=-1)\n",
    "\n",
    "\n",
    "B, T, C = 2, 2, 16\n",
    "l = MultiHeadAttentionLayer(8, 16)\n",
    "l(tf.random.uniform((B, T, 8*16)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
