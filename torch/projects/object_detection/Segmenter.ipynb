{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Sequence Of Boxes](https://docs.google.com/document/d/14vPryt1JFE6Si89VamwMOi-RASeXmMXpo0_BjT_hn-U/edit?usp=sharing)\n",
    "\n",
    "Features:\n",
    "- Incorporates LR decay from the [Segmenter](https://arxiv.org/abs/2105.05633) paper.\n",
    "- Attention and MLP do not use Bias.\n",
    "- Uses mIoU.\n",
    "- Input Augmentations\n",
    "  - Mean Substraction\n",
    "  - Random Horizontal Flip. Both images and masks should be identically augmented.\n",
    "  - Random Resize.\n",
    "  - Rotation and Scaling Augmentations.\n",
    "- Reloads model checkpoints for continued training.\n",
    "- Uses Stochastic Depth for regularization. Reference: https://keras.io/examples/vision/cct/\n",
    "- Ignores background pixels.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Instructions\n",
    "Typically, we develop notebooks on a Apple M1 local machine. When importing this to platforms such as Colab or Kaggle, following adaptations are required:\n",
    "- Enable package installations in the [Import Modules](#import-modules) section.\n",
    "- [Initialize WANDB](#initialize-wandb).\n",
    "- Adjust [dataset splits](#download).\n",
    "\n",
    "# Instructions to Reload Last Run's Weights\n",
    "- [] Adjust EPOCHS and EPOCHS_DONE\n",
    "- [] Ensure that the weights are loaded from the previous run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules\n",
    "\n",
    "Note: This section requires changes to adapt to the target environments. Please refer to the [instructions](#running-instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q git+https://github.com/EfficientDL/codelab_utils.git\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "import codelab_utils.mpl_styles as mpl_styles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from kaggle_secrets import UserSecretsClient # Kaggle\n",
    "from matplotlib import patches as patches\n",
    "from itertools import accumulate\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "mpl_styles.set_default_styles()\n",
    "plt.rcParams['font.family'] = 'Poppins'\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'PT.Segmenter'\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize WANDB\n",
    "\n",
    "Note: This section requires changes to adapt to the target environments. Please refer to the [instructions](#running-instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "# os.environ['WANDB_API_KEY'] = wandb_api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "* Download scene_parse150, an semantic segmentation dataset.\n",
    "* Dataset class labels are available at [CSAILVision](https://github.com/CSAILVision/sceneparsing/blob/master/objectInfo150.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 2\n",
    "BATCH_SIZE = 8 # Kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procurement\n",
    "\n",
    "Note: This section requires changes to adapt to the target environments. Please refer to the [instructions](#running-instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_NAME = \"scene_parse_150\"\n",
    "DS_SPLITS = dict(\n",
    "    train='train[:5]',\n",
    "    validation='validation[:5]',\n",
    "    test='test[:5]',\n",
    ")\n",
    "# DS_SPLITS = dict(\n",
    "#     train='train',\n",
    "#     validation='validation',\n",
    "#     test='test',\n",
    "# )# Kaggle\n",
    "\n",
    "def load_split(ds_name, split, with_annotation=True):\n",
    "    ds = load_dataset(ds_name, split=split).with_format(\"torch\", device=device)\n",
    "#     ds = load_dataset(\n",
    "#         ds_name,\n",
    "#         split=split,\n",
    "#         cache_dir=\"/kaggle/input/scene-parse-150-hf/huggingface\"\n",
    "#     ) # Kaggle\n",
    "    print(f\"Split: {split} Items: {len(ds)} Features: {ds.features.keys()}\")\n",
    "    \n",
    "    return ds, len(ds)\n",
    "\n",
    "ds_builder = load_dataset_builder(DS_NAME)\n",
    "splits = ds_builder.info.splits\n",
    "split_infos = list(map(lambda k: (k, splits[k].num_examples), splits.keys()))\n",
    "\n",
    "print(f\"Available Splits: {split_infos}\")\n",
    "\n",
    "train_ds, train_count = load_split(DS_NAME, DS_SPLITS['train'])\n",
    "val_ds, val_count = load_split(DS_NAME, DS_SPLITS['validation'])\n",
    "test_ds, test_count = load_split(DS_NAME, DS_SPLITS['test'], with_annotation=False)\n",
    "\n",
    "def ds_shape(ds, size, with_annotation=True, name='Training'):\n",
    "    print(f'\\n{name} Set')\n",
    "    print('------------------')\n",
    "    print(f'Size: {size}')\n",
    "    \n",
    "    if with_annotation:\n",
    "        image, mask = next(iter(ds))\n",
    "        print(f'Image Shape: {image.shape} Mask Shape: {mask.shape}')\n",
    "    else:\n",
    "        image = next(iter(ds))\n",
    "        print(f'Image Shape: {image.shape}')\n",
    "\n",
    "ds_shape(train_ds, train_count)\n",
    "ds_shape(val_ds, val_count)\n",
    "ds_shape(test_ds, test_count, with_annotation=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_item(ax, item):\n",
    "    ax.imshow(item)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "def show_related_images(*relatives, batch_to_rows=True, title='', size=1.5):\n",
    "    num_relatives, relative_dims = len(relatives), len(relatives[0].shape)\n",
    "\n",
    "    if relative_dims == 3:\n",
    "        relatives = list(map(lambda x: tf.expand_dims(x, axis=0), relatives))\n",
    "    \n",
    "    batch_size = relatives[0].shape[0]\n",
    "    items = tf.range(batch_size*num_relatives)\n",
    "\n",
    "    if batch_to_rows:\n",
    "        fig, axes = plt.subplots(batch_size, num_relatives, figsize=(num_relatives*size, batch_size*size))\n",
    "        rows, cols = tf.unravel_index(indices=items, dims=[batch_size, num_relatives])\n",
    "\n",
    "        fig.supylabel('Batch')\n",
    "        fig.supxlabel('Relatives')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(num_relatives, batch_size, figsize=(batch_size*size, num_relatives*size))\n",
    "        rows, cols = tf.unravel_index(indices=items, dims=[num_relatives, batch_size])\n",
    "\n",
    "        fig.supxlabel('Batch')\n",
    "        fig.supylabel('Relatives')\n",
    "\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for item_id in range(batch_size*num_relatives):\n",
    "        row, col = rows[item_id], cols[item_id]\n",
    "        ax = axes[item_id]\n",
    "\n",
    "        item = relatives[col][row] if batch_to_rows else relatives[row][col]\n",
    "        visualize_item(ax, item)\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "\n",
    "batch_size, img_size, classes = 3, 8, 3\n",
    "image = tf.random.normal((batch_size, img_size, img_size, 3))\n",
    "mask = tf.random.uniform((batch_size, img_size, img_size, 1), maxval=classes, dtype=tf.int32)\n",
    "\n",
    "show_related_images(image, mask, batch_to_rows=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Inputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
