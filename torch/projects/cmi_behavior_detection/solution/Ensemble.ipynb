{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a08c95a",
   "metadata": {
    "_cell_guid": "d5310163-4789-4430-a3cf-ed3f87501eb7",
    "_uuid": "eec5ac82-bfab-42a7-92fd-264272917ffd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005197,
     "end_time": "2025-08-21T10:02:55.997145",
     "exception": false,
     "start_time": "2025-08-21T10:02:55.991948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is an ensemble of hard and soft label models. The Kaggle notebook is available [here](https://www.kaggle.com/code/naresh/cmi-behavior-detection-84-submission?scriptVersionId=257282787)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01ca0b",
   "metadata": {
    "_cell_guid": "5ee0e7d0-4080-425a-ae4b-eee10ad1136c",
    "_uuid": "46d1c160-11d2-4ae2-99e9-24b406524bac",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003344,
     "end_time": "2025-08-21T10:02:56.005013",
     "exception": false,
     "start_time": "2025-08-21T10:02:56.001669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753224d5",
   "metadata": {
    "_cell_guid": "815bf408-b9fb-46a2-a7e4-e477375d1bb4",
    "_uuid": "f5174bf8-ea06-498a-8834-0b0a3ee2f6fc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00285,
     "end_time": "2025-08-21T10:02:56.011293",
     "exception": false,
     "start_time": "2025-08-21T10:02:56.008443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e4f59a",
   "metadata": {
    "_cell_guid": "b927c445-6938-43bf-b81a-3e87a7d0b5d7",
    "_uuid": "f80ff1c0-41dd-4097-bbae-93038f3c92cb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T10:02:56.021461Z",
     "iopub.status.busy": "2025-08-21T10:02:56.020619Z",
     "iopub.status.idle": "2025-08-21T10:03:01.159132Z",
     "shell.execute_reply": "2025-08-21T10:03:01.158432Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.146186,
     "end_time": "2025-08-21T10:03:01.160525",
     "exception": false,
     "start_time": "2025-08-21T10:02:56.014339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Callable\n",
    "from core_utilities import set_seed\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 42\n",
    "    eps: float = 1e-8\n",
    "    \n",
    "    root: Path = Path('/kaggle/input/cmi-detect-behavior-with-sensor-data/')\n",
    "    ckpt_root: Path = Path('/kaggle/input/cmi-behavior-detection/lightning_logs/version_0/checkpoints')\n",
    "    data_root: Path = Path('/kaggle/input/cmi-behavior-detection-synthetic-dataset')\n",
    "        \n",
    "    # Data groups (train, test, ...)\n",
    "    data_groups: dict[str, str] = field(default_factory=lambda: dict(\n",
    "        train='train.csv',\n",
    "        test='test.csv',\n",
    "    ))\n",
    "    \n",
    "    # Demographics data\n",
    "    demographics: dict[str, str] = field(default_factory=lambda: dict(\n",
    "        train='train_demographics.csv',\n",
    "        test='test_demographics.csv',\n",
    "    ))\n",
    "\n",
    "    # Split ratio\n",
    "    split_ratio: float = 0.2\n",
    "\n",
    "    # Torch Configuration\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Feature Flags\n",
    "    features: List[str] = field(default_factory=lambda: [\n",
    "        ## Data Processing\n",
    "        # 'data.linear_acc',\n",
    "        # 'data.linear_acc.overwrite',\n",
    "        \n",
    "        ## Model\n",
    "        'model.summary',\n",
    "\n",
    "        ## Training\n",
    "        # 'train',\n",
    "        # 'train.metrics',\n",
    "        # 'checkpoint',\n",
    "\n",
    "        ## Competition Metrics\n",
    "        # 'competition.metrics.valid',\n",
    "        'competition.metrics.test',\n",
    "\n",
    "    ])\n",
    "\n",
    "    def data_limit(self) -> int:\n",
    "        return 3 if is_interactive() else None\n",
    "\n",
    "config = Config()\n",
    "# print(f'{config=}')\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991e0ed9",
   "metadata": {
    "_cell_guid": "60e14215-01f5-4456-8d4f-c6339502e8cd",
    "_uuid": "1c6bcfe8-49ee-40e1-a026-af6ed6fd94bf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:01.168907Z",
     "iopub.status.busy": "2025-08-21T10:03:01.168550Z",
     "iopub.status.idle": "2025-08-21T10:03:01.176901Z",
     "shell.execute_reply": "2025-08-21T10:03:01.176234Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013706,
     "end_time": "2025-08-21T10:03:01.177906",
     "exception": false,
     "start_time": "2025-08-21T10:03:01.164200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_config=DataConfig(slice_len=64, sequence_len=64, num_classes=18, column_to_str={'metadata': ['row_id', 'sequence_id', 'sequence_counter', 'subject'], 'metadata_d': ['subject'], 'acc': ['acc_x', 'acc_y', 'acc_z'], 'rot': ['rot_w', 'rot_x', 'rot_y', 'rot_z'], 'thm': ['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5'], 'tof': ['tof_1_v0', 'tof_1_v1', 'tof_1_v2', 'tof_1_v3', 'tof_1_v4', 'tof_1_v5', 'tof_1_v6', 'tof_1_v7', 'tof_1_v8', 'tof_1_v9', 'tof_1_v10', 'tof_1_v11', 'tof_1_v12', 'tof_1_v13', 'tof_1_v14', 'tof_1_v15', 'tof_1_v16', 'tof_1_v17', 'tof_1_v18', 'tof_1_v19', 'tof_1_v20', 'tof_1_v21', 'tof_1_v22', 'tof_1_v23', 'tof_1_v24', 'tof_1_v25', 'tof_1_v26', 'tof_1_v27', 'tof_1_v28', 'tof_1_v29', 'tof_1_v30', 'tof_1_v31', 'tof_1_v32', 'tof_1_v33', 'tof_1_v34', 'tof_1_v35', 'tof_1_v36', 'tof_1_v37', 'tof_1_v38', 'tof_1_v39', 'tof_1_v40', 'tof_1_v41', 'tof_1_v42', 'tof_1_v43', 'tof_1_v44', 'tof_1_v45', 'tof_1_v46', 'tof_1_v47', 'tof_1_v48', 'tof_1_v49', 'tof_1_v50', 'tof_1_v51', 'tof_1_v52', 'tof_1_v53', 'tof_1_v54', 'tof_1_v55', 'tof_1_v56', 'tof_1_v57', 'tof_1_v58', 'tof_1_v59', 'tof_1_v60', 'tof_1_v61', 'tof_1_v62', 'tof_1_v63', 'tof_2_v0', 'tof_2_v1', 'tof_2_v2', 'tof_2_v3', 'tof_2_v4', 'tof_2_v5', 'tof_2_v6', 'tof_2_v7', 'tof_2_v8', 'tof_2_v9', 'tof_2_v10', 'tof_2_v11', 'tof_2_v12', 'tof_2_v13', 'tof_2_v14', 'tof_2_v15', 'tof_2_v16', 'tof_2_v17', 'tof_2_v18', 'tof_2_v19', 'tof_2_v20', 'tof_2_v21', 'tof_2_v22', 'tof_2_v23', 'tof_2_v24', 'tof_2_v25', 'tof_2_v26', 'tof_2_v27', 'tof_2_v28', 'tof_2_v29', 'tof_2_v30', 'tof_2_v31', 'tof_2_v32', 'tof_2_v33', 'tof_2_v34', 'tof_2_v35', 'tof_2_v36', 'tof_2_v37', 'tof_2_v38', 'tof_2_v39', 'tof_2_v40', 'tof_2_v41', 'tof_2_v42', 'tof_2_v43', 'tof_2_v44', 'tof_2_v45', 'tof_2_v46', 'tof_2_v47', 'tof_2_v48', 'tof_2_v49', 'tof_2_v50', 'tof_2_v51', 'tof_2_v52', 'tof_2_v53', 'tof_2_v54', 'tof_2_v55', 'tof_2_v56', 'tof_2_v57', 'tof_2_v58', 'tof_2_v59', 'tof_2_v60', 'tof_2_v61', 'tof_2_v62', 'tof_2_v63', 'tof_3_v0', 'tof_3_v1', 'tof_3_v2', 'tof_3_v3', 'tof_3_v4', 'tof_3_v5', 'tof_3_v6', 'tof_3_v7', 'tof_3_v8', 'tof_3_v9', 'tof_3_v10', 'tof_3_v11', 'tof_3_v12', 'tof_3_v13', 'tof_3_v14', 'tof_3_v15', 'tof_3_v16', 'tof_3_v17', 'tof_3_v18', 'tof_3_v19', 'tof_3_v20', 'tof_3_v21', 'tof_3_v22', 'tof_3_v23', 'tof_3_v24', 'tof_3_v25', 'tof_3_v26', 'tof_3_v27', 'tof_3_v28', 'tof_3_v29', 'tof_3_v30', 'tof_3_v31', 'tof_3_v32', 'tof_3_v33', 'tof_3_v34', 'tof_3_v35', 'tof_3_v36', 'tof_3_v37', 'tof_3_v38', 'tof_3_v39', 'tof_3_v40', 'tof_3_v41', 'tof_3_v42', 'tof_3_v43', 'tof_3_v44', 'tof_3_v45', 'tof_3_v46', 'tof_3_v47', 'tof_3_v48', 'tof_3_v49', 'tof_3_v50', 'tof_3_v51', 'tof_3_v52', 'tof_3_v53', 'tof_3_v54', 'tof_3_v55', 'tof_3_v56', 'tof_3_v57', 'tof_3_v58', 'tof_3_v59', 'tof_3_v60', 'tof_3_v61', 'tof_3_v62', 'tof_3_v63', 'tof_4_v0', 'tof_4_v1', 'tof_4_v2', 'tof_4_v3', 'tof_4_v4', 'tof_4_v5', 'tof_4_v6', 'tof_4_v7', 'tof_4_v8', 'tof_4_v9', 'tof_4_v10', 'tof_4_v11', 'tof_4_v12', 'tof_4_v13', 'tof_4_v14', 'tof_4_v15', 'tof_4_v16', 'tof_4_v17', 'tof_4_v18', 'tof_4_v19', 'tof_4_v20', 'tof_4_v21', 'tof_4_v22', 'tof_4_v23', 'tof_4_v24', 'tof_4_v25', 'tof_4_v26', 'tof_4_v27', 'tof_4_v28', 'tof_4_v29', 'tof_4_v30', 'tof_4_v31', 'tof_4_v32', 'tof_4_v33', 'tof_4_v34', 'tof_4_v35', 'tof_4_v36', 'tof_4_v37', 'tof_4_v38', 'tof_4_v39', 'tof_4_v40', 'tof_4_v41', 'tof_4_v42', 'tof_4_v43', 'tof_4_v44', 'tof_4_v45', 'tof_4_v46', 'tof_4_v47', 'tof_4_v48', 'tof_4_v49', 'tof_4_v50', 'tof_4_v51', 'tof_4_v52', 'tof_4_v53', 'tof_4_v54', 'tof_4_v55', 'tof_4_v56', 'tof_4_v57', 'tof_4_v58', 'tof_4_v59', 'tof_4_v60', 'tof_4_v61', 'tof_4_v62', 'tof_4_v63'], 'd': ['adult_child', 'handedness'], 'label': ['gesture'], 'linear_acc': ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']})\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    slice_len: int = 64\n",
    "    sequence_len: int = 103\n",
    "\n",
    "    # Classes\n",
    "    num_classes: int = 18\n",
    "\n",
    "    column_to_str: {str, List[str]} = field(default_factory=lambda: {\n",
    "        ## Metadata columns\n",
    "        'metadata': ['row_id', 'sequence_id', 'sequence_counter', 'subject'],\n",
    "        'metadata_d': ['subject'],\n",
    "\n",
    "        ## Data Columns\n",
    "        'acc': ['acc_x', 'acc_y', 'acc_z'],\n",
    "        'rot': ['rot_w', 'rot_x', 'rot_y', 'rot_z'],\n",
    "        'thm': [f'thm_{v}' for v in range(1, 6)],\n",
    "        'tof': (\n",
    "            [f'tof_1_v{v}' for v in range(64)]\n",
    "            + [f'tof_2_v{v}' for v in range(64)]\n",
    "            + [f'tof_3_v{v}' for v in range(64)]\n",
    "            + [f'tof_4_v{v}' for v in range(64)]\n",
    "        ),\n",
    "        ## Demographics Columns\n",
    "        'd': ['adult_child', 'handedness'],\n",
    "\n",
    "        ## Label Columns\n",
    "        'label': ['gesture'],\n",
    "\n",
    "        ## Generated Columns\n",
    "        'linear_acc': list(map(lambda s: f'linear_{s}', ['acc_x', 'acc_y', 'acc_z']))\n",
    "    })\n",
    "\n",
    "    def columns(self, hint=['acc', 'rot', 'tof', 'd']):\n",
    "        # Convert hint to a list for processing\n",
    "        if type(hint) is str: hint = [hint]\n",
    "\n",
    "        return sum(map(lambda h: self.column_to_str[h], hint), [])\n",
    "\n",
    "    def sequence_columns(self, generated=[]):\n",
    "        return self.columns(['acc', 'rot', 'thm', 'tof', 'd'] + generated)\n",
    "        \n",
    "data_config = DataConfig(slice_len=64, sequence_len=64)\n",
    "\n",
    "print(f'{data_config=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a57cb7",
   "metadata": {
    "_cell_guid": "5d485242-37ac-4927-985e-2324f1d95a76",
    "_uuid": "09024734-6408-4372-81e9-898440662e42",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00292,
     "end_time": "2025-08-21T10:03:01.184053",
     "exception": false,
     "start_time": "2025-08-21T10:03:01.181133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b57e4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:01.191044Z",
     "iopub.status.busy": "2025-08-21T10:03:01.190813Z",
     "iopub.status.idle": "2025-08-21T10:03:01.197694Z",
     "shell.execute_reply": "2025-08-21T10:03:01.196813Z"
    },
    "papermill": {
     "duration": 0.011672,
     "end_time": "2025-08-21T10:03:01.198790",
     "exception": false,
     "start_time": "2025-08-21T10:03:01.187118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config=ModelConfig(model='BDLstmModel', input_size=64, out_channels=18, in_splits=[3, 4, 5, 256, 2], in_encoders=['acc', 'rot', 'thm', 'tof', 'dem'], slice_encoders={}, rnn_channels=64, squeeze_channels=32, with_noise=False)\n",
      "model_config.input_shape()=(64, 270)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from collections.abc import Callable\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model: str = 'BDLstmModel'\n",
    "\n",
    "    # Required configuration\n",
    "    input_size: int = 64\n",
    "    out_channels: int = 18\n",
    "\n",
    "    # Input split configuration\n",
    "    in_splits: list[int] = field(default_factory=lambda: [3, 4, 5, 256, 2])\n",
    "    in_encoders: list[str] = field(default_factory=lambda: ['acc', 'rot', 'thm', 'tof', 'dem'])\n",
    "\n",
    "    # Input slice configuration\n",
    "    slice_encoders: dict[str, tuple[int]] = field(default_factory=lambda: {\n",
    "        # 'acc_rot': [slice(7)],\n",
    "    })\n",
    "\n",
    "    # Optional configuration\n",
    "    rnn_channels: int = 64\n",
    "    squeeze_channels: int = 128\n",
    "    \n",
    "    # Optional layers\n",
    "    with_noise: bool = False\n",
    "    \n",
    "    def input_shape(self) -> tuple[int, int]:\n",
    "        return (self.input_size, self.in_channels)\n",
    "\n",
    "    @property\n",
    "    def in_channels(self) -> int:\n",
    "        return sum(self.in_splits)\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    input_size=data_config.sequence_len,\n",
    "    rnn_channels=data_config.sequence_len,\n",
    "    squeeze_channels=32,\n",
    ")\n",
    "print(f'{model_config=}')\n",
    "print(f'{model_config.input_shape()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f883b857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:01.206141Z",
     "iopub.status.busy": "2025-08-21T10:03:01.205922Z",
     "iopub.status.idle": "2025-08-21T10:03:07.386334Z",
     "shell.execute_reply": "2025-08-21T10:03:07.385308Z"
    },
    "papermill": {
     "duration": 6.185708,
     "end_time": "2025-08-21T10:03:07.387710",
     "exception": false,
     "start_time": "2025-08-21T10:03:01.202002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
      "============================================================================================================================================\n",
      "BDLstmModel (BDLstmModel)                                    [16, 270, 64]        [16, 18]             --                   True\n",
      "├─ModuleDict (encoders)                                      --                   --                   --                   True\n",
      "│    └─Sequential (acc)                                      [16, 3, 64]          [16, 1024, 64]       --                   True\n",
      "│    │    └─BDConvBlock (0)                                  [16, 3, 64]          [16, 512, 64]        38,912               True\n",
      "│    │    └─BDConvBlock (1)                                  [16, 512, 64]        [16, 768, 64]        2,017,536            True\n",
      "│    │    └─BDConvBlock (2)                                  [16, 768, 64]        [16, 1024, 64]       2,427,904            True\n",
      "│    └─Sequential (rot)                                      [16, 4, 64]          [16, 1024, 64]       --                   True\n",
      "│    │    └─BDConvBlock (0)                                  [16, 4, 64]          [16, 512, 64]        40,448               True\n",
      "│    │    └─BDConvBlock (1)                                  [16, 512, 64]        [16, 768, 64]        2,017,536            True\n",
      "│    │    └─BDConvBlock (2)                                  [16, 768, 64]        [16, 1024, 64]       2,427,904            True\n",
      "│    └─Sequential (thm)                                      [16, 5, 64]          [16, 128, 64]        --                   True\n",
      "│    │    └─ConvBlock1d (0)                                  [16, 5, 64]          [16, 64, 64]         1,088                True\n",
      "│    │    └─Dropout (1)                                      [16, 64, 64]         [16, 64, 64]         --                   --\n",
      "│    │    └─ConvBlock1d (2)                                  [16, 64, 64]         [16, 128, 64]        24,832               True\n",
      "│    │    └─Dropout (3)                                      [16, 128, 64]        [16, 128, 64]        --                   --\n",
      "│    └─Sequential (tof)                                      [16, 256, 64]        [16, 128, 64]        --                   True\n",
      "│    │    └─ConvBlock1d (0)                                  [16, 256, 64]        [16, 64, 64]         49,280               True\n",
      "│    │    └─Dropout (1)                                      [16, 64, 64]         [16, 64, 64]         --                   --\n",
      "│    │    └─ConvBlock1d (2)                                  [16, 64, 64]         [16, 128, 64]        24,832               True\n",
      "│    │    └─Dropout (3)                                      [16, 128, 64]        [16, 128, 64]        --                   --\n",
      "│    └─Sequential (dem)                                      [16, 2, 64]          [16, 128, 64]        --                   True\n",
      "│    │    └─ConvBlock1d (0)                                  [16, 2, 64]          [16, 64, 64]         512                  True\n",
      "│    │    └─Dropout (1)                                      [16, 64, 64]         [16, 64, 64]         --                   --\n",
      "│    │    └─ConvBlock1d (2)                                  [16, 64, 64]         [16, 128, 64]        24,832               True\n",
      "│    │    └─Dropout (3)                                      [16, 128, 64]        [16, 128, 64]        --                   --\n",
      "├─Sequential (conv_m)                                        [16, 270, 64]        [16, 1024, 64]       --                   True\n",
      "│    └─BDConvBlock (0)                                       [16, 270, 64]        [16, 512, 64]        --                   True\n",
      "│    │    └─Sequential (block)                               [16, 270, 64]        [16, 512, 64]        416,256              True\n",
      "│    │    └─ChannelSeBlock (se_block)                        [16, 512, 64]        [16, 512, 64]        32,768               True\n",
      "│    └─BDConvBlock (1)                                       [16, 512, 64]        [16, 768, 64]        --                   True\n",
      "│    │    └─Sequential (block)                               [16, 512, 64]        [16, 768, 64]        1,968,384            True\n",
      "│    │    └─ChannelSeBlock (se_block)                        [16, 768, 64]        [16, 768, 64]        49,152               True\n",
      "│    └─BDConvBlock (2)                                       [16, 768, 64]        [16, 1024, 64]       --                   True\n",
      "│    │    └─Sequential (block)                               [16, 768, 64]        [16, 1024, 64]       2,362,368            True\n",
      "│    │    └─ChannelSeBlock (se_block)                        [16, 1024, 64]       [16, 1024, 64]       65,536               True\n",
      "├─ModuleList (branches)                                      --                   --                   --                   True\n",
      "│    └─LSTM (0)                                              [16, 64, 3456]       [16, 64, 128]        1,803,264            True\n",
      "│    └─GRU (1)                                               [16, 64, 3456]       [16, 64, 128]        1,352,448            True\n",
      "│    └─RNN (2)                                               [16, 64, 3456]       [16, 64, 128]        450,816              True\n",
      "├─Dropout (dropout_m)                                        [16, 64, 384]        [16, 64, 384]        --                   --\n",
      "├─Sequential (linear_backbone)                               [16, 64, 384]        [16, 64, 128]        --                   True\n",
      "│    └─BDLinearBlock (block_0)                               [16, 64, 384]        [16, 64, 128]        --                   True\n",
      "│    │    └─Sequential (block)                               [16, 64, 384]        [16, 64, 128]        49,152               True\n",
      "├─Linear (linear)                                            [16, 128]            [16, 18]             2,322                True\n",
      "============================================================================================================================================\n",
      "Total params: 17,648,082\n",
      "Trainable params: 17,648,082\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 17.56\n",
      "============================================================================================================================================\n",
      "Input size (MB): 1.11\n",
      "Forward/backward pass size (MB): 127.80\n",
      "Params size (MB): 70.59\n",
      "Estimated Total Size (MB): 199.50\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms.v2 import GaussianNoise\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchinfo import summary\n",
    "from torch_layers import BDConvBlock, BDLinearBlock, ChannelSeBlock, ConvNormActivation, ConvBlock1d\n",
    "from torch_layers import Lambda\n",
    "\n",
    "class BDLstmModel(nn.Module):\n",
    "    name = 'lstm-model'\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "\n",
    "        in_splits,\n",
    "        in_encoders,\n",
    "        slice_encoders,\n",
    "        \n",
    "        conv_configs=[\n",
    "            # (256, 3, .1), (512, 5, .1),\n",
    "            # (768, 3, .1),\n",
    "            \n",
    "            (512, 3, .1), (768, 5, .1),\n",
    "            (1024, 3, .1), #(1536, 3, .4),\n",
    "            #(2048, 3, .5),\n",
    "        ],\n",
    "        linear_configs=[\n",
    "            # (2048, 2048, .5), (2048, 1024, .4),\n",
    "            #(1024, 512, .3),\n",
    "            # (512, 128, .3),\n",
    "            (512, 128, .3),\n",
    "        ],\n",
    "        squeeze_channels=128,\n",
    "        rnn_channels=128,\n",
    "        with_noise=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input args\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.in_splits = in_splits\n",
    "        self.in_encoders = in_encoders\n",
    "        self.slice_encoders = slice_encoders\n",
    "        \n",
    "        self.conv_configs = conv_configs\n",
    "        self.linear_configs = linear_configs.copy()\n",
    "        self.squeeze_channels = squeeze_channels\n",
    "        self.with_noise = with_noise\n",
    "\n",
    "        # Derived args\n",
    "        conv_out_channels = conv_configs[-1][0] # Output channels of the last conv block\n",
    "        encoders_out_channels = conv_out_channels*3 + 128*3\n",
    "        rnn_args = (encoders_out_channels, rnn_channels)\n",
    "        rnn_kwargs = dict(bidirectional=True, batch_first=True)\n",
    "\n",
    "        # Adjusted args\n",
    "        _, last_out_channels, last_dropout = linear_configs[-1]\n",
    "        self.linear_configs[-1] = (rnn_channels*6, last_out_channels, last_dropout)\n",
    "\n",
    "        # Additional args\n",
    "        conv_configs_64_128 = [\n",
    "            (64, 3, .2),\n",
    "            (128, 3, .2),\n",
    "        ]\n",
    "        conv_configs_128_256 = [\n",
    "            (128, 3, .2),\n",
    "            (256, 3, .2),\n",
    "        ]\n",
    "        conv_configs_256_512 = [\n",
    "            (256, 3, .2),\n",
    "            (512, 3, .2),\n",
    "        ]\n",
    "        conv_configs_64_128_256 = [\n",
    "            (64, 3, .2),\n",
    "            (128, 3, .2),\n",
    "            (256, 3, .2),\n",
    "        ]\n",
    "        conv_configs_64_128_256_512 = [\n",
    "            (64, 3, .2),\n",
    "            (128, 3, .2),\n",
    "            (256, 3, .2),\n",
    "            (512, 3, .2),\n",
    "        ]\n",
    "        reflex_fn = lambda x: x\n",
    "\n",
    "        # Layers\n",
    "        self.encoders = nn.ModuleDict(OrderedDict(filter(\n",
    "            lambda item: item[1] is not None,\n",
    "            [\n",
    "                (\n",
    "                    'acc',\n",
    "                    self.make_bdconv_module(3, conv_configs, padding='same')\n",
    "                ),\n",
    "                (\n",
    "                    'rot',\n",
    "                    self.make_bdconv_module(4, conv_configs, padding='same')\n",
    "                    # self.make_small_conv_module(4, conv_configs_128_256, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'thm',\n",
    "                    self.make_small_conv_module(5, conv_configs_64_128, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'tof',\n",
    "                    self.make_small_conv_module(256, conv_configs_64_128, padding='same', bias=False)\n",
    "                    # self.make_small_conv_module(256, conv_configs_256_512, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'dem',\n",
    "                    self.make_small_conv_module(2, conv_configs_64_128, padding='same', bias=False)\n",
    "                ),\n",
    "            ]\n",
    "        )))\n",
    "        self.conv_m = self.make_bdconv_module(in_channels, conv_configs, padding='same')\n",
    "\n",
    "        # Noise branch\n",
    "        if with_noise: noise_m = nn.Sequential(\n",
    "            GaussianNoise(),\n",
    "            nn.Linear(self.encoders_out_channels, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # All branches\n",
    "        self.branches = nn.ModuleList(filter(\n",
    "            lambda fn: fn is not None,\n",
    "            [\n",
    "                nn.LSTM(*rnn_args, **rnn_kwargs),\n",
    "                nn.GRU(*rnn_args, **rnn_kwargs),\n",
    "                nn.RNN(*rnn_args, nonlinearity='relu', **rnn_kwargs),\n",
    "                noise_m if self.with_noise else None,\n",
    "            ]\n",
    "        ))\n",
    "        self.branch_fns = list(filter(\n",
    "            lambda fn: fn is not None,\n",
    "            [\n",
    "                ([], []),\n",
    "                # ([], []),\n",
    "                ([torch.fliplr], [torch.fliplr]),\n",
    "                ([], []),\n",
    "                ([], []) if self.with_noise else None,\n",
    "            ]\n",
    "            \n",
    "        ))\n",
    "\n",
    "        self.dropout_m = nn.Dropout(.2)\n",
    "        \n",
    "        self.linear_backbone = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                map(\n",
    "                    self.make_linear_block,\n",
    "                    enumerate(self.linear_configs)\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.linear_configs[-1][1], self.out_channels)\n",
    "\n",
    "    def apply_one_branch(self, branch_m, x, branch_fns=([], [])):\n",
    "        pre_fns, post_fns = branch_fns\n",
    "        # print(f'{type(branch_m)} {type(prep_fns)} {x.shape=}')\n",
    "\n",
    "        # Apply branch input prep functions\n",
    "        x = self.apply_fns(x, pre_fns)\n",
    "        \n",
    "        # Apply RNN branch. RNN branches require channel-last input\n",
    "        # print(f'{x.shape=} {branch_m=}')\n",
    "        x, _ = branch_m(x)\n",
    "\n",
    "        # Apply branch post-process functions\n",
    "        x = self.apply_fns(x, post_fns)\n",
    "\n",
    "        # print(f'{x.shape=}')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_fns(self, x, fns):\n",
    "        for fn in fns: x = fn(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def apply_branches(self, x):\n",
    "        # print(f'apply_branches({x.shape=})')\n",
    "        \n",
    "        # Apply branches\n",
    "        xs = list(map(\n",
    "            lambda args: self.apply_one_branch(*args),\n",
    "            zip(self.branches, [x]*len(self.branches), self.branch_fns)\n",
    "        ))\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def apply_encoders(self, x):\n",
    "        # 1. Split inputs\n",
    "        x_splits = x.split(self.in_splits, dim=1)\n",
    "        # print(f'{len(x_splits)}')\n",
    "\n",
    "        # 2. Group splits by encoder\n",
    "        x_grouped = defaultdict(list)\n",
    "        for x_split, e_name in zip(x_splits, self.in_encoders):\n",
    "            # print(f'{e_name} {x_split.shape}')\n",
    "            x_grouped[e_name].append(x_split)\n",
    "        \n",
    "        # 3. Merge groups\n",
    "        xs = list(map(\n",
    "            lambda e_name: torch.concat(x_grouped[e_name], dim=1) if len(x_grouped[e_name]) > 1 else x_grouped[e_name][0],\n",
    "            self.in_encoders,\n",
    "        ))\n",
    "\n",
    "        # for x in xs:\n",
    "        #     print(f'{x.shape=}')\n",
    "\n",
    "        # 4. Apply encoders\n",
    "        xs = list(map(\n",
    "            lambda item: self.encoders[item[0]](item[1]), \n",
    "            zip(x_grouped.keys(), xs),\n",
    "        ))\n",
    "\n",
    "        return torch.concat(xs, dim=1)\n",
    "\n",
    "    def apply_slice_encoders(self, x):\n",
    "        # 1. Get slices\n",
    "        x_slices = []\n",
    "        for slices in self.slice_encoders.values():\n",
    "            x_slice = torch.concat(\n",
    "                list(map(lambda s: x[:, s, :], slices)),\n",
    "                dim=1\n",
    "            )\n",
    "            x_slices.append(x_slice)\n",
    "            # print(f'{x_slice.shape=}')\n",
    "            \n",
    "        # print(f'{len(x_slices)}')\n",
    "\n",
    "        # 4. Apply encoders\n",
    "        xs = list(map(\n",
    "            lambda item: self.encoders[item[0]](item[1]), \n",
    "            zip(self.slice_encoders.keys(), x_slices),\n",
    "        ))\n",
    "\n",
    "        return torch.concat(xs, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save input\n",
    "        inp = x\n",
    "        \n",
    "        # -> Apply encoders\n",
    "        x_enc = self.apply_encoders(x)\n",
    "        # print(f'{x.shape=} <- apply_encoders()')\n",
    "\n",
    "        # -> Apply slice_encoders\n",
    "        if self.slice_encoders:\n",
    "            x_slice_enc = self.apply_slice_encoders(x)\n",
    "            # print(f'{x_slice_enc.shape=} <- apply_slice_encoders()')\n",
    "\n",
    "            ## -> Merge encoders\n",
    "            x = torch.concat([x_enc, x_slice_enc], dim=1)\n",
    "        else:\n",
    "            x = x_enc\n",
    "\n",
    "        x = torch.concat([x, self.conv_m(inp)], dim=1)\n",
    "        \n",
    "        # -> Apply branches\n",
    "        xs = self.apply_branches(x.permute(0, 2, 1))\n",
    "        \n",
    "        # -> Join the branches on the channel dimension and apply attention\n",
    "        x = torch.concat(xs, dim=2)\n",
    "\n",
    "        # -> Apply Dropout\n",
    "        # print(f'{x.shape=}')\n",
    "        x = self.dropout_m(x)\n",
    "        \n",
    "        # -> Join the branches on the channel dimension and apply attention\n",
    "        x = F.scaled_dot_product_attention(x, x, x)\n",
    "        \n",
    "        # -> Apply linear backbone and pick the last element for classification\n",
    "        x = self.linear_backbone(x)\n",
    "        x = self.linear(x[..., -1, :])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def make_small_conv_module(self, in_channels, conv_configs, **kwargs):\n",
    "        def conv_fn(args):\n",
    "            # Extract parameters\n",
    "            conv_idx, (conv_out, kernel, dropout) = args\n",
    "            conv_in = in_channels if conv_idx == 0 else conv_configs[conv_idx - 1][0]\n",
    "\n",
    "            # Create layers\n",
    "            layers = [\n",
    "                ConvBlock1d(conv_in, conv_out, kernel, **kwargs)\n",
    "            ]\n",
    "            \n",
    "            if dropout is not None: layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            return layers\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            *sum(\n",
    "                map(\n",
    "                    conv_fn,\n",
    "                    enumerate(conv_configs)\n",
    "                ),\n",
    "                []\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def make_bdconv_module(self, in_channels, conv_configs, **kwargs):\n",
    "        def bdconv_fn(args):\n",
    "            # Extract parameters\n",
    "            conv_idx, config = args\n",
    "            conv_in = in_channels if conv_idx == 0 else conv_configs[conv_idx - 1][0]\n",
    "\n",
    "            return BDConvBlock(\n",
    "                f'bdconv_{conv_idx}', conv_in, *config,\n",
    "                squeeze_channels=self.squeeze_channels, **kwargs,\n",
    "            )\n",
    "            \n",
    "        return nn.Sequential(*map(bdconv_fn, enumerate(conv_configs)))\n",
    "\n",
    "    def make_linear_block(self, args):\n",
    "        idx, config = args\n",
    "        block_name = f'block_{idx}'\n",
    "        block = BDLinearBlock(block_name, *config, bias=False)\n",
    "\n",
    "        return block_name, block\n",
    "        \n",
    "model = BDLstmModel(\n",
    "    in_channels=model_config.in_channels,\n",
    "    out_channels=model_config.out_channels,\n",
    "    \n",
    "    in_splits=model_config.in_splits,\n",
    "    in_encoders=model_config.in_encoders,\n",
    "    slice_encoders=model_config.slice_encoders,\n",
    "    \n",
    "    squeeze_channels=model_config.squeeze_channels,\n",
    "    rnn_channels=model_config.rnn_channels,\n",
    "    \n",
    "    with_noise=model_config.with_noise,\n",
    ").to(config.device)\n",
    "\n",
    "if 'model.summary' in config.features:\n",
    "    print(\n",
    "        summary(\n",
    "            model=model, \n",
    "            input_size=(16, model_config.in_channels, model_config.input_size),\n",
    "            col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "            col_width=20,\n",
    "            row_settings=[\"var_names\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "230ebdf0",
   "metadata": {
    "_cell_guid": "e2fefd3c-5b3a-4926-bab4-621ea1ab91c1",
    "_uuid": "df60be5d-9e8e-4d3c-9976-80048f149c72",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:07.396780Z",
     "iopub.status.busy": "2025-08-21T10:03:07.396500Z",
     "iopub.status.idle": "2025-08-21T10:03:07.699655Z",
     "shell.execute_reply": "2025-08-21T10:03:07.698778Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.308289,
     "end_time": "2025-08-21T10:03:07.700631",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.392342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
      "============================================================================================================================================\n",
      "HardLabelModel (HardLabelModel)                              [16, 270, 64]        [16, 18]             --                   True\n",
      "├─ModuleDict (encoders)                                      --                   --                   --                   True\n",
      "│    └─Sequential (acc)                                      [16, 3, 64]          [16, 1024, 64]       --                   True\n",
      "│    │    └─BDConvBlock (0)                                  [16, 3, 64]          [16, 512, 64]        38,912               True\n",
      "│    │    └─BDConvBlock (1)                                  [16, 512, 64]        [16, 768, 64]        2,017,536            True\n",
      "│    │    └─BDConvBlock (2)                                  [16, 768, 64]        [16, 1024, 64]       2,427,904            True\n",
      "│    └─Sequential (rot)                                      [16, 4, 64]          [16, 1024, 64]       --                   True\n",
      "│    │    └─BDConvBlock (0)                                  [16, 4, 64]          [16, 512, 64]        40,448               True\n",
      "│    │    └─BDConvBlock (1)                                  [16, 512, 64]        [16, 768, 64]        2,017,536            True\n",
      "│    │    └─BDConvBlock (2)                                  [16, 768, 64]        [16, 1024, 64]       2,427,904            True\n",
      "│    └─Sequential (thm)                                      [16, 5, 64]          [16, 128, 64]        --                   True\n",
      "│    │    └─ConvBlock1d (0)                                  [16, 5, 64]          [16, 64, 64]         1,088                True\n",
      "│    │    └─Dropout (1)                                      [16, 64, 64]         [16, 64, 64]         --                   --\n",
      "│    │    └─ConvBlock1d (2)                                  [16, 64, 64]         [16, 128, 64]        24,832               True\n",
      "│    │    └─Dropout (3)                                      [16, 128, 64]        [16, 128, 64]        --                   --\n",
      "│    └─Sequential (tof)                                      [16, 256, 64]        [16, 128, 64]        --                   True\n",
      "│    │    └─ConvBlock1d (0)                                  [16, 256, 64]        [16, 64, 64]         49,280               True\n",
      "│    │    └─Dropout (1)                                      [16, 64, 64]         [16, 64, 64]         --                   --\n",
      "│    │    └─ConvBlock1d (2)                                  [16, 64, 64]         [16, 128, 64]        24,832               True\n",
      "│    │    └─Dropout (3)                                      [16, 128, 64]        [16, 128, 64]        --                   --\n",
      "│    └─Sequential (dem)                                      [16, 2, 64]          [16, 128, 64]        --                   True\n",
      "│    │    └─ConvBlock1d (0)                                  [16, 2, 64]          [16, 64, 64]         512                  True\n",
      "│    │    └─Dropout (1)                                      [16, 64, 64]         [16, 64, 64]         --                   --\n",
      "│    │    └─ConvBlock1d (2)                                  [16, 64, 64]         [16, 128, 64]        24,832               True\n",
      "│    │    └─Dropout (3)                                      [16, 128, 64]        [16, 128, 64]        --                   --\n",
      "├─LSTM (lstm_m)                                              [16, 64, 2432]       [16, 64, 128]        1,278,976            True\n",
      "├─GRU (gru_m)                                                [16, 64, 2432]       [16, 64, 128]        959,232              True\n",
      "├─Dropout (dropout_m)                                        [16, 64, 256]        [16, 64, 256]        --                   --\n",
      "├─Sequential (linear_backbone)                               [16, 64, 256]        [16, 64, 128]        --                   True\n",
      "│    └─BDLinearBlock (block_0)                               [16, 64, 256]        [16, 64, 128]        --                   True\n",
      "│    │    └─Sequential (block)                               [16, 64, 256]        [16, 64, 128]        32,768               True\n",
      "├─Linear (linear)                                            [16, 128]            [16, 18]             2,322                True\n",
      "============================================================================================================================================\n",
      "Total params: 11,368,914\n",
      "Trainable params: 11,368,914\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 11.30\n",
      "============================================================================================================================================\n",
      "Input size (MB): 1.11\n",
      "Forward/backward pass size (MB): 88.70\n",
      "Params size (MB): 45.48\n",
      "Estimated Total Size (MB): 135.28\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms.v2 import GaussianNoise\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchinfo import summary\n",
    "from torch_layers import BDConvBlock, BDLinearBlock, ChannelSeBlock, ConvNormActivation, ConvBlock1d\n",
    "from torch_layers import Lambda\n",
    "\n",
    "class HardLabelModel(nn.Module):\n",
    "    name = 'lstm-model'\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "\n",
    "        in_splits,\n",
    "        in_encoders,\n",
    "        slice_encoders,\n",
    "        \n",
    "        conv_configs=[\n",
    "            # (256, 3, .1), (512, 5, .1),\n",
    "            # (768, 3, .1),\n",
    "            \n",
    "            (512, 3, .1), (768, 5, .1),\n",
    "            (1024, 3, .1), #(1536, 3, .4),\n",
    "            #(2048, 3, .5),\n",
    "        ],\n",
    "        linear_configs=[\n",
    "            # (2048, 2048, .5), (2048, 1024, .4),\n",
    "            #(1024, 512, .3),\n",
    "            # (512, 128, .3),\n",
    "            (512, 128, .3),\n",
    "        ],\n",
    "        squeeze_channels=128,\n",
    "        rnn_channels=128,\n",
    "        with_noise=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input args\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.in_splits = in_splits\n",
    "        self.in_encoders = in_encoders\n",
    "        self.slice_encoders = slice_encoders\n",
    "        \n",
    "        self.conv_configs = conv_configs\n",
    "        self.linear_configs = linear_configs.copy()\n",
    "        self.squeeze_channels = squeeze_channels\n",
    "        self.with_noise = with_noise\n",
    "\n",
    "        # Derived args\n",
    "        self.conv_out_channels = conv_configs[-1][0] # Output channels of the last conv block\n",
    "        self.encoders_out_channels = self.conv_out_channels*2 + 128*3\n",
    "\n",
    "        # Adjusted args\n",
    "        _, last_out_channels, last_dropout = linear_configs[-1]\n",
    "        self.linear_configs[-1] = (rnn_channels*4, last_out_channels, last_dropout)\n",
    "\n",
    "        # Additional args\n",
    "        conv_configs_64_128 = [\n",
    "            (64, 3, .2),\n",
    "            (128, 3, .2),\n",
    "        ]\n",
    "        conv_configs_128_256 = [\n",
    "            (128, 3, .2),\n",
    "            (256, 3, .2),\n",
    "        ]\n",
    "        conv_configs_256_512 = [\n",
    "            (256, 3, .2),\n",
    "            (512, 3, .2),\n",
    "        ]\n",
    "        conv_configs_64_128_256 = [\n",
    "            (64, 3, .2),\n",
    "            (128, 3, .2),\n",
    "            (256, 3, .2),\n",
    "        ]\n",
    "        conv_configs_64_128_256_512 = [\n",
    "            (64, 3, .2),\n",
    "            (128, 3, .2),\n",
    "            (256, 3, .2),\n",
    "            (512, 3, .2),\n",
    "        ]\n",
    "\n",
    "        # Layers\n",
    "        self.encoders = nn.ModuleDict(OrderedDict(filter(\n",
    "            lambda item: item[1] is not None,\n",
    "            [\n",
    "                (\n",
    "                    'acc',\n",
    "                    self.make_bdconv_module(3, conv_configs, padding='same')\n",
    "                ),\n",
    "                (\n",
    "                    'rot',\n",
    "                    self.make_bdconv_module(4, conv_configs, padding='same')\n",
    "                    # self.make_small_conv_module(4, conv_configs_128_256, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'thm',\n",
    "                    self.make_small_conv_module(5, conv_configs_64_128, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'tof',\n",
    "                    self.make_small_conv_module(256, conv_configs_64_128, padding='same', bias=False)\n",
    "                    # self.make_small_conv_module(256, conv_configs_256_512, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'dem',\n",
    "                    self.make_small_conv_module(2, conv_configs_64_128, padding='same', bias=False)\n",
    "                ),\n",
    "                (\n",
    "                    'linear_acc',\n",
    "                    self.make_small_conv_module(3, conv_configs_64_128, padding='same', bias=False) if 'linear_acc' in in_encoders else None\n",
    "                ),\n",
    "                (\n",
    "                    'acc_rot',\n",
    "                    self.make_bdconv_module(\n",
    "                        7, conv_configs,\n",
    "                        padding='same',\n",
    "                        # 7, conv_configs_64_128_256,\n",
    "                        # padding='same',\n",
    "                    ) if 'acc_rot' in slice_encoders else None\n",
    "                ),\n",
    "            ]\n",
    "        )))\n",
    "        \n",
    "\n",
    "        self.lstm_m = nn.LSTM(\n",
    "            self.encoders_out_channels,\n",
    "            rnn_channels,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.gru_m = nn.GRU(\n",
    "            self.encoders_out_channels,\n",
    "            rnn_channels,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        if with_noise: self.noise_m = nn.Sequential(\n",
    "            GaussianNoise(),\n",
    "            nn.Linear(self.encoders_out_channels, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.dropout_m = nn.Dropout(.4)\n",
    "        \n",
    "        self.linear_backbone = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                map(\n",
    "                    self.make_linear_block,\n",
    "                    enumerate(self.linear_configs)\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.linear_configs[-1][1], self.out_channels)\n",
    "\n",
    "    def apply_one_branch(self, branch_m, x, prep_fns=[]):\n",
    "        # print(f'{type(branch_m)} {type(prep_fns)} {x.shape=}')\n",
    "\n",
    "        # Apply branch input prep functions\n",
    "        x = self.apply_prep_fns(x, prep_fns)\n",
    "        \n",
    "        # Apply RNN branch. RNN branches require channel-last input\n",
    "        # print(f'{x.shape=} {branch_m=}')\n",
    "        x, _ = branch_m(x)\n",
    "\n",
    "        # print(f'{x.shape=}')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_prep_fns(self, x, fns):\n",
    "        for fn in fns: x = fn(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def apply_branches(self, x):\n",
    "        # RNN branches\n",
    "        branches = list(filter(\n",
    "            lambda fn: fn is not None,\n",
    "            [\n",
    "                self.lstm_m,\n",
    "                self.gru_m,\n",
    "                self.noise_m if self.with_noise else None,\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        # Apply branches\n",
    "        xs = list(map(\n",
    "            lambda args: self.apply_one_branch(*args),\n",
    "            zip(branches, [x]*len(branches))\n",
    "        ))\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def apply_encoders(self, x):\n",
    "        # 1. Split inputs\n",
    "        x_splits = x.split(self.in_splits, dim=1)\n",
    "        # print(f'{len(x_splits)}')\n",
    "\n",
    "        # 2. Group splits by encoder\n",
    "        x_grouped = defaultdict(list)\n",
    "        for x_split, e_name in zip(x_splits, self.in_encoders):\n",
    "            # print(f'{e_name} {x_split.shape}')\n",
    "            x_grouped[e_name].append(x_split)\n",
    "        \n",
    "        # 3. Merge groups\n",
    "        xs = list(map(\n",
    "            lambda e_name: torch.concat(x_grouped[e_name], dim=1) if len(x_grouped[e_name]) > 1 else x_grouped[e_name][0],\n",
    "            self.in_encoders,\n",
    "        ))\n",
    "\n",
    "        # for x in xs:\n",
    "        #     print(f'{x.shape=}')\n",
    "\n",
    "        # 4. Apply encoders\n",
    "        xs = list(map(\n",
    "            lambda item: self.encoders[item[0]](item[1]), \n",
    "            zip(x_grouped.keys(), xs),\n",
    "        ))\n",
    "\n",
    "        return torch.concat(xs, dim=1)\n",
    "\n",
    "    def apply_slice_encoders(self, x):\n",
    "        # 1. Get slices\n",
    "        x_slices = []\n",
    "        for slices in self.slice_encoders.values():\n",
    "            x_slice = torch.concat(\n",
    "                list(map(lambda s: x[:, s, :], slices)),\n",
    "                dim=1\n",
    "            )\n",
    "            x_slices.append(x_slice)\n",
    "            # print(f'{x_slice.shape=}')\n",
    "            \n",
    "        # print(f'{len(x_slices)}')\n",
    "\n",
    "        # 4. Apply encoders\n",
    "        xs = list(map(\n",
    "            lambda item: self.encoders[item[0]](item[1]), \n",
    "            zip(self.slice_encoders.keys(), x_slices),\n",
    "        ))\n",
    "\n",
    "        return torch.concat(xs, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> Apply encoders\n",
    "        x_enc = self.apply_encoders(x)\n",
    "        # print(f'{x.shape=} <- apply_encoders()')\n",
    "\n",
    "        # -> Apply slice_encoders\n",
    "        if self.slice_encoders:\n",
    "            x_slice_enc = self.apply_slice_encoders(x)\n",
    "            # print(f'{x_slice_enc.shape=} <- apply_slice_encoders()')\n",
    "\n",
    "            ## -> Merge encoders\n",
    "            x = torch.concat([x_enc, x_slice_enc], dim=1)\n",
    "        else:\n",
    "            x = x_enc\n",
    "        \n",
    "        # -> Apply branches\n",
    "        xs = self.apply_branches(x.permute(0, 2, 1))\n",
    "        \n",
    "        # -> Join the branches on the channel dimension and apply attention\n",
    "        x = torch.concat(xs, dim=2)\n",
    "\n",
    "        # -> Apply Dropout\n",
    "        # print(f'{x.shape=}')\n",
    "        x = self.dropout_m(x)\n",
    "        \n",
    "        # -> Join the branches on the channel dimension and apply attention\n",
    "        x = F.scaled_dot_product_attention(x, x, x)\n",
    "        \n",
    "        # -> Apply linear backbone and pick the last element for classification\n",
    "        x = self.linear_backbone(x)\n",
    "        x = self.linear(x[..., -1, :])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def make_small_conv_module(self, in_channels, conv_configs, **kwargs):\n",
    "        def conv_fn(args):\n",
    "            # Extract parameters\n",
    "            conv_idx, (conv_out, kernel, dropout) = args\n",
    "            conv_in = in_channels if conv_idx == 0 else conv_configs[conv_idx - 1][0]\n",
    "\n",
    "            # Create layers\n",
    "            layers = [\n",
    "                ConvBlock1d(conv_in, conv_out, kernel, **kwargs)\n",
    "            ]\n",
    "            \n",
    "            if dropout is not None: layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            return layers\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            *sum(\n",
    "                map(\n",
    "                    conv_fn,\n",
    "                    enumerate(conv_configs)\n",
    "                ),\n",
    "                []\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def make_bdconv_module(self, in_channels, conv_configs, **kwargs):\n",
    "        def bdconv_fn(args):\n",
    "            # Extract parameters\n",
    "            conv_idx, config = args\n",
    "            conv_in = in_channels if conv_idx == 0 else conv_configs[conv_idx - 1][0]\n",
    "\n",
    "            return BDConvBlock(\n",
    "                f'bdconv_{conv_idx}', conv_in, *config,\n",
    "                squeeze_channels=self.squeeze_channels, **kwargs,\n",
    "            )\n",
    "            \n",
    "        return nn.Sequential(*map(bdconv_fn, enumerate(conv_configs)))\n",
    "\n",
    "    def make_linear_block(self, args):\n",
    "        idx, config = args\n",
    "        block_name = f'block_{idx}'\n",
    "        block = BDLinearBlock(block_name, *config, bias=False)\n",
    "\n",
    "        return block_name, block\n",
    "        \n",
    "hard_label_model = HardLabelModel(\n",
    "    in_channels=model_config.in_channels,\n",
    "    out_channels=model_config.out_channels,\n",
    "    \n",
    "    in_splits=model_config.in_splits,\n",
    "    in_encoders=model_config.in_encoders,\n",
    "    slice_encoders={},\n",
    "    \n",
    "    squeeze_channels=model_config.squeeze_channels,\n",
    "    rnn_channels=model_config.rnn_channels,\n",
    "    \n",
    "    with_noise=model_config.with_noise,\n",
    ")\n",
    "\n",
    "if 'model.summary' in config.features:\n",
    "    print(\n",
    "        summary(\n",
    "            model=hard_label_model, \n",
    "            input_size=(16, model_config.in_channels, model_config.input_size),\n",
    "            col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "            col_width=20,\n",
    "            row_settings=[\"var_names\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6676153",
   "metadata": {
    "_cell_guid": "7e81583c-21fb-40c8-9e95-88dea6b779c8",
    "_uuid": "fb8a3fb8-b4ed-4afc-b026-aa5e320dbaa4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003328,
     "end_time": "2025-08-21T10:03:07.707666",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.704338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d30f15",
   "metadata": {
    "_cell_guid": "85798e87-73b4-477a-98c6-e9d763cd7138",
    "_uuid": "9118e769-38c3-4cf4-bdde-8635a7d101a2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003294,
     "end_time": "2025-08-21T10:03:07.714273",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.710979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c421b61",
   "metadata": {
    "_cell_guid": "1cd47fde-9411-4545-9d27-d21d70a298eb",
    "_uuid": "07aef3d5-eee2-4167-982d-1eea134982ba",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:07.721772Z",
     "iopub.status.busy": "2025-08-21T10:03:07.721572Z",
     "iopub.status.idle": "2025-08-21T10:03:07.728785Z",
     "shell.execute_reply": "2025-08-21T10:03:07.728111Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012222,
     "end_time": "2025-08-21T10:03:07.729790",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.717568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_config=TrainConfig(name='train', num_classes=18, batch_size=256, epochs=150, optimizer=OptimizerConfig(name='adam', lr=0.0007, early_stopping=15, patience=3, lr_decay=0.65, min_lr=3e-05, momentum=0.9, weight_decay=0.003), early_stop=False, device=device(type='cpu'), dataloader_workers=3, checkpoint=PosixPath('weights.pt'), load_best_ckpt=False, items_to_mix=2, mixup_prob=0.4)\n",
      "valid_config=TrainConfig(name='valid', num_classes=18, batch_size=256, epochs=150, optimizer=OptimizerConfig(name='adam', lr=0.0007, early_stopping=15, patience=3, lr_decay=0.65, min_lr=3e-05, momentum=0.9, weight_decay=0.003), early_stop=False, device=device(type='cpu'), dataloader_workers=3, checkpoint=None, load_best_ckpt=False, items_to_mix=2, mixup_prob=0.4)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Callable, Union\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    name: str = 'adam'\n",
    "    lr: float = 7E-4\n",
    "    \n",
    "    early_stopping: int = 15\n",
    "\n",
    "    # Scheduler\n",
    "    patience: int = 3\n",
    "    lr_decay: float = 0.65\n",
    "    min_lr: float = 3E-5\n",
    "    \n",
    "    momentum: float = 0.9\n",
    "\n",
    "    # Regularization\n",
    "    weight_decay: float = 3E-3\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    name: str = 'train'\n",
    "    num_classes: int = 18\n",
    "        \n",
    "    batch_size: int = 256\n",
    "    epochs: int = 150\n",
    "        \n",
    "    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n",
    "    early_stop: bool = False\n",
    "\n",
    "    device: str = 'cpu'\n",
    "    dataloader_workers: int = 3\n",
    "\n",
    "    checkpoint: Path = None\n",
    "    load_best_ckpt: bool = False\n",
    "\n",
    "    # Collater\n",
    "    items_to_mix: int = 2\n",
    "    mixup_prob: float = 0.4\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    name='train',\n",
    "    num_classes=model_config.out_channels,\n",
    "    device=config.device,\n",
    "    checkpoint=Path('weights.pt')\n",
    ")\n",
    "valid_config = TrainConfig(\n",
    "    name='valid',\n",
    "    num_classes=model_config.out_channels,\n",
    "    device=config.device,\n",
    ")\n",
    "\n",
    "print(f'{train_config=}')\n",
    "print(f'{valid_config=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eee1628",
   "metadata": {
    "_cell_guid": "632c6f29-9284-4d37-8a0d-4e4ccc81b118",
    "_uuid": "bd8a0b56-cc9b-4a89-a7bb-8fbb173a86f8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:07.737520Z",
     "iopub.status.busy": "2025-08-21T10:03:07.737334Z",
     "iopub.status.idle": "2025-08-21T10:03:07.741001Z",
     "shell.execute_reply": "2025-08-21T10:03:07.740436Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008592,
     "end_time": "2025-08-21T10:03:07.742052",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.733460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if 'checkpoint' in config.features:\n",
    "    ckpt_path = Path('/kaggle/input/cmi-behavior-detection-lstm/weights.pt')\n",
    "    print(f'Load:: {ckpt_path=}')\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            ckpt_path,\n",
    "            weights_only=True,\n",
    "            map_location=torch.device(config.device)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92317b00",
   "metadata": {
    "_cell_guid": "6f380759-2ddf-4159-9213-8833552e244f",
    "_uuid": "d0b7309f-7315-4d6f-93aa-2e5cfbcee66d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003359,
     "end_time": "2025-08-21T10:03:07.749488",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.746129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea934f8",
   "metadata": {
    "_cell_guid": "49e1f159-dc0e-458e-9296-8a4da0fcdccb",
    "_uuid": "130c0374-9503-44a2-8c08-358e5d18c91f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003188,
     "end_time": "2025-08-21T10:03:07.756061",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.752873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Competition Metrics\n",
    "\n",
    "* We use: https://www.kaggle.com/code/richolson/cmi-2025-metric-copy-for-import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe99137",
   "metadata": {
    "_cell_guid": "9d0520ab-4b28-4839-8ab4-756787eb04ef",
    "_uuid": "2a40dbec-57e1-4a8b-8fe7-be7098d76088",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003188,
     "end_time": "2025-08-21T10:03:07.762780",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.759592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e74cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:07.770237Z",
     "iopub.status.busy": "2025-08-21T10:03:07.770035Z",
     "iopub.status.idle": "2025-08-21T10:03:10.508688Z",
     "shell.execute_reply": "2025-08-21T10:03:10.508128Z"
    },
    "papermill": {
     "duration": 2.743787,
     "end_time": "2025-08-21T10:03:10.509894",
     "exception": false,
     "start_time": "2025-08-21T10:03:07.766107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def get_sequence_prep_fn(config, data_config, sm_window=None):\n",
    "    def fn(sequence_data):\n",
    "        ## Placeholder for data\n",
    "        # data = sequence_data[sequence_data.columns]\n",
    "        data = sequence_data[data_config.sequence_columns()].copy()\n",
    "\n",
    "        ## Fix tof data\n",
    "        if data_config.columns('tof'):\n",
    "            ### Get tof_columns values\n",
    "            tof_values = data[data_config.columns('tof')].values\n",
    "\n",
    "            ### Replace -1 with NaN\n",
    "            tof_values[tof_values == -1.] = np.nan\n",
    "\n",
    "            ### Compute mean of tof_columns while skipping NaNs\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                tof_mean = np.nan_to_num(np.nanmean(tof_values, axis=0))\n",
    "\n",
    "            ### Replace NaN with means\n",
    "            tof_values = np.where(np.isnan(tof_values), tof_mean, tof_values)\n",
    "\n",
    "            ### Update the DataFrame\n",
    "            data[data_config.columns('tof')] = tof_values\n",
    "\n",
    "        ### Fill NaN with the mean value\n",
    "        means = data.mean().fillna(0)\n",
    "        data = data.ffill().bfill().fillna(means.to_dict())\n",
    "        \n",
    "        ## Data scaling\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "\n",
    "        ## Data smoothening\n",
    "        if sm_window:\n",
    "            data = savgol_filter(data, sm_window, 3, axis=0)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f294ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:10.518277Z",
     "iopub.status.busy": "2025-08-21T10:03:10.517839Z",
     "iopub.status.idle": "2025-08-21T10:03:10.522194Z",
     "shell.execute_reply": "2025-08-21T10:03:10.521716Z"
    },
    "papermill": {
     "duration": 0.009382,
     "end_time": "2025-08-21T10:03:10.523029",
     "exception": false,
     "start_time": "2025-08-21T10:03:10.513647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_models(model_weights, device):\n",
    "    def load_fn(args):\n",
    "        name, (m, p) = args\n",
    "        \n",
    "        print(f'Loading [({name})]:: {p=}')\n",
    "        m.load_state_dict(\n",
    "            torch.load(\n",
    "                p,\n",
    "                weights_only=True,\n",
    "                map_location=torch.device(device)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return m\n",
    "\n",
    "    models = list(map(load_fn, model_weights.items()))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c1f56f",
   "metadata": {
    "_cell_guid": "f400a6ca-f5c0-45f0-be65-b0c62c3922f5",
    "_uuid": "c967e19e-a5f4-45c9-8428-22fadd65073e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T10:03:10.530845Z",
     "iopub.status.busy": "2025-08-21T10:03:10.530654Z",
     "iopub.status.idle": "2025-08-21T10:03:14.554812Z",
     "shell.execute_reply": "2025-08-21T10:03:14.554212Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.029239,
     "end_time": "2025-08-21T10:03:14.555815",
     "exception": false,
     "start_time": "2025-08-21T10:03:10.526576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading [(mixup)]:: p=PosixPath('/kaggle/input/cmi-behavior-detection-84/weights.pt')\n",
      "Loading [(hard-labels)]:: p=PosixPath('/kaggle/input/cmi-behavior-detection-lstm/weights.pt')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>gesture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEQ_000001</td>\n",
       "      <td>Neck - scratch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEQ_000011</td>\n",
       "      <td>Eyelash - pull hair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequence_id              gesture\n",
       "0  SEQ_000001       Neck - scratch\n",
       "1  SEQ_000011  Eyelash - pull hair"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from transforms import FixLength, Transpose, ToType, ToTensor, Resize, Clip\n",
    "from torch_evaluation import get_predict_fn, get_multi_predict_fn, get_slices_fn, get_slices_predict_fn\n",
    "from core_utilities import make_header, load_pkl\n",
    "from pathlib import Path\n",
    "\n",
    "# Sequence Builder\n",
    "test_sequence_prep_fn = get_sequence_prep_fn(config, data_config)\n",
    "\n",
    "# Class names\n",
    "class_names = load_pkl(config.data_root / 'class_names.pkl')\n",
    "\n",
    "# Model Weights\n",
    "model_weights = {\n",
    "    'mixup': (\n",
    "        model,\n",
    "        Path('/kaggle/input/cmi-behavior-detection-84/weights.pt')\n",
    "    ),\n",
    "    'hard-labels': (\n",
    "        hard_label_model,\n",
    "        Path('/kaggle/input/cmi-behavior-detection-lstm/weights.pt')\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Load all models\n",
    "models = load_models(model_weights, config.device)\n",
    "\n",
    "# Multi-slice test transforms\n",
    "# base_predict_fn = get_predict_fn(model, config.device)\n",
    "base_predict_fn = get_multi_predict_fn(models, config.device)\n",
    "slices_fn = get_slices_fn(\n",
    "    slice_dim=0,\n",
    "    slice_size=data_config.slice_len,\n",
    "    stride=16,\n",
    "    transforms=[Clip(limits=(-3.5, 2.5)), FixLength(data_config.sequence_len)]\n",
    ")\n",
    "\n",
    "predict_fn = get_slices_predict_fn(\n",
    "    base_predict_fn,\n",
    "    slices_fn,\n",
    "    transforms=[\n",
    "        Transpose(dims=(0, 2, 1)),\n",
    "        ToType(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    ## Convert daa\n",
    "    data = sequence.to_pandas()[data_config.columns(['metadata', 'acc', 'rot', 'thm', 'tof'])]\n",
    "    demographics_data = demographics.to_pandas()[data_config.columns(['metadata_d', 'd'])]\n",
    "    \n",
    "    selection = data_config.sequence_columns()\n",
    "    data = pd.merge(data, demographics_data, on='subject')[selection]\n",
    "\n",
    "    ## Normalize\n",
    "    X = test_sequence_prep_fn(data)\n",
    "\n",
    "    ## Compute probabilities\n",
    "    probs = predict_fn(X)\n",
    "\n",
    "    ## Compute predicted class\n",
    "    y_label = np.squeeze(torch.argmax(probs, dim=1).cpu().numpy())\n",
    "\n",
    "    ## Compute gesture\n",
    "    gesture = class_names[y_label]\n",
    "\n",
    "    return gesture\n",
    "\n",
    "if 'competition.metrics.test' in config.features:\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "                '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "            )\n",
    "        )\n",
    "        display(pd.read_parquet('submission.parquet'))\n",
    "\n",
    "# if 'competition.metrics.test' in config.features:\n",
    "#     print(make_header('Valid Gestures'))\n",
    "#     evaluate_score(X_test, None, label_encoders[0].classes_, model, valid_config)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "sourceId": 242954653,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 254765745,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 254773128,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 254975243,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 255778641,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 256586827,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 257063621,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 257277832,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.483665,
   "end_time": "2025-08-21T10:03:16.960990",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-21T10:02:51.477325",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
