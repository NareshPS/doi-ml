{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet1d, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet1d._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet1d._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet1d._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet1d._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet1d._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose1d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet1d._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose1d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet1d._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose1d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet1d._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose1d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet1d._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.conv(dec1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm1d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm1d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "# in_channels = 6\n",
    "# unet = UNet1d(in_channels=in_channels)\n",
    "# print(summary(\n",
    "#     model=unet, \n",
    "#     input_size=(1, in_channels, 512),\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 512])\n",
      "torch.Size([16, 32, 256])\n",
      "torch.Size([16, 64, 256])\n",
      "torch.Size([16, 64, 128])\n",
      "torch.Size([16, 128, 128])\n",
      "torch.Size([16, 128, 64])\n",
      "torch.Size([16, 256, 64])\n",
      "torch.Size([16, 256, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                  Output Shape         Param #              Trainable\n",
       "====================================================================================================\n",
       "UNetFlex1d (UNetFlex1d)                  [16, 1, 512]         --                   True\n",
       "├─Sequential (encoder1)                  [16, 32, 512]        --                   True\n",
       "│    └─Conv1d (enc1conv1)                [16, 32, 512]        96                   True\n",
       "│    └─BatchNorm1d (enc1norm1)           [16, 32, 512]        64                   True\n",
       "│    └─ReLU (enc1relu1)                  [16, 32, 512]        --                   --\n",
       "│    └─Conv1d (enc1conv2)                [16, 32, 512]        3,072                True\n",
       "│    └─BatchNorm1d (enc1norm2)           [16, 32, 512]        64                   True\n",
       "│    └─ReLU (enc1relu2)                  [16, 32, 512]        --                   --\n",
       "├─MaxPool1d (pool1)                      [16, 32, 256]        --                   --\n",
       "├─Sequential (encoder2)                  [16, 64, 256]        --                   True\n",
       "│    └─Conv1d (enc2conv1)                [16, 64, 256]        6,144                True\n",
       "│    └─BatchNorm1d (enc2norm1)           [16, 64, 256]        128                  True\n",
       "│    └─ReLU (enc2relu1)                  [16, 64, 256]        --                   --\n",
       "│    └─Conv1d (enc2conv2)                [16, 64, 256]        12,288               True\n",
       "│    └─BatchNorm1d (enc2norm2)           [16, 64, 256]        128                  True\n",
       "│    └─ReLU (enc2relu2)                  [16, 64, 256]        --                   --\n",
       "├─MaxPool1d (pool2)                      [16, 64, 128]        --                   --\n",
       "├─Sequential (encoder3)                  [16, 128, 128]       --                   True\n",
       "│    └─Conv1d (enc3conv1)                [16, 128, 128]       24,576               True\n",
       "│    └─BatchNorm1d (enc3norm1)           [16, 128, 128]       256                  True\n",
       "│    └─ReLU (enc3relu1)                  [16, 128, 128]       --                   --\n",
       "│    └─Conv1d (enc3conv2)                [16, 128, 128]       49,152               True\n",
       "│    └─BatchNorm1d (enc3norm2)           [16, 128, 128]       256                  True\n",
       "│    └─ReLU (enc3relu2)                  [16, 128, 128]       --                   --\n",
       "├─MaxPool1d (pool3)                      [16, 128, 64]        --                   --\n",
       "├─Sequential (encoder4)                  [16, 256, 64]        --                   True\n",
       "│    └─Conv1d (enc4conv1)                [16, 256, 64]        98,304               True\n",
       "│    └─BatchNorm1d (enc4norm1)           [16, 256, 64]        512                  True\n",
       "│    └─ReLU (enc4relu1)                  [16, 256, 64]        --                   --\n",
       "│    └─Conv1d (enc4conv2)                [16, 256, 64]        196,608              True\n",
       "│    └─BatchNorm1d (enc4norm2)           [16, 256, 64]        512                  True\n",
       "│    └─ReLU (enc4relu2)                  [16, 256, 64]        --                   --\n",
       "├─MaxPool1d (pool4)                      [16, 256, 32]        --                   --\n",
       "├─Sequential (bottleneck)                [16, 512, 32]        --                   True\n",
       "│    └─Conv1d (bottleneckconv1)          [16, 512, 32]        393,216              True\n",
       "│    └─BatchNorm1d (bottlenecknorm1)     [16, 512, 32]        1,024                True\n",
       "│    └─ReLU (bottleneckrelu1)            [16, 512, 32]        --                   --\n",
       "│    └─Conv1d (bottleneckconv2)          [16, 512, 32]        786,432              True\n",
       "│    └─BatchNorm1d (bottlenecknorm2)     [16, 512, 32]        1,024                True\n",
       "│    └─ReLU (bottleneckrelu2)            [16, 512, 32]        --                   --\n",
       "├─ConvTranspose1d (upconv4)              [16, 256, 64]        262,400              True\n",
       "├─Sequential (decoder4)                  [16, 256, 64]        --                   True\n",
       "│    └─Conv1d (dec4conv1)                [16, 256, 64]        393,216              True\n",
       "│    └─BatchNorm1d (dec4norm1)           [16, 256, 64]        512                  True\n",
       "│    └─ReLU (dec4relu1)                  [16, 256, 64]        --                   --\n",
       "│    └─Conv1d (dec4conv2)                [16, 256, 64]        196,608              True\n",
       "│    └─BatchNorm1d (dec4norm2)           [16, 256, 64]        512                  True\n",
       "│    └─ReLU (dec4relu2)                  [16, 256, 64]        --                   --\n",
       "├─ConvTranspose1d (upconv3)              [16, 128, 128]       65,664               True\n",
       "├─Sequential (decoder3)                  [16, 128, 128]       --                   True\n",
       "│    └─Conv1d (dec3conv1)                [16, 128, 128]       98,304               True\n",
       "│    └─BatchNorm1d (dec3norm1)           [16, 128, 128]       256                  True\n",
       "│    └─ReLU (dec3relu1)                  [16, 128, 128]       --                   --\n",
       "│    └─Conv1d (dec3conv2)                [16, 128, 128]       49,152               True\n",
       "│    └─BatchNorm1d (dec3norm2)           [16, 128, 128]       256                  True\n",
       "│    └─ReLU (dec3relu2)                  [16, 128, 128]       --                   --\n",
       "├─ConvTranspose1d (upconv2)              [16, 64, 256]        16,448               True\n",
       "├─Sequential (decoder2)                  [16, 64, 256]        --                   True\n",
       "│    └─Conv1d (dec2conv1)                [16, 64, 256]        24,576               True\n",
       "│    └─BatchNorm1d (dec2norm1)           [16, 64, 256]        128                  True\n",
       "│    └─ReLU (dec2relu1)                  [16, 64, 256]        --                   --\n",
       "│    └─Conv1d (dec2conv2)                [16, 64, 256]        12,288               True\n",
       "│    └─BatchNorm1d (dec2norm2)           [16, 64, 256]        128                  True\n",
       "│    └─ReLU (dec2relu2)                  [16, 64, 256]        --                   --\n",
       "├─ConvTranspose1d (upconv1)              [16, 32, 512]        4,128                True\n",
       "├─Sequential (decoder1)                  [16, 32, 512]        --                   True\n",
       "│    └─Conv1d (dec1conv1)                [16, 32, 512]        6,144                True\n",
       "│    └─BatchNorm1d (dec1norm1)           [16, 32, 512]        64                   True\n",
       "│    └─ReLU (dec1relu1)                  [16, 32, 512]        --                   --\n",
       "│    └─Conv1d (dec1conv2)                [16, 32, 512]        3,072                True\n",
       "│    └─BatchNorm1d (dec1norm2)           [16, 32, 512]        64                   True\n",
       "│    └─ReLU (dec1relu2)                  [16, 32, 512]        --                   --\n",
       "├─Conv1d (output)                        [16, 1, 512]         33                   True\n",
       "====================================================================================================\n",
       "Total params: 2,707,809\n",
       "Trainable params: 2,707,809\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.80\n",
       "====================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 83.95\n",
       "Params size (MB): 10.83\n",
       "Estimated Total Size (MB): 94.82\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torchinfo import summary\n",
    "from functools import reduce\n",
    "from itertools import accumulate\n",
    "from torchview import draw_graph\n",
    "\n",
    "class UNetFlex1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, blocks=[32, 64, 128, 256]):\n",
    "        super(UNetFlex1d, self).__init__()\n",
    "\n",
    "        self.blocks = blocks\n",
    "\n",
    "        # Encoder Modules\n",
    "        in_features = in_channels\n",
    "        for enc_id, block in enumerate(blocks, start=1):\n",
    "            setattr(\n",
    "                self, f'encoder{enc_id}',\n",
    "                UNetFlex1d._block(in_features, block, name=f'enc{enc_id}')\n",
    "            )\n",
    "            in_features = block\n",
    "\n",
    "        # Pooling Layers\n",
    "        for pool_id, _ in enumerate(blocks, start=1):\n",
    "            setattr(\n",
    "                self, f'pool{pool_id}',\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "            )\n",
    "\n",
    "        # Bottleneck Module\n",
    "        self.bottleneck = UNetFlex1d._block(blocks[-1], blocks[-1]*2, name=\"bottleneck\")\n",
    "\n",
    "        # Upconv Layers\n",
    "        for up_id, block in enumerate(blocks, start=1):\n",
    "            setattr(\n",
    "                self, f'upconv{up_id}',\n",
    "                nn.ConvTranspose1d(\n",
    "                    block*2, block, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Decoder Modules\n",
    "        for dec_id, block in enumerate(blocks, start=1):\n",
    "            setattr(\n",
    "                self, f'decoder{dec_id}',\n",
    "                UNetFlex1d._block(block*2, block, name=f'dec{dec_id}')\n",
    "            )\n",
    "\n",
    "        # Output Layer\n",
    "        self.output = nn.Conv1d(\n",
    "            in_channels=blocks[0], out_channels=out_channels, kernel_size=1,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1. Encoder Leg\n",
    "        block_encodings = [None]\n",
    "        for block_id, _ in enumerate(self.blocks, start=1):\n",
    "            encoder = getattr(self, f'encoder{block_id}')\n",
    "            pool = getattr(self, f'pool{block_id}')\n",
    "\n",
    "            x = encoder(x)\n",
    "            block_encodings.append(x)\n",
    "            x = pool(x)\n",
    "\n",
    "        # 2. Apply Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # 3. Decoder Leg\n",
    "        for block_id in range(len(self.blocks), 0, -1):\n",
    "            upconv = getattr(self, f'upconv{block_id}')\n",
    "            decoder = getattr(self, f'decoder{block_id}')\n",
    "            block_encoding = block_encodings[block_id]\n",
    "\n",
    "            x = upconv(x)\n",
    "            x = torch.cat((block_encoding, x), dim=1)\n",
    "            x = decoder(x)\n",
    "\n",
    "        # 4. Output\n",
    "        output = self.output(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm1d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm1d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "model = UNetFlex1d(in_channels=1)\n",
    "\n",
    "summary(\n",
    "    model=model, \n",
    "    input_size=(16, 1, 512),\n",
    "    # input_size=(4, 1, 512, 512),\n",
    "    col_names=[\"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")\n",
    "\n",
    "# graph = draw_graph(\n",
    "#     model, \n",
    "#     input_size=(1, 1, 224, 224), \n",
    "#     # expand_nested=True\n",
    "# )\n",
    "\n",
    "# # View Model Architecture\n",
    "# graph.visual_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
