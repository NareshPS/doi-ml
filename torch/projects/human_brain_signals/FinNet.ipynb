{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ChannelSeBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class ChannelSeBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, squeeze_channels,\n",
    "        activation_layer=nn.ReLU,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "    ):\n",
    "        \"\"\"ChannelSeBlock is a Squeeze-And-Excitation block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = pool_block(1)\n",
    "        self.conv1 = conv_block(\n",
    "            in_channels, squeeze_channels, 1,\n",
    "            activation_layer=activation_layer,\n",
    "            norm_layer=None, bias=False,\n",
    "        )\n",
    "        self.conv2 = conv_block(\n",
    "            squeeze_channels, in_channels, 1,\n",
    "            norm_layer=None, activation_layer=None, bias=False,\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "\n",
    "        # 1. (...) -> (..., in_channels, 1, 1)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 2. -> (..., squeeze_channels, 1, 1)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # 3. -> (..., in_channels, 1, 1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 4. Scale\n",
    "        x = self.sigmoid(x)\n",
    "        x = inp * x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10, 10)\n",
    "# l = ChannelSeBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. SpatialSeBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class SpatialSeBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, squeeze_channels,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"SpatialSeBlock is a spatial Squeeze-And-Excitation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(\n",
    "            in_channels, 1, 1,\n",
    "            norm_layer=None, activation_layer=None, bias=None,\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Save input -> (..., in_channels, H, W)\n",
    "        inp = x\n",
    "\n",
    "        # 2. -> (..., 1, H, W)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # 3. Scale\n",
    "        x = self.sigmoid(x)\n",
    "        x = inp * x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10, 10)\n",
    "# l = SpatialSeBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. SpatialChannelSeBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class SpatialChannelSeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, **kwargs):\n",
    "        \"\"\"SpatialChannelSeBlock is a Squeeze-And-Excitation block\n",
    "        which combines spatial and channel squeeze-and-excitation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_se = SpatialSeBlock(\n",
    "            in_channels=in_channels, squeeze_channels=squeeze_channels,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.channel_se = ChannelSeBlock(\n",
    "            in_channels=in_channels, squeeze_channels=squeeze_channels,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Spatial scaling\n",
    "        spatial_x = self.spatial_se(x)\n",
    "\n",
    "        # 2. Channel scaling\n",
    "        channel_x = self.channel_se(x)\n",
    "\n",
    "        # 3. Combine spatial and channel scaling results\n",
    "        x = spatial_x + channel_x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10, 10)\n",
    "# l = SpatialChannelSeBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. MBConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same', squeeze_ratio=4,\n",
    "        activation_layer=nn.SiLU,\n",
    "        se_block=ChannelSeBlock,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"MBConv is a MBConv block with SEBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        modules = nn.ModuleList()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        # 1. (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "        if in_channels != bottleneck_size:\n",
    "            modules.append(\n",
    "                conv_block(\n",
    "                    in_channels, bottleneck_size,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=activation_layer,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # 2. (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "        modules.append(\n",
    "            conv_block(\n",
    "                bottleneck_size, bottleneck_size,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                groups=bottleneck_size, # Depthwise Convolution\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 3. Squeeze and excitation block\n",
    "        squeeze_channels = max(1, in_channels // squeeze_ratio)\n",
    "        modules.append(\n",
    "            se_block(\n",
    "                bottleneck_size, squeeze_channels,\n",
    "                activation_layer=activation_layer,\n",
    "                conv_block=conv_block,\n",
    "                pool_block=pool_block,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 4. (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "        modules.append(\n",
    "            conv_block(\n",
    "                bottleneck_size, out_channels,\n",
    "                kernel_size=1, stride=1, padding='same',\n",
    "                activation_layer=nn.Identity,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.block = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = self.block(x)\n",
    "\n",
    "        if self.residual:\n",
    "            x = x + inp\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels, out_channels = 32, 4\n",
    "# kernel_size = 3\n",
    "# batch_size = 1\n",
    "# l = MBConv(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size, bottleneck=1,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, 20, 20)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FinNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1. FinNetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class FinNetConfig(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Efficient B4 configuration\n",
    "        width_mult=1.4, depth_mult=1.8, dropout=0.4, last_channels=1280,\n",
    "\n",
    "        # Block configuration\n",
    "        kernel_1=3, kernel_2=5,\n",
    "\n",
    "        # Blocks\n",
    "        se_block=ChannelSeBlock,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "\n",
    "        # Fin configuration\n",
    "        fins=4,\n",
    "        fin_depth=4,\n",
    "        fin_output=False,\n",
    "    ):\n",
    "        self.width_mult = width_mult\n",
    "        self.depth_mult =depth_mult\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.last_channels = last_channels\n",
    "\n",
    "        # Conv type 1\n",
    "        self.kernel_1 = kernel_1\n",
    "\n",
    "        # Conv type 2\n",
    "        self.kernel_2 = kernel_2\n",
    "\n",
    "        # Blocks\n",
    "        self.se_block = se_block\n",
    "        self.conv_block = conv_block\n",
    "        self.pool_block = pool_block\n",
    "\n",
    "        # Fin configuration\n",
    "        self.fins = fins\n",
    "        self.fin_depth = fin_depth\n",
    "        self.fin_output = fin_output\n",
    "\n",
    "        # MBConv configs\n",
    "        self.block_configs = [\n",
    "            # (in_channels, out_channels, bottleneck, squeeze_channels, kernel, padding, stride, layers)\n",
    "            (32, 16, 1, None, kernel_1, 'same', 1, 1),\n",
    "            (16, 24, 6, None, 3, 1, 2, 2),\n",
    "            (24, 40, 6, None, 5, 2, 2, 2),\n",
    "            (40, 80, 6, None, 3, 1, 2, 3),\n",
    "            (80, 112, 6, None, kernel_2, 'same', 1, 3),\n",
    "            (112, 192, 6, None, 5, 2, 2, 4),\n",
    "            (192, 320, 6, None, kernel_1, 'same', 1, 1),\n",
    "        ]\n",
    "\n",
    "    def adjust_channels(self, channels):\n",
    "        return self.round_to(channels*self.width_mult)\n",
    "    \n",
    "    def adjust_depth(self, num_layers):\n",
    "        return int(math.ceil(num_layers*self.depth_mult))\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_to(v, multiple=8):\n",
    "        return int(multiple * round(v / multiple))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_squeeze_ratio(channels, squeeze_channels, default=4):\n",
    "        return channels//squeeze_channels if squeeze_channels else default\n",
    "    \n",
    "    def _block(self, args, **kwargs):\n",
    "        in_channels, out_channels, bottleneck, squeeze_channels, kernel, padding, stride, layers = args\n",
    "\n",
    "        # 1. Update in_channels and out_channels based on the width_mult\n",
    "        in_channels = self.adjust_channels(in_channels)\n",
    "        out_channels = self.adjust_channels(out_channels)\n",
    "\n",
    "        # 2. Update layers based on depth_mult\n",
    "        layers = self.adjust_depth(layers)\n",
    "        \n",
    "        block = nn.Sequential(\n",
    "            MBConv(\n",
    "                in_channels, out_channels,\n",
    "                bottleneck=bottleneck, squeeze_ratio=self.get_squeeze_ratio(in_channels, squeeze_channels),\n",
    "                kernel_size=kernel, stride=stride, padding=padding,\n",
    "                se_block=self.se_block,\n",
    "                conv_block=self.conv_block,\n",
    "                pool_block=self.pool_block,\n",
    "                **kwargs\n",
    "            ),\n",
    "            *map(\n",
    "                lambda _: MBConv(\n",
    "                    out_channels, out_channels,\n",
    "                    bottleneck=bottleneck, squeeze_ratio=self.get_squeeze_ratio(out_channels, squeeze_channels),\n",
    "                    kernel_size=kernel, stride=1, padding='same',\n",
    "                    se_block=self.se_block,\n",
    "                    conv_block=self.conv_block,\n",
    "                    pool_block=self.pool_block,\n",
    "                    **kwargs\n",
    "                ),\n",
    "                range(layers - 1)\n",
    "            )\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def make_blocks(self, **kwargs):\n",
    "        # 1. Create 'fins' fin modules until fin_depth.\n",
    "        fin_modules = nn.ModuleList(map(\n",
    "            lambda _: nn.Sequential(\n",
    "                *map(\n",
    "                    lambda b_config: self._block(b_config,  **kwargs),\n",
    "                    self.block_configs[:self.fin_depth]\n",
    "                )\n",
    "            ),\n",
    "            range(self.fins)\n",
    "        ))\n",
    "\n",
    "        # 2. Create fin extension\n",
    "        extension = nn.Sequential(\n",
    "            *map(\n",
    "                lambda b_config: self._block(b_config,  **kwargs),\n",
    "                self.block_configs[self.fin_depth:]\n",
    "            )\n",
    "        )\n",
    "        return fin_modules, extension\n",
    "\n",
    "# fin_config = FinNetConfig(\n",
    "#     # se_block=ChannelSeBlock,\n",
    "#     # conv_block=Conv1dNormActivation,\n",
    "#     # pool_block=nn.AdaptiveAvgPool1d\n",
    "    \n",
    "# )\n",
    "# block_config = fin_config.block_configs[0]\n",
    "# fin_modules, extension = fin_config.make_blocks()\n",
    "\n",
    "# x = torch.randn(1, 48, 224, 224)\n",
    "# print(summary(\n",
    "#     model=fin_modules[0], \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"],\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. FinNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================\n",
      "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
      "=================================================================================================================================================\n",
      "FinNet (FinNet)                                                   [1, 16, 128, 128]    [1, 6]               --                   True\n",
      "├─ModuleList (fin_init_convs)                                     --                   --                   --                   True\n",
      "│    └─Conv2dNormActivation (0)                                   [1, 4, 128, 128]     [1, 48, 64, 64]      --                   True\n",
      "│    │    └─Conv2d (0)                                            [1, 4, 128, 128]     [1, 48, 64, 64]      1,728                True\n",
      "│    │    └─BatchNorm2d (1)                                       [1, 48, 64, 64]      [1, 48, 64, 64]      96                   True\n",
      "│    │    └─SiLU (2)                                              [1, 48, 64, 64]      [1, 48, 64, 64]      --                   --\n",
      "│    └─Conv2dNormActivation (1)                                   [1, 4, 128, 128]     [1, 48, 64, 64]      --                   True\n",
      "│    │    └─Conv2d (0)                                            [1, 4, 128, 128]     [1, 48, 64, 64]      1,728                True\n",
      "│    │    └─BatchNorm2d (1)                                       [1, 48, 64, 64]      [1, 48, 64, 64]      96                   True\n",
      "│    │    └─SiLU (2)                                              [1, 48, 64, 64]      [1, 48, 64, 64]      --                   --\n",
      "│    └─Conv2dNormActivation (2)                                   [1, 4, 128, 128]     [1, 48, 64, 64]      --                   True\n",
      "│    │    └─Conv2d (0)                                            [1, 4, 128, 128]     [1, 48, 64, 64]      1,728                True\n",
      "│    │    └─BatchNorm2d (1)                                       [1, 48, 64, 64]      [1, 48, 64, 64]      96                   True\n",
      "│    │    └─SiLU (2)                                              [1, 48, 64, 64]      [1, 48, 64, 64]      --                   --\n",
      "│    └─Conv2dNormActivation (3)                                   [1, 4, 128, 128]     [1, 48, 64, 64]      --                   True\n",
      "│    │    └─Conv2d (0)                                            [1, 4, 128, 128]     [1, 48, 64, 64]      1,728                True\n",
      "│    │    └─BatchNorm2d (1)                                       [1, 48, 64, 64]      [1, 48, 64, 64]      96                   True\n",
      "│    │    └─SiLU (2)                                              [1, 48, 64, 64]      [1, 48, 64, 64]      --                   --\n",
      "├─ModuleList (fins)                                               --                   --                   --                   True\n",
      "│    └─Sequential (0)                                             [1, 48, 64, 64]      [1, 112, 8, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                        [1, 48, 64, 64]      [1, 24, 64, 64]      4,056                True\n",
      "│    │    └─Sequential (1)                                        [1, 24, 64, 64]      [1, 32, 32, 32]      65,488               True\n",
      "│    │    └─Sequential (2)                                        [1, 32, 32, 32]      [1, 56, 16, 16]      196,336              True\n",
      "│    │    └─Sequential (3)                                        [1, 56, 16, 16]      [1, 112, 8, 8]       1,056,048            True\n",
      "│    └─Sequential (1)                                             [1, 48, 64, 64]      [1, 112, 8, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                        [1, 48, 64, 64]      [1, 24, 64, 64]      4,056                True\n",
      "│    │    └─Sequential (1)                                        [1, 24, 64, 64]      [1, 32, 32, 32]      65,488               True\n",
      "│    │    └─Sequential (2)                                        [1, 32, 32, 32]      [1, 56, 16, 16]      196,336              True\n",
      "│    │    └─Sequential (3)                                        [1, 56, 16, 16]      [1, 112, 8, 8]       1,056,048            True\n",
      "│    └─Sequential (2)                                             [1, 48, 64, 64]      [1, 112, 8, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                        [1, 48, 64, 64]      [1, 24, 64, 64]      4,056                True\n",
      "│    │    └─Sequential (1)                                        [1, 24, 64, 64]      [1, 32, 32, 32]      65,488               True\n",
      "│    │    └─Sequential (2)                                        [1, 32, 32, 32]      [1, 56, 16, 16]      196,336              True\n",
      "│    │    └─Sequential (3)                                        [1, 56, 16, 16]      [1, 112, 8, 8]       1,056,048            True\n",
      "│    └─Sequential (3)                                             [1, 48, 64, 64]      [1, 112, 8, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                        [1, 48, 64, 64]      [1, 24, 64, 64]      4,056                True\n",
      "│    │    └─Sequential (1)                                        [1, 24, 64, 64]      [1, 32, 32, 32]      65,488               True\n",
      "│    │    └─Sequential (2)                                        [1, 32, 32, 32]      [1, 56, 16, 16]      196,336              True\n",
      "│    │    └─Sequential (3)                                        [1, 56, 16, 16]      [1, 112, 8, 8]       1,056,048            True\n",
      "├─Sequential (extension)                                          [1, 112, 8, 8]       [1, 448, 4, 4]       --                   True\n",
      "│    └─Sequential (0)                                             [1, 112, 8, 8]       [1, 160, 8, 8]       --                   True\n",
      "│    │    └─MBConv (0)                                            [1, 112, 8, 8]       [1, 160, 8, 8]       240,224              True\n",
      "│    │    └─MBConv (1)                                            [1, 160, 8, 8]       [1, 160, 8, 8]       412,160              True\n",
      "│    │    └─MBConv (2)                                            [1, 160, 8, 8]       [1, 160, 8, 8]       412,160              True\n",
      "│    │    └─MBConv (3)                                            [1, 160, 8, 8]       [1, 160, 8, 8]       412,160              True\n",
      "│    │    └─MBConv (4)                                            [1, 160, 8, 8]       [1, 160, 8, 8]       412,160              True\n",
      "│    │    └─MBConv (5)                                            [1, 160, 8, 8]       [1, 160, 8, 8]       412,160              True\n",
      "│    └─Sequential (1)                                             [1, 160, 8, 8]       [1, 272, 4, 4]       --                   True\n",
      "│    │    └─MBConv (0)                                            [1, 160, 8, 8]       [1, 272, 4, 4]       519,904              True\n",
      "│    │    └─MBConv (1)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    │    └─MBConv (2)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    │    └─MBConv (3)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    │    └─MBConv (4)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    │    └─MBConv (5)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    │    └─MBConv (6)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    │    └─MBConv (7)                                            [1, 272, 4, 4]       [1, 272, 4, 4]       1,157,632            True\n",
      "│    └─Sequential (2)                                             [1, 272, 4, 4]       [1, 448, 4, 4]       --                   True\n",
      "│    │    └─MBConv (0)                                            [1, 272, 4, 4]       [1, 448, 4, 4]       1,419,104            True\n",
      "│    │    └─MBConv (1)                                            [1, 448, 4, 4]       [1, 448, 4, 4]       3,046,400            True\n",
      "├─AdaptiveAvgPool2d (pool)                                        [1, 448, 4, 4]       [1, 448, 1, 1]       --                   --\n",
      "├─Linear (classifier)                                             [1, 448]             [1, 6]               2,694                True\n",
      "=================================================================================================================================================\n",
      "Total params: 20,687,558\n",
      "Trainable params: 20,687,558\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.03\n",
      "=================================================================================================================================================\n",
      "Input size (MB): 1.05\n",
      "Forward/backward pass size (MB): 287.34\n",
      "Params size (MB): 82.75\n",
      "Estimated Total Size (MB): 371.14\n",
      "=================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class FinNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels, config,\n",
    "        activation_layer=nn.SiLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"ReflectionNet\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottle_factor (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            phi (float, optional): The compound scaling coefficient. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels // config.fins\n",
    "        self.config = config\n",
    "        \n",
    "        in_mb_channels = config.adjust_channels(32)\n",
    "        out_mb_channels = config.adjust_channels(320)\n",
    "\n",
    "        self.fins, self.extension = config.make_blocks()\n",
    "        self.fin_init_convs = nn.ModuleList(map(\n",
    "            lambda _: config.conv_block(\n",
    "                self.in_channels, in_mb_channels,\n",
    "                kernel_size=3, padding=1, stride=2,\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            range(config.fins)\n",
    "        ))\n",
    "\n",
    "        self.pool = config.pool_block(1)\n",
    "        self.classifier = nn.Linear(out_mb_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1. Split x into fins\n",
    "        xs = torch.split(x, self.in_channels, dim=1)\n",
    "\n",
    "        # 2. Apply top-level conv\n",
    "        xs = list(map(\n",
    "            lambda item: item[1](item[0]),\n",
    "            zip(xs, self.fin_init_convs)\n",
    "        ))\n",
    "\n",
    "        # 3. Run fin modules\n",
    "        xs = list(map(\n",
    "            lambda item: item[1](item[0]), \n",
    "            zip(xs, self.fins)\n",
    "        ))\n",
    "\n",
    "        # 4. Combine fin outputs\n",
    "        x = torch.stack(xs, dim=1)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # 5. Apply extension\n",
    "        x = self.extension(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 6. Classification head\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x, xs if self.config.fin_output else x\n",
    "\n",
    "# in_channels, out_channels = 4, 6\n",
    "in_channels, out_channels = 16, 6\n",
    "fin_config = FinNetConfig()\n",
    "model = FinNet(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    fin_config,\n",
    ")\n",
    "\n",
    "x = torch.randn(1, in_channels, 128, 128)\n",
    "\n",
    "print(summary(\n",
    "    model=model, \n",
    "    input_data=x,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Props(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Props, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "    \n",
    "    def __getattribute__(self, name):\n",
    "        try:\n",
    "            return super(Props, self).__getattribute__(name)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "model_config = Props(x=2)\n",
    "model_config.x, model_config.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HBAFinNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        ref_config = FinNetConfig(\n",
    "            se_block=SpatialChannelSeBlock,\n",
    "\n",
    "            # B0\n",
    "            width_mult=1., depth_mult=1.,\n",
    "\n",
    "            # Fin configuration\n",
    "            fin_output=config.fin_output,\n",
    "        )\n",
    "        self.finnet = FinNet(\n",
    "            config.in_channels,\n",
    "            config.out_channels,\n",
    "            ref_config,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.finnet(x)\n",
    "\n",
    "config = Props(\n",
    "    in_channels=16,\n",
    "    out_channels=6,\n",
    "    fin_output=True,\n",
    ")\n",
    "\n",
    "model = HBAFinNet(config)\n",
    "\n",
    "x = torch.randn(1, config.in_channels, 256, 256)\n",
    "# print(summary(\n",
    "#     model=model, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"],\n",
    "# ))\n",
    "output = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
