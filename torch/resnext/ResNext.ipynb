{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, stride, layer_fns, name):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Initialize attributes\n",
    "        self.block = block\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.layer_fns = layer_fns\n",
    "\n",
    "        # 2. Initialize a downsample layer\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\n",
    "                    name + '_residual_conv',\n",
    "                    self.layer_fns['Conv'](\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                    )\n",
    "                ),\n",
    "                (\n",
    "                    name + '_residual_bn',\n",
    "                    self.layer_fns['BatchNorm'](out_channels)\n",
    "                )\n",
    "            ]))\n",
    "        \n",
    "        # 3. Initialize the ReLU layer\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        block_x = self.block(x)\n",
    "\n",
    "        # 1. Downsample if block_x and x are not the same size\n",
    "        if hasattr(self, 'downsample'):\n",
    "            x = self.downsample(x)\n",
    "        \n",
    "        # 2. Add residual connection\n",
    "        x = x + block_x\n",
    "\n",
    "        # 3. Apply ReLU\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "ResNext (ResNext)                                            [2, 1, 5, 224, 224]  [2, 1]               --                   True\n",
       "├─Conv3dNormActivation (block_0)                             [2, 1, 5, 224, 224]  [2, 64, 3, 112, 112] --                   True\n",
       "│    └─Conv3d (0)                                            [2, 1, 5, 224, 224]  [2, 64, 3, 112, 112] 21,952               True\n",
       "│    └─BatchNorm3d (1)                                       [2, 64, 3, 112, 112] [2, 64, 3, 112, 112] 128                  True\n",
       "│    └─ReLU (2)                                              [2, 64, 3, 112, 112] [2, 64, 3, 112, 112] --                   --\n",
       "├─MaxPool3d (pool_0)                                         [2, 64, 3, 112, 112] [2, 64, 2, 56, 56]   --                   --\n",
       "├─Sequential (block_1)                                       [2, 64, 2, 56, 56]   [2, 256, 2, 56, 56]  --                   True\n",
       "│    └─ResidualAdd (block_1_1)                               [2, 64, 2, 56, 56]   [2, 256, 2, 56, 56]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 64, 2, 56, 56]   [2, 256, 2, 56, 56]  55,808               True\n",
       "│    │    └─Sequential (downsample)                          [2, 64, 2, 56, 56]   [2, 256, 2, 56, 56]  17,152               True\n",
       "│    │    └─ReLU (relu)                                      [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  --                   --\n",
       "│    └─ResidualAdd (block_1_2)                               [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  80,384               True\n",
       "│    │    └─ReLU (relu)                                      [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  --                   --\n",
       "│    └─ResidualAdd (block_1_3)                               [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  80,384               True\n",
       "│    │    └─ReLU (relu)                                      [2, 256, 2, 56, 56]  [2, 256, 2, 56, 56]  --                   --\n",
       "├─Sequential (block_2)                                       [2, 256, 2, 56, 56]  [2, 512, 1, 28, 28]  --                   True\n",
       "│    └─ResidualAdd (block_2_1)                               [2, 256, 2, 56, 56]  [2, 512, 1, 28, 28]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 256, 2, 56, 56]  [2, 512, 1, 28, 28]  253,952              True\n",
       "│    │    └─Sequential (downsample)                          [2, 256, 2, 56, 56]  [2, 512, 1, 28, 28]  132,608              True\n",
       "│    │    └─ReLU (relu)                                      [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   --\n",
       "│    └─ResidualAdd (block_2_2)                               [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  319,488              True\n",
       "│    │    └─ReLU (relu)                                      [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   --\n",
       "│    └─ResidualAdd (block_2_3)                               [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  319,488              True\n",
       "│    │    └─ReLU (relu)                                      [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   --\n",
       "│    └─ResidualAdd (block_2_4)                               [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   True\n",
       "│    │    └─Sequential (block)                               [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  319,488              True\n",
       "│    │    └─ReLU (relu)                                      [2, 512, 1, 28, 28]  [2, 512, 1, 28, 28]  --                   --\n",
       "├─Sequential (block_3)                                       [2, 512, 1, 28, 28]  [2, 1024, 1, 14, 14] --                   True\n",
       "│    └─ResidualAdd (block_3_1)                               [2, 512, 1, 28, 28]  [2, 1024, 1, 14, 14] --                   True\n",
       "│    │    └─Sequential (block)                               [2, 512, 1, 28, 28]  [2, 1024, 1, 14, 14] 1,011,712            True\n",
       "│    │    └─Sequential (downsample)                          [2, 512, 1, 28, 28]  [2, 1024, 1, 14, 14] 527,360              True\n",
       "│    │    └─ReLU (relu)                                      [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   --\n",
       "│    └─ResidualAdd (block_3_2)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   True\n",
       "│    │    └─Sequential (block)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] 1,273,856            True\n",
       "│    │    └─ReLU (relu)                                      [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   --\n",
       "│    └─ResidualAdd (block_3_3)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   True\n",
       "│    │    └─Sequential (block)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] 1,273,856            True\n",
       "│    │    └─ReLU (relu)                                      [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   --\n",
       "│    └─ResidualAdd (block_3_4)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   True\n",
       "│    │    └─Sequential (block)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] 1,273,856            True\n",
       "│    │    └─ReLU (relu)                                      [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   --\n",
       "│    └─ResidualAdd (block_3_5)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   True\n",
       "│    │    └─Sequential (block)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] 1,273,856            True\n",
       "│    │    └─ReLU (relu)                                      [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   --\n",
       "│    └─ResidualAdd (block_3_6)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   True\n",
       "│    │    └─Sequential (block)                               [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] 1,273,856            True\n",
       "│    │    └─ReLU (relu)                                      [2, 1024, 1, 14, 14] [2, 1024, 1, 14, 14] --                   --\n",
       "├─Sequential (block_4)                                       [2, 1024, 1, 14, 14] [2, 2048, 1, 7, 7]   --                   True\n",
       "│    └─ResidualAdd (block_4_1)                               [2, 1024, 1, 14, 14] [2, 2048, 1, 7, 7]   --                   True\n",
       "│    │    └─Sequential (block)                               [2, 1024, 1, 14, 14] [2, 2048, 1, 7, 7]   4,038,656            True\n",
       "│    │    └─Sequential (downsample)                          [2, 1024, 1, 14, 14] [2, 2048, 1, 7, 7]   2,103,296            True\n",
       "│    │    └─ReLU (relu)                                      [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   --                   --\n",
       "│    └─ResidualAdd (block_4_2)                               [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   --                   True\n",
       "│    │    └─Sequential (block)                               [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   5,087,232            True\n",
       "│    │    └─ReLU (relu)                                      [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   --                   --\n",
       "│    └─ResidualAdd (block_4_3)                               [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   --                   True\n",
       "│    │    └─Sequential (block)                               [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   5,087,232            True\n",
       "│    │    └─ReLU (relu)                                      [2, 2048, 1, 7, 7]   [2, 2048, 1, 7, 7]   --                   --\n",
       "├─AdaptiveAvgPool2d (global_pool)                            [2, 2048, 1, 7, 7]   [2, 2048, 1, 1, 1]   --                   --\n",
       "├─Linear (output)                                            [2, 2048]            [2, 1]               2,049                True\n",
       "============================================================================================================================================\n",
       "Total params: 25,827,649\n",
       "Trainable params: 25,827,649\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.66\n",
       "============================================================================================================================================\n",
       "Input size (MB): 2.01\n",
       "Forward/backward pass size (MB): 717.72\n",
       "Params size (MB): 103.31\n",
       "Estimated Total Size (MB): 823.04\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torchinfo import summary\n",
    "from torchvision import ops as vision_ops\n",
    "\n",
    "class ResNext(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        # A tuple of (features, repeats, stride)\n",
    "        # blocks=[(128, 1, 1), (256, 1, 2), (512, 1, 2), (1024, 1, 2)],\n",
    "        blocks=[(128, 3, 1), (256, 4, 2), (512, 6, 2), (1024, 3, 2)],\n",
    "        layer_fns=dict(\n",
    "            ConvNormActivation=vision_ops.Conv2dNormActivation,\n",
    "            Conv=nn.Conv2d,\n",
    "            MaxPool=nn.MaxPool2d,\n",
    "            AvgPool=nn.AdaptiveAvgPool2d,\n",
    "            BatchNorm=nn.BatchNorm2d,\n",
    "            ReLU=nn.ReLU,\n",
    "        )\n",
    "    ):\n",
    "        super(ResNext, self).__init__()\n",
    "\n",
    "        # 1. Initialize block configurations\n",
    "        self.blocks = blocks\n",
    "        self.layer_fns = layer_fns\n",
    "        \n",
    "        # 2. Get backbone's input and output sizes\n",
    "        block_0_output, last_block_features = 64, blocks[-1][0]\n",
    "\n",
    "        # 3. Initialize the first convolution block\n",
    "        self.block_0 = self.layer_fns['ConvNormActivation'](\n",
    "            in_channels=in_channels,\n",
    "            out_channels=block_0_output,\n",
    "            kernel_size=7,\n",
    "            padding=3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # 4. Initialize the pooling layer after the first convolution block\n",
    "        self.pool_0 = self.layer_fns['MaxPool'](kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # 5. Iterate over and create blocks\n",
    "        block_input = block_0_output\n",
    "        for block_id, (features, repeats, stride) in enumerate(blocks, start=1):\n",
    "            name = f'block_{block_id}'\n",
    "            setattr(\n",
    "                self, name,\n",
    "                self._repeated_block(\n",
    "                    block_input,\n",
    "                    features,\n",
    "                    repeats,\n",
    "                    stride,\n",
    "                    name=name,\n",
    "                )\n",
    "            )\n",
    "            block_input = 2*features\n",
    "\n",
    "        # 6. Global average pooling to merge the spatial dimensions\n",
    "        self.global_pool = self.layer_fns['AvgPool'](1)\n",
    "\n",
    "        # 7. Output layer\n",
    "        self.output = nn.Linear(\n",
    "            in_features=last_block_features*2,\n",
    "            out_features=out_channels,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 1. Apply first block\n",
    "        x = self.block_0(x)\n",
    "        x = self.pool_0(x)\n",
    "\n",
    "        # 2. Apply rest of the blocks\n",
    "        for block_id, _ in enumerate(self.blocks, start=1):\n",
    "            x = getattr(self, f'block_{block_id}')(x)\n",
    "\n",
    "        # 3. Apply the global average pooling layer\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        # 4. Output\n",
    "        output = self.output(x.flatten(1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _repeated_block(self, in_channels, features, repeats, stride, name, start=1, bias=False):\n",
    "        # 1. Create all but last repeated blocks\n",
    "        blocks = list(map(\n",
    "            lambda repeat_id: (\n",
    "                f'{name}_{repeat_id}',\n",
    "                self._block(\n",
    "                    in_channels if repeat_id == 1 else 2*features,\n",
    "                    features,\n",
    "                    stride=stride if repeat_id == 1 else 1,\n",
    "                    name=f'{name}_{repeat_id}'\n",
    "                )\n",
    "            ),\n",
    "            range(1, repeats + 1)\n",
    "        ))\n",
    "\n",
    "        return nn.Sequential(OrderedDict(blocks))\n",
    "\n",
    "\n",
    "    def _block(self, in_channels, features, stride, name, bias=False):\n",
    "        return ResidualAdd(\n",
    "            nn.Sequential(OrderedDict([\n",
    "                (\n",
    "                    name + \"_conv1\",\n",
    "                    self.layer_fns['ConvNormActivation'](\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=features,\n",
    "                        kernel_size=1,\n",
    "                        bias=bias,\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    name + \"_conv2\",\n",
    "                    self.layer_fns['ConvNormActivation'](\n",
    "                        in_channels=features,\n",
    "                        out_channels=features,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                        stride=stride,\n",
    "                        groups=32,\n",
    "                        bias=bias,\n",
    "                    ),\n",
    "                ),\n",
    "                # We use Conv + BatchNorm here. Activation is applied in the ResidualAdd module.\n",
    "                (\n",
    "                    name + \"_conv3\",\n",
    "                    self.layer_fns['Conv'](\n",
    "                        in_channels=features,\n",
    "                        out_channels=features*2,\n",
    "                        kernel_size=1,\n",
    "                        bias=bias,\n",
    "                    ),\n",
    "                ),\n",
    "                (\n",
    "                    name + \"_bn\",\n",
    "                    self.layer_fns['BatchNorm'](num_features=features*2),\n",
    "                ),\n",
    "            ])),\n",
    "            in_channels=in_channels,\n",
    "            out_channels=features*2,\n",
    "            stride=stride,\n",
    "            layer_fns=self.layer_fns,\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "model = ResNext(\n",
    "    in_channels=1,\n",
    "    layer_fns=dict(\n",
    "        ConvNormActivation=vision_ops.Conv3dNormActivation,\n",
    "        Conv=nn.Conv3d,\n",
    "        MaxPool=nn.MaxPool3d,\n",
    "        AvgPool=nn.AdaptiveAvgPool3d,\n",
    "        BatchNorm=nn.BatchNorm3d,\n",
    "        ReLU=nn.ReLU,\n",
    "    )\n",
    ")\n",
    "\n",
    "summary(\n",
    "    model=model, \n",
    "    input_size=(2, 1, 5, 224, 224),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
