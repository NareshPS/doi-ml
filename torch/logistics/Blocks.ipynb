{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b387d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ConvNormActivation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        conv_layer=nn.Conv1d,\n",
    "        norm_layer=nn.BatchNorm1d,\n",
    "        activation_layer=nn.ReLU,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input args\n",
    "        self.name = name\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv_layer = conv_layer\n",
    "        self.norm_layer = norm_layer\n",
    "        self.activation_layer = activation_layer\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "        # Layers\n",
    "        self.block = self.make_block()\n",
    "\n",
    "    def make_block(self):\n",
    "        items = [\n",
    "            (\n",
    "                self.name + '_conv',\n",
    "                self.conv_layer(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if self.norm_layer is not None:\n",
    "            items.append(\n",
    "                (self.name + '_bn', self.norm_layer(self.out_channels))\n",
    "            )\n",
    "\n",
    "        if self.activation_layer is not None:\n",
    "            items.append(\n",
    "                (self.name + '_relu', self.activation_layer(inplace=True))\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(OrderedDict(items))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ConvNormActivationPoolDropout(nn.Module):\n",
    "    def __init__(self, name, in_channels, out_channels, kernel_size, dropout, gp=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input args\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Layers\n",
    "        self.block = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                (\n",
    "                    name + '_conv',\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                    )\n",
    "                ),\n",
    "                (name + '_relu', nn.ReLU(inplace=True)),\n",
    "                (name + '_bn', nn.BatchNorm1d(out_channels)),\n",
    "                (name + '_mp', nn.MaxPool1d(2) if not gp else nn.AdaptiveMaxPool1d(1)),\n",
    "                (name + '_dropout', nn.Dropout(dropout)),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
