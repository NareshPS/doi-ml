{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. BottleNeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        activation_layer=nn.ReLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Bottleneck block with an option to behave as a linear bottle-neck block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            act (nn.Module, optional): It activation layer applied to the block output.\n",
    "                If it is set to the identity layer, it becomes a linear bottleneck block.\n",
    "                Defaults to nn.ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        self.block = ResidualAdd(\n",
    "            nn.Sequential(\n",
    "                # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                ops.Conv2dNormActivation(\n",
    "                    in_channels, bottleneck_size,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    **kwargs\n",
    "                ),\n",
    "\n",
    "                # (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "                ops.Conv2dNormActivation(\n",
    "                    bottleneck_size, bottleneck_size,\n",
    "                    kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                    **kwargs\n",
    "                ),\n",
    "\n",
    "                # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                ops.Conv2dNormActivation(\n",
    "                    bottleneck_size, out_channels,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=nn.Identity,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            ),\n",
    "            # (..., out_channels, ...) -> (..., in_channels, ...)\n",
    "            shortcut=ConvBnReLU2d(\n",
    "                in_channels, out_channels,\n",
    "                kernel_size=1, stride=stride,\n",
    "                **kwargs\n",
    "            ) if in_channels != out_channels else None\n",
    "        )\n",
    "        self.act = activation_layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.block(x))\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = BottleNeck(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size, stride=2, padding=1,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. MobileNet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class MobileNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        activation_layer=nn.Identity,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"MobileNetBlock is a bottleneck block which applies residual connections\n",
    "        if in_features == out_features. It uses a bottle_factor of 4.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            activation_layer (nn.Module, optional): It activation layer applied to the block output.\n",
    "                If it is set to the identity layer, it becomes a linear bottleneck block.\n",
    "                Defaults to nn.ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        residualOrNot = ResidualAdd if in_channels == out_channels else nn.Sequential\n",
    "        self.block = (\n",
    "            residualOrNot(\n",
    "                nn.Sequential(\n",
    "                    # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        in_channels, bottleneck_size,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, bottleneck_size,\n",
    "                        kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, out_channels,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.Identity,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        self.act = activation_layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.block(x))\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = MobileNetBlock(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. MBConv\n",
    "\n",
    "MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"MBConv is a MobileNetBlock with Depthwise Convolution.\n",
    "        It replaces ReLU with ReLU6\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        residualOrNot = ResidualAdd if in_channels == out_channels else nn.Sequential\n",
    "        self.block = (\n",
    "            residualOrNot(\n",
    "                nn.Sequential(\n",
    "                    # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        in_channels, bottleneck_size,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.ReLU6,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, bottleneck_size,\n",
    "                        kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                        groups=bottleneck_size, # Depthwise Convolution\n",
    "                        activation_layer=nn.ReLU6,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, out_channels,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.Identity,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = MBConv(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. FusedMBConv\n",
    "\n",
    "EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"FusedMBConv is a MBConv block with fused 1st and 2nd convs.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        residualOrNot = ResidualAdd if in_channels == out_channels else nn.Sequential\n",
    "        self.block = (\n",
    "            residualOrNot(\n",
    "                nn.Sequential(\n",
    "                    # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        in_channels, bottleneck_size,\n",
    "                        kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                        activation_layer=nn.ReLU6,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, out_channels,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.Identity,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = FusedMBConv(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. SEBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, activation_layer=nn.ReLU):\n",
    "        \"\"\"SEBlock is a FusedMBConv block with Squeeze-And-Excitation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, squeeze_channels, 1)\n",
    "        self.activation = activation_layer()\n",
    "        self.conv2 = nn.Conv2d(squeeze_channels, in_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "\n",
    "        # 1. (...) -> (..., in_channels, 1, 1)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 2. -> (..., squeeze_channels, 1, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # 3. -> (..., in_channels, 1, 1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 4. Scale\n",
    "        x = self.sigmoid(x)\n",
    "        x = inp * x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10, 10)\n",
    "# l = SEBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. EfficientNetBlock\n",
    "\n",
    "EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same', squeeze_ratio=4,\n",
    "        activation_layer=nn.SiLU, se_block=SEBlock,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"EfficientNetBlock is a MBConv block with SEBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        modules = nn.ModuleList()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        # 1. (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "        if in_channels != bottleneck_size:\n",
    "            modules.append(\n",
    "                ops.Conv2dNormActivation(\n",
    "                    in_channels, bottleneck_size,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=activation_layer,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # 2. (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "        modules.append(\n",
    "            ops.Conv2dNormActivation(\n",
    "                bottleneck_size, bottleneck_size,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                groups=bottleneck_size, # Depthwise Convolution\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 3. Squeeze and excitation block\n",
    "        squeeze_channels = max(1, in_channels // squeeze_ratio)\n",
    "        modules.append(\n",
    "            se_block(bottleneck_size, squeeze_channels, activation_layer=activation_layer)\n",
    "        )\n",
    "\n",
    "        # 4. (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "        modules.append(\n",
    "            ops.Conv2dNormActivation(\n",
    "                bottleneck_size, out_channels,\n",
    "                kernel_size=1, stride=1, padding='same',\n",
    "                activation_layer=nn.Identity,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.block = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = self.block(x)\n",
    "\n",
    "        if self.residual:\n",
    "            x = x + inp\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels, out_channels = 32, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = EfficientNetBlock(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size, bottleneck=1,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. HBASeBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class HBASeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, activation_layer=nn.ReLU):\n",
    "        \"\"\"SEBlock is a FusedMBConv block with Squeeze-And-Excitation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, squeeze_channels, 1)\n",
    "        self.activation = activation_layer()\n",
    "        self.conv2 = nn.Conv2d(squeeze_channels, in_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "\n",
    "        # 1. (...) -> (..., in_channels, :, 1)\n",
    "        x = x.mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # 2. -> (..., squeeze_channels, 1, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # 3. -> (..., in_channels, 1, 1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 4. Scale\n",
    "        x = self.sigmoid(x)\n",
    "        x = inp * x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10, 10)\n",
    "# l = HBASeBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n",
    "\n",
    "* ReLU6 and SiLU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. EfficientNetConfig\n",
    "\n",
    "#### TODO\n",
    "- [x] Pipe activation_layer all the way through.\n",
    "- [x] Residual connection in SEBlock is broken for stride != 1.\n",
    "- [x] Round channels to multiple of 8.\n",
    "- [-] Pipe dropout.\n",
    "- [-] Configure the minimum number of channels in EfficientNetConfig.\n",
    "- [x] Adjust variable blocks to have same number of input and output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
      "=============================================================================================================================\n",
      "Sequential (Sequential)                       [1, 48, 224, 224]    [1, 448, 14, 14]     --                   True\n",
      "├─Sequential (0)                              [1, 48, 224, 224]    [1, 24, 224, 224]    --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 48, 224, 224]    [1, 24, 224, 224]    --                   True\n",
      "│    │    └─Sequential (block)                [1, 48, 224, 224]    [1, 24, 224, 224]    2,940                True\n",
      "│    └─EfficientNetBlock (1)                  [1, 24, 224, 224]    [1, 24, 224, 224]    --                   True\n",
      "│    │    └─Sequential (block)                [1, 24, 224, 224]    [1, 24, 224, 224]    1,206                True\n",
      "├─Sequential (1)                              [1, 24, 224, 224]    [1, 32, 112, 112]    --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 24, 224, 224]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                [1, 24, 224, 224]    [1, 32, 112, 112]    11,878               True\n",
      "│    └─EfficientNetBlock (1)                  [1, 32, 112, 112]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                [1, 32, 112, 112]    [1, 32, 112, 112]    18,120               True\n",
      "│    └─EfficientNetBlock (2)                  [1, 32, 112, 112]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                [1, 32, 112, 112]    [1, 32, 112, 112]    18,120               True\n",
      "│    └─EfficientNetBlock (3)                  [1, 32, 112, 112]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                [1, 32, 112, 112]    [1, 32, 112, 112]    18,120               True\n",
      "├─Sequential (2)                              [1, 32, 112, 112]    [1, 56, 56, 56]      --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 32, 112, 112]    [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                [1, 32, 112, 112]    [1, 56, 56, 56]      25,848               True\n",
      "│    └─EfficientNetBlock (1)                  [1, 56, 56, 56]      [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                [1, 56, 56, 56]      [1, 56, 56, 56]      57,246               True\n",
      "│    └─EfficientNetBlock (2)                  [1, 56, 56, 56]      [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                [1, 56, 56, 56]      [1, 56, 56, 56]      57,246               True\n",
      "│    └─EfficientNetBlock (3)                  [1, 56, 56, 56]      [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                [1, 56, 56, 56]      [1, 56, 56, 56]      57,246               True\n",
      "├─Sequential (3)                              [1, 56, 56, 56]      [1, 112, 28, 28]     --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 56, 56, 56]      [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 56, 56, 56]      [1, 112, 28, 28]     70,798               True\n",
      "│    └─EfficientNetBlock (1)                  [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (2)                  [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (3)                  [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (4)                  [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (5)                  [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "├─Sequential (4)                              [1, 112, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 112, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 112, 28, 28]     [1, 160, 28, 28]     240,924              True\n",
      "│    └─EfficientNetBlock (1)                  [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (2)                  [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (3)                  [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (4)                  [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (5)                  [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "├─Sequential (5)                              [1, 160, 28, 28]     [1, 272, 14, 14]     --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 160, 28, 28]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 160, 28, 28]     [1, 272, 14, 14]     520,904              True\n",
      "│    └─EfficientNetBlock (1)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (2)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (3)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (4)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (5)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (6)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (7)                  [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "├─Sequential (6)                              [1, 272, 14, 14]     [1, 448, 14, 14]     --                   True\n",
      "│    └─EfficientNetBlock (0)                  [1, 272, 14, 14]     [1, 448, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 272, 14, 14]     [1, 448, 14, 14]     1,420,804            True\n",
      "│    └─EfficientNetBlock (1)                  [1, 448, 14, 14]     [1, 448, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                [1, 448, 14, 14]     [1, 448, 14, 14]     3,049,200            True\n",
      "=============================================================================================================================\n",
      "Total params: 16,740,824\n",
      "Trainable params: 16,740,824\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 5.83\n",
      "=============================================================================================================================\n",
      "Input size (MB): 9.63\n",
      "Forward/backward pass size (MB): 1050.35\n",
      "Params size (MB): 66.96\n",
      "Estimated Total Size (MB): 1126.94\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNetConfig(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # B4 configuration\n",
    "        width_mult=1.4, depth_mult=1.8, dropout=0.4, last_channels=1280,\n",
    "\n",
    "        # Block configuration\n",
    "        kernel_1=3, kernel_2=5,\n",
    "\n",
    "        # SEBlock\n",
    "        se_block=SEBlock,\n",
    "    ):\n",
    "        self.width_mult = width_mult\n",
    "        self.depth_mult =depth_mult\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.last_channels = last_channels\n",
    "\n",
    "        # Conv type 1\n",
    "        self.kernel_1 = kernel_1\n",
    "\n",
    "        # Conv type 2\n",
    "        self.kernel_2 = kernel_2\n",
    "\n",
    "        # SE block\n",
    "        self.se_block = se_block\n",
    "\n",
    "        # MBConv configs\n",
    "        self.block_configs = [\n",
    "            # (in_channels, out_channels, bottleneck, kernel, padding, stride, layers)\n",
    "            (32, 16, 1, kernel_1, 'same', 1, 1),\n",
    "            (16, 24, 6, 3, 1, 2, 2),\n",
    "            (24, 40, 6, 5, 2, 2, 2),\n",
    "            (40, 80, 6, 3, 1, 2, 3),\n",
    "            (80, 112, 6, kernel_2, 'same', 1, 3),\n",
    "            (112, 192, 6, 5, 2, 2, 4),\n",
    "            (192, 320, 6, kernel_1, 'same', 1, 1),\n",
    "        ]\n",
    "\n",
    "    def adjust_channels(self, channels):\n",
    "        return self.round_to(channels*self.width_mult)\n",
    "    \n",
    "    def adjust_depth(self, num_layers):\n",
    "        return int(math.ceil(num_layers*self.depth_mult))\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_to(v, multiple=8):\n",
    "        return int(multiple * round(v / multiple))\n",
    "    \n",
    "    def _block(self, args, **kwargs):\n",
    "        in_channels, out_channels, bottleneck, kernel, padding, stride, layers = args\n",
    "\n",
    "        # 1. Update in_channels and out_channels based on the width_mult\n",
    "        in_channels = self.adjust_channels(in_channels)\n",
    "        out_channels = self.adjust_channels(out_channels)\n",
    "\n",
    "        # 2. Update layers based on depth_mult\n",
    "        layers = self.adjust_depth(layers)\n",
    "        \n",
    "        block = nn.Sequential(\n",
    "            EfficientNetBlock(\n",
    "                in_channels, out_channels,\n",
    "                bottleneck=bottleneck,\n",
    "                kernel_size=kernel, stride=stride, padding=padding,\n",
    "                se_block=self.se_block,\n",
    "                **kwargs\n",
    "            ),\n",
    "            *map(\n",
    "                lambda _: EfficientNetBlock(\n",
    "                    out_channels, out_channels,\n",
    "                    bottleneck=bottleneck,\n",
    "                    kernel_size=kernel, stride=1, padding='same',\n",
    "                    se_block=self.se_block,\n",
    "                    **kwargs\n",
    "                ),\n",
    "                range(layers - 1)\n",
    "            )\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def make_blocks(self, **kwargs):\n",
    "        modules = nn.Sequential(\n",
    "            *map(\n",
    "                lambda b_config: self._block(b_config,  **kwargs),\n",
    "                self.block_configs\n",
    "            )\n",
    "        )\n",
    "        return modules\n",
    "\n",
    "eff_config = EfficientNetConfig(\n",
    "    se_block=HBASeBlock\n",
    ")\n",
    "block_config = eff_config.block_configs[0]\n",
    "block = eff_config.make_blocks()\n",
    "\n",
    "x = torch.randn(1, 48, 224, 224)\n",
    "print(summary(\n",
    "    model=block, \n",
    "    input_data=x,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
      "============================================================================================================================================\n",
      "EfficientNet (EfficientNet)                                  [1, 3, 128, 256]     [1, 6]               --                   True\n",
      "├─Sequential (model)                                         [1, 3, 128, 256]     [1, 1280, 1, 1]      --                   True\n",
      "│    └─Conv2dNormActivation (0)                              [1, 3, 128, 256]     [1, 48, 64, 128]     --                   True\n",
      "│    │    └─Conv2d (0)                                       [1, 3, 128, 256]     [1, 48, 64, 128]     1,296                True\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 48, 64, 128]     [1, 48, 64, 128]     96                   True\n",
      "│    │    └─SiLU (2)                                         [1, 48, 64, 128]     [1, 48, 64, 128]     --                   --\n",
      "│    └─Sequential (1)                                        [1, 48, 64, 128]     [1, 448, 4, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                   [1, 48, 64, 128]     [1, 24, 64, 128]     --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 48, 64, 128]     [1, 24, 64, 128]     --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 48, 64, 128]     [1, 24, 64, 128]     --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 48, 64, 128]     [1, 48, 64, 128]     528                  True\n",
      "│    │    │    │    │    └─SEBlock (1)                       [1, 48, 64, 128]     [1, 48, 64, 128]     1,212                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (2)          [1, 48, 64, 128]     [1, 24, 64, 128]     1,200                True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 24, 64, 128]     [1, 24, 64, 128]     --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 24, 64, 128]     [1, 24, 64, 128]     --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 24, 64, 128]     [1, 24, 64, 128]     264                  True\n",
      "│    │    │    │    │    └─SEBlock (1)                       [1, 24, 64, 128]     [1, 24, 64, 128]     318                  True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (2)          [1, 24, 64, 128]     [1, 24, 64, 128]     624                  True\n",
      "│    │    └─Sequential (1)                                   [1, 24, 64, 128]     [1, 32, 32, 64]      --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 24, 64, 128]     [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 24, 64, 128]     [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 24, 64, 128]     [1, 144, 64, 128]    3,744                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 144, 64, 128]    [1, 144, 32, 64]     1,584                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 144, 32, 64]     [1, 144, 32, 64]     1,878                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 144, 32, 64]     [1, 32, 32, 64]      4,672                True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 32, 32, 64]      [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 32, 32, 64]      [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 32, 32, 64]      [1, 192, 32, 64]     6,528                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 192, 32, 64]     [1, 192, 32, 64]     2,112                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 192, 32, 64]     [1, 192, 32, 64]     3,272                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 192, 32, 64]     [1, 32, 32, 64]      6,208                True\n",
      "│    │    │    └─EfficientNetBlock (2)                       [1, 32, 32, 64]      [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 32, 32, 64]      [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 32, 32, 64]      [1, 192, 32, 64]     6,528                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 192, 32, 64]     [1, 192, 32, 64]     2,112                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 192, 32, 64]     [1, 192, 32, 64]     3,272                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 192, 32, 64]     [1, 32, 32, 64]      6,208                True\n",
      "│    │    │    └─EfficientNetBlock (3)                       [1, 32, 32, 64]      [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 32, 32, 64]      [1, 32, 32, 64]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 32, 32, 64]      [1, 192, 32, 64]     6,528                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 192, 32, 64]     [1, 192, 32, 64]     2,112                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 192, 32, 64]     [1, 192, 32, 64]     3,272                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 192, 32, 64]     [1, 32, 32, 64]      6,208                True\n",
      "│    │    └─Sequential (2)                                   [1, 32, 32, 64]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 32, 32, 64]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 32, 32, 64]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 32, 32, 64]      [1, 192, 32, 64]     6,528                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 192, 32, 64]     [1, 192, 16, 32]     5,184                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 192, 16, 32]     [1, 192, 16, 32]     3,272                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 192, 16, 32]     [1, 56, 16, 32]      10,864               True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 56, 16, 32]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 56, 16, 32]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 56, 16, 32]      [1, 336, 16, 32]     19,488               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 336, 16, 32]     [1, 336, 16, 32]     9,072                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 336, 16, 32]     [1, 336, 16, 32]     9,758                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 336, 16, 32]     [1, 56, 16, 32]      18,928               True\n",
      "│    │    │    └─EfficientNetBlock (2)                       [1, 56, 16, 32]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 56, 16, 32]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 56, 16, 32]      [1, 336, 16, 32]     19,488               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 336, 16, 32]     [1, 336, 16, 32]     9,072                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 336, 16, 32]     [1, 336, 16, 32]     9,758                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 336, 16, 32]     [1, 56, 16, 32]      18,928               True\n",
      "│    │    │    └─EfficientNetBlock (3)                       [1, 56, 16, 32]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 56, 16, 32]      [1, 56, 16, 32]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 56, 16, 32]      [1, 336, 16, 32]     19,488               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 336, 16, 32]     [1, 336, 16, 32]     9,072                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 336, 16, 32]     [1, 336, 16, 32]     9,758                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 336, 16, 32]     [1, 56, 16, 32]      18,928               True\n",
      "│    │    └─Sequential (3)                                   [1, 56, 16, 32]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 56, 16, 32]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 56, 16, 32]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 56, 16, 32]      [1, 336, 16, 32]     19,488               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 336, 16, 32]     [1, 336, 8, 16]      3,696                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 336, 8, 16]      [1, 336, 8, 16]      9,758                True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 336, 8, 16]      [1, 112, 8, 16]      37,856               True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 112, 8, 16]      [1, 672, 8, 16]      76,608               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 672, 8, 16]      [1, 672, 8, 16]      7,392                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 672, 8, 16]      [1, 672, 8, 16]      38,332               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 672, 8, 16]      [1, 112, 8, 16]      75,488               True\n",
      "│    │    │    └─EfficientNetBlock (2)                       [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 112, 8, 16]      [1, 672, 8, 16]      76,608               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 672, 8, 16]      [1, 672, 8, 16]      7,392                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 672, 8, 16]      [1, 672, 8, 16]      38,332               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 672, 8, 16]      [1, 112, 8, 16]      75,488               True\n",
      "│    │    │    └─EfficientNetBlock (3)                       [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 112, 8, 16]      [1, 672, 8, 16]      76,608               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 672, 8, 16]      [1, 672, 8, 16]      7,392                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 672, 8, 16]      [1, 672, 8, 16]      38,332               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 672, 8, 16]      [1, 112, 8, 16]      75,488               True\n",
      "│    │    │    └─EfficientNetBlock (4)                       [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 112, 8, 16]      [1, 672, 8, 16]      76,608               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 672, 8, 16]      [1, 672, 8, 16]      7,392                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 672, 8, 16]      [1, 672, 8, 16]      38,332               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 672, 8, 16]      [1, 112, 8, 16]      75,488               True\n",
      "│    │    │    └─EfficientNetBlock (5)                       [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 112, 8, 16]      [1, 112, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 112, 8, 16]      [1, 672, 8, 16]      76,608               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 672, 8, 16]      [1, 672, 8, 16]      7,392                True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 672, 8, 16]      [1, 672, 8, 16]      38,332               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 672, 8, 16]      [1, 112, 8, 16]      75,488               True\n",
      "│    │    └─Sequential (4)                                   [1, 112, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 112, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 112, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 112, 8, 16]      [1, 672, 8, 16]      76,608               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 672, 8, 16]      [1, 672, 8, 16]      18,144               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 672, 8, 16]      [1, 672, 8, 16]      38,332               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 672, 8, 16]      [1, 160, 8, 16]      107,840              True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 160, 8, 16]      [1, 960, 8, 16]      155,520              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 960, 8, 16]      [1, 960, 8, 16]      25,920               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 960, 8, 16]      [1, 960, 8, 16]      77,800               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 960, 8, 16]      [1, 160, 8, 16]      153,920              True\n",
      "│    │    │    └─EfficientNetBlock (2)                       [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 160, 8, 16]      [1, 960, 8, 16]      155,520              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 960, 8, 16]      [1, 960, 8, 16]      25,920               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 960, 8, 16]      [1, 960, 8, 16]      77,800               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 960, 8, 16]      [1, 160, 8, 16]      153,920              True\n",
      "│    │    │    └─EfficientNetBlock (3)                       [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 160, 8, 16]      [1, 960, 8, 16]      155,520              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 960, 8, 16]      [1, 960, 8, 16]      25,920               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 960, 8, 16]      [1, 960, 8, 16]      77,800               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 960, 8, 16]      [1, 160, 8, 16]      153,920              True\n",
      "│    │    │    └─EfficientNetBlock (4)                       [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 160, 8, 16]      [1, 960, 8, 16]      155,520              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 960, 8, 16]      [1, 960, 8, 16]      25,920               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 960, 8, 16]      [1, 960, 8, 16]      77,800               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 960, 8, 16]      [1, 160, 8, 16]      153,920              True\n",
      "│    │    │    └─EfficientNetBlock (5)                       [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 160, 8, 16]      [1, 160, 8, 16]      --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 160, 8, 16]      [1, 960, 8, 16]      155,520              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 960, 8, 16]      [1, 960, 8, 16]      25,920               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 960, 8, 16]      [1, 960, 8, 16]      77,800               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 960, 8, 16]      [1, 160, 8, 16]      153,920              True\n",
      "│    │    └─Sequential (5)                                   [1, 160, 8, 16]      [1, 272, 4, 8]       --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 160, 8, 16]      [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 160, 8, 16]      [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 160, 8, 16]      [1, 960, 8, 16]      155,520              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 960, 8, 16]      [1, 960, 4, 8]       25,920               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 960, 4, 8]       [1, 960, 4, 8]       77,800               True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 960, 4, 8]       [1, 272, 4, 8]       261,664              True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    │    └─EfficientNetBlock (2)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    │    └─EfficientNetBlock (3)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    │    └─EfficientNetBlock (4)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    │    └─EfficientNetBlock (5)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    │    └─EfficientNetBlock (6)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    │    └─EfficientNetBlock (7)                       [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 272, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      44,064               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 272, 4, 8]       444,448              True\n",
      "│    │    └─Sequential (6)                                   [1, 272, 4, 8]       [1, 448, 4, 8]       --                   True\n",
      "│    │    │    └─EfficientNetBlock (0)                       [1, 272, 4, 8]       [1, 448, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 272, 4, 8]       [1, 448, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 272, 4, 8]       [1, 1632, 4, 8]      447,168              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 1632, 4, 8]      [1, 1632, 4, 8]      17,952               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 1632, 4, 8]      [1, 1632, 4, 8]      223,652              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 1632, 4, 8]      [1, 448, 4, 8]       732,032              True\n",
      "│    │    │    └─EfficientNetBlock (1)                       [1, 448, 4, 8]       [1, 448, 4, 8]       --                   True\n",
      "│    │    │    │    └─Sequential (block)                     [1, 448, 4, 8]       [1, 448, 4, 8]       --                   True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (0)          [1, 448, 4, 8]       [1, 2688, 4, 8]      1,209,600            True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (1)          [1, 2688, 4, 8]      [1, 2688, 4, 8]      29,568               True\n",
      "│    │    │    │    │    └─SEBlock (2)                       [1, 2688, 4, 8]      [1, 2688, 4, 8]      604,912              True\n",
      "│    │    │    │    │    └─Conv2dNormActivation (3)          [1, 2688, 4, 8]      [1, 448, 4, 8]       1,205,120            True\n",
      "│    └─Conv2dNormActivation (2)                              [1, 448, 4, 8]       [1, 1280, 4, 8]      --                   True\n",
      "│    │    └─Conv2d (0)                                       [1, 448, 4, 8]       [1, 1280, 4, 8]      573,440              True\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 1280, 4, 8]      [1, 1280, 4, 8]      2,560                True\n",
      "│    │    └─SiLU (2)                                         [1, 1280, 4, 8]      [1, 1280, 4, 8]      --                   --\n",
      "│    └─AdaptiveAvgPool2d (3)                                 [1, 1280, 4, 8]      [1, 1280, 1, 1]      --                   --\n",
      "├─Sequential (classifier)                                    [1, 1280]            [1, 6]               --                   True\n",
      "│    └─Linear (0)                                            [1, 1280]            [1, 6]               7,686                True\n",
      "============================================================================================================================================\n",
      "Total params: 17,325,902\n",
      "Trainable params: 17,325,902\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 974.75\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 177.77\n",
      "Params size (MB): 69.30\n",
      "Estimated Total Size (MB): 247.47\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels, config,\n",
    "        activation_layer=nn.SiLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"EfficientNet\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottle_factor (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            phi (float, optional): The compound scaling coefficient. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        in_mb_channels = config.adjust_channels(32)\n",
    "        out_mb_channels = config.adjust_channels(320)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ops.Conv2dNormActivation(\n",
    "                in_channels, in_mb_channels,\n",
    "                kernel_size=3, padding=1, stride=2,\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            self.config.make_blocks(activation_layer=activation_layer, **kwargs),\n",
    "            ops.Conv2dNormActivation(\n",
    "                out_mb_channels, config.last_channels,\n",
    "                kernel_size=1, activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(p=config.dropout, inplace=True),\n",
    "            nn.Linear(config.last_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# in_channels, out_channels = 4, 6\n",
    "in_channels, out_channels = 3, 6\n",
    "eff_config = EfficientNetConfig(\n",
    "    # se_block=HBASeBlock,\n",
    "    # kernel_1=(1, 3),\n",
    ")\n",
    "model = EfficientNet(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    eff_config,\n",
    ")\n",
    "\n",
    "x = torch.randn(1, in_channels, 128, 256)\n",
    "print(summary(\n",
    "    model=model, \n",
    "    input_data=x,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
