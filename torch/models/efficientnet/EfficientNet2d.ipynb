{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. BottleNeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
      "========================================================================================================================\n",
      "BottleNeck (BottleNeck)                  [1, 2, 20, 40]       [1, 4, 10, 20]       --                   True\n",
      "├─ResidualAdd (block)                    [1, 2, 20, 40]       [1, 4, 10, 20]       --                   True\n",
      "│    └─Sequential (block)                [1, 2, 20, 40]       [1, 4, 10, 20]       --                   True\n",
      "│    │    └─Conv2dNormActivation (0)     [1, 2, 20, 40]       [1, 8, 20, 40]       32                   True\n",
      "│    │    └─Conv2dNormActivation (1)     [1, 8, 20, 40]       [1, 8, 10, 20]       592                  True\n",
      "│    │    └─Conv2dNormActivation (2)     [1, 8, 10, 20]       [1, 4, 10, 20]       40                   True\n",
      "│    └─ConvBnReLU2d (shortcut)           [1, 2, 20, 40]       [1, 4, 10, 20]       --                   True\n",
      "│    │    └─Sequential (block)           [1, 2, 20, 40]       [1, 4, 10, 20]       20                   True\n",
      "├─ReLU (act)                             [1, 4, 10, 20]       [1, 4, 10, 20]       --                   --\n",
      "========================================================================================================================\n",
      "Total params: 684\n",
      "Trainable params: 684\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.14\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.16\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        activation_layer=nn.ReLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Bottleneck block with an option to behave as a linear bottle-neck block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            act (nn.Module, optional): It activation layer applied to the block output.\n",
    "                If it is set to the identity layer, it becomes a linear bottleneck block.\n",
    "                Defaults to nn.ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        self.block = ResidualAdd(\n",
    "            nn.Sequential(\n",
    "                # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                ops.Conv2dNormActivation(\n",
    "                    in_channels, bottleneck_size,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    **kwargs\n",
    "                ),\n",
    "\n",
    "                # (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "                ops.Conv2dNormActivation(\n",
    "                    bottleneck_size, bottleneck_size,\n",
    "                    kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                    **kwargs\n",
    "                ),\n",
    "\n",
    "                # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                ops.Conv2dNormActivation(\n",
    "                    bottleneck_size, out_channels,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=nn.Identity,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            ),\n",
    "            # (..., out_channels, ...) -> (..., in_channels, ...)\n",
    "            shortcut=ConvBnReLU2d(\n",
    "                in_channels, out_channels,\n",
    "                kernel_size=1, stride=stride,\n",
    "                **kwargs\n",
    "            ) if in_channels != out_channels else None\n",
    "        )\n",
    "        self.act = activation_layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.block(x))\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = BottleNeck(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size, stride=2, padding=1,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. MobileNet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
      "========================================================================================================================\n",
      "MobileNetBlock (MobileNetBlock)          [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "├─Sequential (block)                     [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "│    └─Sequential (0)                    [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "│    │    └─Conv2dNormActivation (0)     [1, 2, 20, 40]       [1, 8, 20, 40]       32                   True\n",
      "│    │    └─Conv2dNormActivation (1)     [1, 8, 20, 40]       [1, 8, 20, 40]       592                  True\n",
      "│    │    └─Conv2dNormActivation (2)     [1, 8, 20, 40]       [1, 4, 20, 40]       40                   True\n",
      "├─Identity (act)                         [1, 4, 20, 40]       [1, 4, 20, 40]       --                   --\n",
      "========================================================================================================================\n",
      "Total params: 664\n",
      "Trainable params: 664\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.50\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.26\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.27\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class MobileNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        activation_layer=nn.Identity,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"MobileNetBlock is a bottleneck block which applies residual connections\n",
    "        if in_features == out_features. It uses a bottle_factor of 4.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            activation_layer (nn.Module, optional): It activation layer applied to the block output.\n",
    "                If it is set to the identity layer, it becomes a linear bottleneck block.\n",
    "                Defaults to nn.ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        residualOrNot = ResidualAdd if in_channels == out_channels else nn.Sequential\n",
    "        self.block = (\n",
    "            residualOrNot(\n",
    "                nn.Sequential(\n",
    "                    # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        in_channels, bottleneck_size,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, bottleneck_size,\n",
    "                        kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, out_channels,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.Identity,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        self.act = activation_layer()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.block(x))\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = MobileNetBlock(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. MBConv\n",
    "\n",
    "MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
      "========================================================================================================================\n",
      "MBConv (MBConv)                          [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "├─Sequential (block)                     [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "│    └─Sequential (0)                    [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "│    │    └─Conv2dNormActivation (0)     [1, 2, 20, 40]       [1, 8, 20, 40]       32                   True\n",
      "│    │    └─Conv2dNormActivation (1)     [1, 8, 20, 40]       [1, 8, 20, 40]       88                   True\n",
      "│    │    └─Conv2dNormActivation (2)     [1, 8, 20, 40]       [1, 4, 20, 40]       40                   True\n",
      "========================================================================================================================\n",
      "Total params: 160\n",
      "Trainable params: 160\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.10\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.26\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.26\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"MBConv is a MobileNetBlock with Depthwise Convolution.\n",
    "        It replaces ReLU with ReLU6\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        residualOrNot = ResidualAdd if in_channels == out_channels else nn.Sequential\n",
    "        self.block = (\n",
    "            residualOrNot(\n",
    "                nn.Sequential(\n",
    "                    # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        in_channels, bottleneck_size,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.ReLU6,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, bottleneck_size,\n",
    "                        kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                        groups=bottleneck_size, # Depthwise Convolution\n",
    "                        activation_layer=nn.ReLU6,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, out_channels,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.Identity,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = MBConv(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. FusedMBConv\n",
    "\n",
    "EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
      "========================================================================================================================\n",
      "FusedMBConv (FusedMBConv)                [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "├─Sequential (block)                     [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "│    └─Sequential (0)                    [1, 2, 20, 40]       [1, 4, 20, 40]       --                   True\n",
      "│    │    └─Conv2dNormActivation (0)     [1, 2, 20, 40]       [1, 8, 20, 40]       160                  True\n",
      "│    │    └─Conv2dNormActivation (1)     [1, 8, 20, 40]       [1, 4, 20, 40]       40                   True\n",
      "========================================================================================================================\n",
      "Total params: 200\n",
      "Trainable params: 200\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.14\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.16\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"FusedMBConv is a MBConv block with fused 1st and 2nd convs.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        residualOrNot = ResidualAdd if in_channels == out_channels else nn.Sequential\n",
    "        self.block = (\n",
    "            residualOrNot(\n",
    "                nn.Sequential(\n",
    "                    # (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        in_channels, bottleneck_size,\n",
    "                        kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                        activation_layer=nn.ReLU6,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "\n",
    "                    # (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "                    ops.Conv2dNormActivation(\n",
    "                        bottleneck_size, out_channels,\n",
    "                        kernel_size=1, stride=1, padding='same',\n",
    "                        activation_layer=nn.Identity,\n",
    "                        **kwargs\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# in_channels, out_channels = 2, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = FusedMBConv(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. SEBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
      "========================================================================================================================\n",
      "SEBlock (SEBlock)                        [1, 144, 10, 10]     [1, 144, 10, 10]     --                   True\n",
      "├─AdaptiveAvgPool2d (pool)               [1, 144, 10, 10]     [1, 144, 1, 1]       --                   --\n",
      "├─Conv2d (conv1)                         [1, 144, 1, 1]       [1, 6, 1, 1]         870                  True\n",
      "├─ReLU (activation)                      [1, 6, 1, 1]         [1, 6, 1, 1]         --                   --\n",
      "├─Conv2d (conv2)                         [1, 6, 1, 1]         [1, 144, 1, 1]       1,008                True\n",
      "├─Sigmoid (sigmoid)                      [1, 144, 1, 1]       [1, 144, 1, 1]       --                   --\n",
      "========================================================================================================================\n",
      "Total params: 1,878\n",
      "Trainable params: 1,878\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.07\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, activation_layer=nn.ReLU):\n",
    "        \"\"\"SEBlock is a FusedMBConv block with Squeeze-And-Excitation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, squeeze_channels, 1)\n",
    "        self.activation = activation_layer()\n",
    "        self.conv2 = nn.Conv2d(squeeze_channels, in_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "\n",
    "        # 1. (...) -> (..., in_channels, 1, 1)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 2. -> (..., squeeze_channels, 1, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # 3. -> (..., in_channels, 1, 1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 4. Scale\n",
    "        x = self.sigmoid(x)\n",
    "        x = inp * x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10, 10)\n",
    "# l = SEBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. EfficientNetBlock\n",
    "\n",
    "EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
      "========================================================================================================================\n",
      "EfficientNetBlock (EfficientNetBlock)    [1, 32, 20, 40]      [1, 4, 20, 40]       --                   True\n",
      "├─Sequential (block)                     [1, 32, 20, 40]      [1, 4, 20, 40]       --                   True\n",
      "│    └─Conv2dNormActivation (0)          [1, 32, 20, 40]      [1, 32, 20, 40]      --                   True\n",
      "│    │    └─Conv2d (0)                   [1, 32, 20, 40]      [1, 32, 20, 40]      288                  True\n",
      "│    │    └─BatchNorm2d (1)              [1, 32, 20, 40]      [1, 32, 20, 40]      64                   True\n",
      "│    │    └─SiLU (2)                     [1, 32, 20, 40]      [1, 32, 20, 40]      --                   --\n",
      "│    └─SEBlock (1)                       [1, 32, 20, 40]      [1, 32, 20, 40]      --                   True\n",
      "│    │    └─AdaptiveAvgPool2d (pool)     [1, 32, 20, 40]      [1, 32, 1, 1]        --                   --\n",
      "│    │    └─Conv2d (conv1)               [1, 32, 1, 1]        [1, 8, 1, 1]         264                  True\n",
      "│    │    └─SiLU (activation)            [1, 8, 1, 1]         [1, 8, 1, 1]         --                   --\n",
      "│    │    └─Conv2d (conv2)               [1, 8, 1, 1]         [1, 32, 1, 1]        288                  True\n",
      "│    │    └─Sigmoid (sigmoid)            [1, 32, 1, 1]        [1, 32, 1, 1]        --                   --\n",
      "│    └─Conv2dNormActivation (2)          [1, 32, 20, 40]      [1, 4, 20, 40]       --                   True\n",
      "│    │    └─Conv2d (0)                   [1, 32, 20, 40]      [1, 4, 20, 40]       128                  True\n",
      "│    │    └─BatchNorm2d (1)              [1, 4, 20, 40]       [1, 4, 20, 40]       8                    True\n",
      "│    │    └─Identity (2)                 [1, 4, 20, 40]       [1, 4, 20, 40]       --                   --\n",
      "========================================================================================================================\n",
      "Total params: 1,040\n",
      "Trainable params: 1,040\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.33\n",
      "========================================================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 0.46\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.57\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same', squeeze_ratio=4,\n",
    "        activation_layer=nn.SiLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"EfficientNetBlock is a MBConv block with SEBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        modules = nn.ModuleList()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        # 1. (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "        if in_channels != bottleneck_size:\n",
    "            modules.append(\n",
    "                ops.Conv2dNormActivation(\n",
    "                    in_channels, bottleneck_size,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=activation_layer,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # 2. (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "        modules.append(\n",
    "            ops.Conv2dNormActivation(\n",
    "                bottleneck_size, bottleneck_size,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                groups=bottleneck_size, # Depthwise Convolution\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 3. Squeeze and excitation block\n",
    "        squeeze_channels = max(1, in_channels // squeeze_ratio)\n",
    "        modules.append(\n",
    "            SEBlock(bottleneck_size, squeeze_channels, activation_layer=activation_layer)\n",
    "        )\n",
    "\n",
    "        # 4. (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "        modules.append(\n",
    "            ops.Conv2dNormActivation(\n",
    "                bottleneck_size, out_channels,\n",
    "                kernel_size=1, stride=1, padding='same',\n",
    "                activation_layer=nn.Identity,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.block = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = self.block(x)\n",
    "\n",
    "        if self.residual:\n",
    "            x = x + inp\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels, out_channels = 32, 4\n",
    "# kernel_size = 3\n",
    "# H, W = 20, 40\n",
    "# batch_size = 1\n",
    "# l = EfficientNetBlock(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size, bottleneck=1,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n",
    "\n",
    "* ReLU6 and SiLU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. EfficientNetConfig\n",
    "\n",
    "#### TODO\n",
    "- [x] Pipe activation_layer all the way through.\n",
    "- [x] Residual connection in SEBlock is broken for stride != 1.\n",
    "- [x] Round channels to multiple of 8.\n",
    "- [-] Pipe dropout.\n",
    "- [-] Configure the minimum number of channels in EfficientNetConfig.\n",
    "- [x] Adjust variable blocks to have same number of input and output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
      "==================================================================================================================================\n",
      "Sequential (Sequential)                            [1, 48, 224, 224]    [1, 448, 14, 14]     --                   True\n",
      "├─Sequential (0)                                   [1, 48, 224, 224]    [1, 24, 224, 224]    --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 48, 224, 224]    [1, 24, 224, 224]    --                   True\n",
      "│    │    └─Sequential (block)                     [1, 48, 224, 224]    [1, 24, 224, 224]    2,940                True\n",
      "│    └─EfficientNetBlock (1)                       [1, 24, 224, 224]    [1, 24, 224, 224]    --                   True\n",
      "│    │    └─Sequential (block)                     [1, 24, 224, 224]    [1, 24, 224, 224]    1,206                True\n",
      "├─Sequential (1)                                   [1, 24, 224, 224]    [1, 32, 112, 112]    --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 24, 224, 224]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                     [1, 24, 224, 224]    [1, 32, 112, 112]    11,878               True\n",
      "│    └─EfficientNetBlock (1)                       [1, 32, 112, 112]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                     [1, 32, 112, 112]    [1, 32, 112, 112]    18,120               True\n",
      "│    └─EfficientNetBlock (2)                       [1, 32, 112, 112]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                     [1, 32, 112, 112]    [1, 32, 112, 112]    18,120               True\n",
      "│    └─EfficientNetBlock (3)                       [1, 32, 112, 112]    [1, 32, 112, 112]    --                   True\n",
      "│    │    └─Sequential (block)                     [1, 32, 112, 112]    [1, 32, 112, 112]    18,120               True\n",
      "├─Sequential (2)                                   [1, 32, 112, 112]    [1, 56, 56, 56]      --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 32, 112, 112]    [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                     [1, 32, 112, 112]    [1, 56, 56, 56]      25,848               True\n",
      "│    └─EfficientNetBlock (1)                       [1, 56, 56, 56]      [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                     [1, 56, 56, 56]      [1, 56, 56, 56]      57,246               True\n",
      "│    └─EfficientNetBlock (2)                       [1, 56, 56, 56]      [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                     [1, 56, 56, 56]      [1, 56, 56, 56]      57,246               True\n",
      "│    └─EfficientNetBlock (3)                       [1, 56, 56, 56]      [1, 56, 56, 56]      --                   True\n",
      "│    │    └─Sequential (block)                     [1, 56, 56, 56]      [1, 56, 56, 56]      57,246               True\n",
      "├─Sequential (3)                                   [1, 56, 56, 56]      [1, 112, 28, 28]     --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 56, 56, 56]      [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 56, 56, 56]      [1, 112, 28, 28]     70,798               True\n",
      "│    └─EfficientNetBlock (1)                       [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (2)                       [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (3)                       [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (4)                       [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "│    └─EfficientNetBlock (5)                       [1, 112, 28, 28]     [1, 112, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 112, 28, 28]     [1, 112, 28, 28]     197,820              True\n",
      "├─Sequential (4)                                   [1, 112, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 112, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 112, 28, 28]     [1, 160, 28, 28]     240,924              True\n",
      "│    └─EfficientNetBlock (1)                       [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (2)                       [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (3)                       [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (4)                       [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "│    └─EfficientNetBlock (5)                       [1, 160, 28, 28]     [1, 160, 28, 28]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 160, 28, 28]     [1, 160, 28, 28]     413,160              True\n",
      "├─Sequential (5)                                   [1, 160, 28, 28]     [1, 272, 14, 14]     --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 160, 28, 28]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 160, 28, 28]     [1, 272, 14, 14]     520,904              True\n",
      "│    └─EfficientNetBlock (1)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (2)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (3)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (4)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (5)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (6)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "│    └─EfficientNetBlock (7)                       [1, 272, 14, 14]     [1, 272, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 272, 14, 14]     1,159,332            True\n",
      "├─Sequential (6)                                   [1, 272, 14, 14]     [1, 448, 14, 14]     --                   True\n",
      "│    └─EfficientNetBlock (0)                       [1, 272, 14, 14]     [1, 448, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 272, 14, 14]     [1, 448, 14, 14]     1,420,804            True\n",
      "│    └─EfficientNetBlock (1)                       [1, 448, 14, 14]     [1, 448, 14, 14]     --                   True\n",
      "│    │    └─Sequential (block)                     [1, 448, 14, 14]     [1, 448, 14, 14]     3,049,200            True\n",
      "==================================================================================================================================\n",
      "Total params: 16,740,824\n",
      "Trainable params: 16,740,824\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 5.78\n",
      "==================================================================================================================================\n",
      "Input size (MB): 9.63\n",
      "Forward/backward pass size (MB): 1045.10\n",
      "Params size (MB): 66.96\n",
      "Estimated Total Size (MB): 1121.69\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNetConfig(object):\n",
    "    BaseResolution = 224\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # B4 configuration\n",
    "        width_mult=1.4, depth_mult=1.8, dropout=0.4, last_channels=1280,\n",
    "\n",
    "        # Block configuration\n",
    "        kernel_1=3, kernel_2=5,\n",
    "        padding_1=1, padding_2=2,\n",
    "    ):\n",
    "        self.width_mult = width_mult\n",
    "        self.depth_mult =depth_mult\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.last_channels = last_channels\n",
    "\n",
    "        # Conv type 1\n",
    "        self.kernel_1 = kernel_1\n",
    "        self.padding_1 = padding_1\n",
    "\n",
    "        # Conv type 2\n",
    "        self.kernel_2 = kernel_2\n",
    "        self.padding_2 = padding_2\n",
    "\n",
    "        # MBConv configs\n",
    "        self.block_configs = [\n",
    "            # (in_channels, out_channels, bottleneck, kernel, padding, stride, layers)\n",
    "            (32, 16, 1, kernel_1, 'same', 1, 1),\n",
    "            (16, 24, 6, kernel_1, padding_1, 2, 2),\n",
    "            (24, 40, 6, kernel_2, padding_2, 2, 2),\n",
    "            (40, 80, 6, kernel_1, padding_1, 2, 3),\n",
    "            (80, 112, 6, kernel_2, 'same', 1, 3),\n",
    "            (112, 192, 6, kernel_2, padding_2, 2, 4),\n",
    "            (192, 320, 6, kernel_1, 'same', 1, 1),\n",
    "        ]\n",
    "\n",
    "    def adjust_channels(self, channels):\n",
    "        return self.round_to(channels*self.width_mult)\n",
    "    \n",
    "    def adjust_depth(self, num_layers):\n",
    "        return int(math.ceil(num_layers*self.depth_mult))\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_to(v, multiple=8):\n",
    "        return int(multiple * round(v / multiple))\n",
    "    \n",
    "    def _block(self, args, **kwargs):\n",
    "        in_channels, out_channels, bottleneck, kernel, padding, stride, layers = args\n",
    "\n",
    "        # 1. Update in_channels and out_channels based on the width_mult\n",
    "        in_channels = self.adjust_channels(in_channels)\n",
    "        out_channels = self.adjust_channels(out_channels)\n",
    "\n",
    "        # 2. Update layers based on depth_mult\n",
    "        layers = self.adjust_depth(layers)\n",
    "        \n",
    "        block = nn.Sequential(\n",
    "            EfficientNetBlock(\n",
    "                in_channels, out_channels,\n",
    "                bottleneck=bottleneck,\n",
    "                kernel_size=kernel, stride=stride, padding=padding,\n",
    "                **kwargs\n",
    "            ),\n",
    "            *map(\n",
    "                lambda _: EfficientNetBlock(\n",
    "                    out_channels, out_channels,\n",
    "                    bottleneck=bottleneck,\n",
    "                    kernel_size=kernel, stride=1, padding='same',\n",
    "                    **kwargs\n",
    "                ),\n",
    "                range(layers - 1)\n",
    "            )\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def make_blocks(self, **kwargs):\n",
    "        modules = nn.Sequential(\n",
    "            *map(\n",
    "                lambda b_config: self._block(b_config,  **kwargs),\n",
    "                self.block_configs\n",
    "            )\n",
    "        )\n",
    "        return modules\n",
    "\n",
    "# eff_config = EfficientNetConfig()\n",
    "# block_config = eff_config.block_configs[0]\n",
    "# block = eff_config.make_blocks()\n",
    "\n",
    "# x = torch.randn(1, 48, 224, 224)\n",
    "# print(summary(\n",
    "#     model=block, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"],\n",
    "#     # depth=8,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
      "============================================================================================================================================\n",
      "EfficientNet (EfficientNet)                                  [1, 3, 256, 256]     --                   --                   True\n",
      "├─Sequential (model)                                         [1, 3, 256, 256]     [1, 1280, 1, 1]      --                   True\n",
      "│    └─Conv2dNormActivation (0)                              [1, 3, 256, 256]     [1, 48, 128, 128]    --                   True\n",
      "│    │    └─Conv2d (0)                                       [1, 3, 256, 256]     [1, 48, 128, 128]    1,296                True\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 48, 128, 128]    [1, 48, 128, 128]    96                   True\n",
      "│    │    └─SiLU (2)                                         [1, 48, 128, 128]    [1, 48, 128, 128]    --                   --\n",
      "│    └─Sequential (1)                                        [1, 48, 128, 128]    [1, 448, 8, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                   [1, 48, 128, 128]    [1, 24, 128, 128]    4,146                True\n",
      "│    │    └─Sequential (1)                                   [1, 24, 128, 128]    [1, 32, 64, 64]      66,238               True\n",
      "│    │    └─Sequential (2)                                   [1, 32, 64, 64]      [1, 56, 32, 32]      197,586              True\n",
      "│    │    └─Sequential (3)                                   [1, 56, 32, 32]      [1, 112, 16, 16]     1,059,898            True\n",
      "│    │    └─Sequential (4)                                   [1, 112, 16, 16]     [1, 160, 16, 16]     2,306,724            True\n",
      "│    │    └─Sequential (5)                                   [1, 160, 16, 16]     [1, 272, 8, 8]       8,636,228            True\n",
      "│    │    └─Sequential (6)                                   [1, 272, 8, 8]       [1, 448, 8, 8]       4,470,004            True\n",
      "│    └─Conv2dNormActivation (2)                              [1, 448, 8, 8]       [1, 1280, 8, 8]      --                   True\n",
      "│    │    └─Conv2d (0)                                       [1, 448, 8, 8]       [1, 1280, 8, 8]      573,440              True\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 1280, 8, 8]      [1, 1280, 8, 8]      2,560                True\n",
      "│    │    └─SiLU (2)                                         [1, 1280, 8, 8]      [1, 1280, 8, 8]      --                   --\n",
      "│    └─AdaptiveAvgPool2d (3)                                 [1, 1280, 8, 8]      [1, 1280, 1, 1]      --                   --\n",
      "├─Sequential (classifier)                                    [1, 1280]            [1, 6]               --                   True\n",
      "│    └─Linear (0)                                            [1, 1280]            [1, 6]               7,686                True\n",
      "============================================================================================================================================\n",
      "Total params: 17,325,902\n",
      "Trainable params: 17,325,902\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.95\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 355.31\n",
      "Params size (MB): 69.30\n",
      "Estimated Total Size (MB): 425.40\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels, config,\n",
    "        activation_layer=nn.SiLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"EfficientNet\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottle_factor (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            phi (float, optional): The compound scaling coefficient. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        in_mb_channels = config.adjust_channels(32)\n",
    "        out_mb_channels = config.adjust_channels(320)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ops.Conv2dNormActivation(\n",
    "                in_channels, in_mb_channels,\n",
    "                kernel_size=config.kernel_1, padding=config.padding_1,\n",
    "                stride=2,\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            self.config.make_blocks(activation_layer=activation_layer, **kwargs),\n",
    "            ops.Conv2dNormActivation(\n",
    "                out_mb_channels, config.last_channels,\n",
    "                kernel_size=1, activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(p=config.dropout, inplace=True),\n",
    "            nn.Linear(config.last_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "# # in_channels, out_channels = 4, 6\n",
    "# in_channels, out_channels = 3, 6\n",
    "# eff_config = EfficientNetConfig()\n",
    "# model = EfficientNet(\n",
    "#     in_channels,\n",
    "#     out_channels,\n",
    "#     eff_config,\n",
    "# )\n",
    "\n",
    "# x = torch.randn(1, in_channels, 256, 256)\n",
    "# print(summary(\n",
    "#     model=model, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"],\n",
    "# ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
