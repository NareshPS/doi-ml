{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Conv1dNormActivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Conv1dNormActivation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        norm_layer=nn.BatchNorm1d,\n",
    "        activation_layer=nn.ReLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, **kwargs)\n",
    "        self.norm = norm_layer(out_channels) if norm_layer else None\n",
    "        self.activation = activation_layer() if activation_layer else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x) if self.norm else x\n",
    "        x = self.activation(x) if self.activation else x\n",
    "        return x\n",
    "\n",
    "# in_channels, out_channels = 144, 64\n",
    "# x = torch.randn(1, in_channels, 10)\n",
    "# l = Conv1dNormActivation(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=3, padding='same',\n",
    "# )\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. SEBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, squeeze_channels,\n",
    "        activation_layer=nn.ReLU,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "    ):\n",
    "        \"\"\"SEBlock is a FusedMBConv block with Squeeze-And-Excitation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            squeeze_channels (int): The number of channels to squeeze to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = pool_block(1)\n",
    "        self.conv1 = conv_block(\n",
    "            in_channels, squeeze_channels, 1,\n",
    "            norm_layer=None, activation_layer=activation_layer,\n",
    "        )\n",
    "        self.conv2 = conv_block(\n",
    "            squeeze_channels, in_channels, 1,\n",
    "            norm_layer=None, activation_layer=None,\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "\n",
    "        # 1. (...) -> (..., in_channels, 1, 1)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # 2. -> (..., squeeze_channels, 1, 1)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # 3. -> (..., in_channels, 1, 1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 4. Scale\n",
    "        x = self.sigmoid(x)\n",
    "        x = inp * x\n",
    "\n",
    "        return x\n",
    "\n",
    "# in_channels = 144\n",
    "# x = torch.randn(1, in_channels, 10)\n",
    "# l = SEBlock(in_channels, squeeze_channels=6)\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. EfficientNetBlock\n",
    "\n",
    "EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
      "=============================================================================================================================\n",
      "MBConv (MBConv)                               [1, 32, 20, 20]      [1, 4, 20, 20]       --                   True\n",
      "├─Sequential (block)                          [1, 32, 20, 20]      [1, 4, 20, 20]       --                   True\n",
      "│    └─Conv2dNormActivation (0)               [1, 32, 20, 20]      [1, 32, 20, 20]      --                   True\n",
      "│    │    └─Conv2d (0)                        [1, 32, 20, 20]      [1, 32, 20, 20]      288                  True\n",
      "│    │    └─BatchNorm2d (1)                   [1, 32, 20, 20]      [1, 32, 20, 20]      64                   True\n",
      "│    │    └─SiLU (2)                          [1, 32, 20, 20]      [1, 32, 20, 20]      --                   --\n",
      "│    └─SEBlock (1)                            [1, 32, 20, 20]      [1, 32, 20, 20]      --                   True\n",
      "│    │    └─AdaptiveAvgPool2d (pool)          [1, 32, 20, 20]      [1, 32, 1, 1]        --                   --\n",
      "│    │    └─Conv2dNormActivation (conv1)      [1, 32, 1, 1]        [1, 8, 1, 1]         264                  True\n",
      "│    │    └─Conv2dNormActivation (conv2)      [1, 8, 1, 1]         [1, 32, 1, 1]        288                  True\n",
      "│    │    └─Sigmoid (sigmoid)                 [1, 32, 1, 1]        [1, 32, 1, 1]        --                   --\n",
      "│    └─Conv2dNormActivation (2)               [1, 32, 20, 20]      [1, 4, 20, 20]       --                   True\n",
      "│    │    └─Conv2d (0)                        [1, 32, 20, 20]      [1, 4, 20, 20]       128                  True\n",
      "│    │    └─BatchNorm2d (1)                   [1, 4, 20, 20]       [1, 4, 20, 20]       8                    True\n",
      "│    │    └─Identity (2)                      [1, 4, 20, 20]       [1, 4, 20, 20]       --                   --\n",
      "=============================================================================================================================\n",
      "Total params: 1,040\n",
      "Trainable params: 1,040\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.17\n",
      "=============================================================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.23\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.29\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same', squeeze_ratio=4,\n",
    "        activation_layer=nn.SiLU,\n",
    "        se_block=SEBlock,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"EfficientNetBlock is a MBConv block with SEBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        modules = nn.ModuleList()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        # 1. (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "        if in_channels != bottleneck_size:\n",
    "            modules.append(\n",
    "                conv_block(\n",
    "                    in_channels, bottleneck_size,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=activation_layer,\n",
    "                    **kwargs\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # 2. (..., bottleneck_size, ...) -> (..., bottleneck_size, ...)\n",
    "        modules.append(\n",
    "            conv_block(\n",
    "                bottleneck_size, bottleneck_size,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                groups=bottleneck_size, # Depthwise Convolution\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 3. Squeeze and excitation block\n",
    "        squeeze_channels = max(1, in_channels // squeeze_ratio)\n",
    "        modules.append(\n",
    "            se_block(\n",
    "                bottleneck_size, squeeze_channels,\n",
    "                activation_layer=activation_layer,\n",
    "                conv_block=conv_block,\n",
    "                pool_block=pool_block,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 4. (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "        modules.append(\n",
    "            conv_block(\n",
    "                bottleneck_size, out_channels,\n",
    "                kernel_size=1, stride=1, padding='same',\n",
    "                activation_layer=nn.Identity,\n",
    "                **kwargs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.block = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = self.block(x)\n",
    "\n",
    "        if self.residual:\n",
    "            x = x + inp\n",
    "\n",
    "        return x\n",
    "\n",
    "in_channels, out_channels = 32, 4\n",
    "kernel_size = 3\n",
    "batch_size = 1\n",
    "l = MBConv(\n",
    "    in_channels, out_channels,\n",
    "    kernel_size=kernel_size, bottleneck=1,\n",
    ")\n",
    "x = torch.randn(batch_size, in_channels, 20, 20)\n",
    "out = l(x)\n",
    "x.shape, out.shape\n",
    "\n",
    "print(summary(\n",
    "    model=l, \n",
    "    input_data=x,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. FusedMBConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels,\n",
    "        bottleneck=4, kernel_size=3, stride=1, padding='same',\n",
    "        activation_layer=nn.SiLU,\n",
    "        se_block=SEBlock,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"FusedMBConv is a MBConv block with fused 1st and 2nd convs.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottleneck (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        modules = nn.ModuleList()\n",
    "        bottleneck_size = int(in_channels*bottleneck)\n",
    "\n",
    "        # 1. (..., in_channels, ...) -> (..., bottleneck_size, ...)\n",
    "        modules.append(\n",
    "            conv_block(\n",
    "                in_channels, bottleneck_size,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 2. (..., bottleneck_size, ...) -> (..., out_channels, ...)\n",
    "        if in_channels != bottleneck_size:\n",
    "            modules.append(\n",
    "                conv_block(\n",
    "                    bottleneck_size, out_channels,\n",
    "                    kernel_size=1, stride=1, padding='same',\n",
    "                    activation_layer=nn.Identity,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "        self.block = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# in_channels, out_channels = 24, 48\n",
    "# kernel_size = 3\n",
    "# H, W = 256, 256\n",
    "# batch_size = 1\n",
    "# l = FusedMBConv(\n",
    "#     in_channels, out_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "#     bottleneck=1,\n",
    "# )\n",
    "# x = torch.randn(batch_size, in_channels, H, W)\n",
    "# out = l(x)\n",
    "# x.shape, out.shape\n",
    "\n",
    "# print(summary(\n",
    "#     model=l, \n",
    "#     input_data=x,\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#     col_width=20,\n",
    "#     row_settings=[\"var_names\"],\n",
    "#     depth=2\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n",
    "\n",
    "* ReLU6 and SiLU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. EfficientNetConfig\n",
    "\n",
    "#### TODO\n",
    "- [x] Pipe activation_layer all the way through.\n",
    "- [x] Residual connection in SEBlock is broken for stride != 1.\n",
    "- [x] Round channels to multiple of 8.\n",
    "- [-] Pipe dropout.\n",
    "- [-] Configure the minimum number of channels in EfficientNetConfig.\n",
    "- [x] Adjust variable blocks to have same number of input and output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================================\n",
      "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
      "=======================================================================================================================================\n",
      "Sequential (Sequential)                                 [1, 24, 256, 256]    [1, 256, 16, 16]     --                   True\n",
      "├─Sequential (0)                                        [1, 24, 256, 256]    [1, 24, 256, 256]    --                   True\n",
      "│    └─FusedMBConv (0)                                  [1, 24, 256, 256]    [1, 24, 256, 256]    --                   True\n",
      "│    │    └─Sequential (block)                          [1, 24, 256, 256]    [1, 24, 256, 256]    --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 24, 256, 256]    [1, 24, 256, 256]    5,232                True\n",
      "│    └─FusedMBConv (1)                                  [1, 24, 256, 256]    [1, 24, 256, 256]    --                   True\n",
      "│    │    └─Sequential (block)                          [1, 24, 256, 256]    [1, 24, 256, 256]    --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 24, 256, 256]    [1, 24, 256, 256]    5,232                True\n",
      "├─Sequential (1)                                        [1, 24, 256, 256]    [1, 48, 128, 128]    --                   True\n",
      "│    └─FusedMBConv (0)                                  [1, 24, 256, 256]    [1, 48, 128, 128]    --                   True\n",
      "│    │    └─Sequential (block)                          [1, 24, 256, 256]    [1, 48, 128, 128]    --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 24, 256, 256]    [1, 96, 128, 128]    20,928               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 96, 128, 128]    [1, 48, 128, 128]    4,704                True\n",
      "│    └─FusedMBConv (1)                                  [1, 48, 128, 128]    [1, 48, 128, 128]    --                   True\n",
      "│    │    └─Sequential (block)                          [1, 48, 128, 128]    [1, 48, 128, 128]    --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 48, 128, 128]    [1, 192, 128, 128]   83,328               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 192, 128, 128]   [1, 48, 128, 128]    9,312                True\n",
      "│    └─FusedMBConv (2)                                  [1, 48, 128, 128]    [1, 48, 128, 128]    --                   True\n",
      "│    │    └─Sequential (block)                          [1, 48, 128, 128]    [1, 48, 128, 128]    --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 48, 128, 128]    [1, 192, 128, 128]   83,328               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 192, 128, 128]   [1, 48, 128, 128]    9,312                True\n",
      "│    └─FusedMBConv (3)                                  [1, 48, 128, 128]    [1, 48, 128, 128]    --                   True\n",
      "│    │    └─Sequential (block)                          [1, 48, 128, 128]    [1, 48, 128, 128]    --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 48, 128, 128]    [1, 192, 128, 128]   83,328               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 192, 128, 128]   [1, 48, 128, 128]    9,312                True\n",
      "├─Sequential (2)                                        [1, 48, 128, 128]    [1, 64, 64, 64]      --                   True\n",
      "│    └─FusedMBConv (0)                                  [1, 48, 128, 128]    [1, 64, 64, 64]      --                   True\n",
      "│    │    └─Sequential (block)                          [1, 48, 128, 128]    [1, 64, 64, 64]      --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 48, 128, 128]    [1, 192, 64, 64]     83,328               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 192, 64, 64]     [1, 64, 64, 64]      12,416               True\n",
      "│    └─FusedMBConv (1)                                  [1, 64, 64, 64]      [1, 64, 64, 64]      --                   True\n",
      "│    │    └─Sequential (block)                          [1, 64, 64, 64]      [1, 64, 64, 64]      --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 64, 64, 64]      [1, 256, 64, 64]     147,968              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 256, 64, 64]     [1, 64, 64, 64]      16,512               True\n",
      "│    └─FusedMBConv (2)                                  [1, 64, 64, 64]      [1, 64, 64, 64]      --                   True\n",
      "│    │    └─Sequential (block)                          [1, 64, 64, 64]      [1, 64, 64, 64]      --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 64, 64, 64]      [1, 256, 64, 64]     147,968              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 256, 64, 64]     [1, 64, 64, 64]      16,512               True\n",
      "│    └─FusedMBConv (3)                                  [1, 64, 64, 64]      [1, 64, 64, 64]      --                   True\n",
      "│    │    └─Sequential (block)                          [1, 64, 64, 64]      [1, 64, 64, 64]      --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 64, 64, 64]      [1, 256, 64, 64]     147,968              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 256, 64, 64]     [1, 64, 64, 64]      16,512               True\n",
      "├─Sequential (3)                                        [1, 64, 64, 64]      [1, 128, 32, 32]     --                   True\n",
      "│    └─MBConv (0)                                       [1, 64, 64, 64]      [1, 128, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 64, 64, 64]      [1, 128, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 64, 64, 64]      [1, 256, 64, 64]     16,896               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 256, 64, 64]     [1, 256, 32, 32]     2,816                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 256, 32, 32]     [1, 256, 32, 32]     8,464                True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 256, 32, 32]     [1, 128, 32, 32]     33,024               True\n",
      "│    └─MBConv (1)                                       [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 128, 32, 32]     [1, 512, 32, 32]     66,560               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 512, 32, 32]     [1, 512, 32, 32]     5,632                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 512, 32, 32]     [1, 512, 32, 32]     33,312               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 512, 32, 32]     [1, 128, 32, 32]     65,792               True\n",
      "│    └─MBConv (2)                                       [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 128, 32, 32]     [1, 512, 32, 32]     66,560               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 512, 32, 32]     [1, 512, 32, 32]     5,632                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 512, 32, 32]     [1, 512, 32, 32]     33,312               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 512, 32, 32]     [1, 128, 32, 32]     65,792               True\n",
      "│    └─MBConv (3)                                       [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 128, 32, 32]     [1, 512, 32, 32]     66,560               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 512, 32, 32]     [1, 512, 32, 32]     5,632                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 512, 32, 32]     [1, 512, 32, 32]     33,312               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 512, 32, 32]     [1, 128, 32, 32]     65,792               True\n",
      "│    └─MBConv (4)                                       [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 128, 32, 32]     [1, 512, 32, 32]     66,560               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 512, 32, 32]     [1, 512, 32, 32]     5,632                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 512, 32, 32]     [1, 512, 32, 32]     33,312               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 512, 32, 32]     [1, 128, 32, 32]     65,792               True\n",
      "│    └─MBConv (5)                                       [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 128, 32, 32]     [1, 128, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 128, 32, 32]     [1, 512, 32, 32]     66,560               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 512, 32, 32]     [1, 512, 32, 32]     5,632                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 512, 32, 32]     [1, 512, 32, 32]     33,312               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 512, 32, 32]     [1, 128, 32, 32]     65,792               True\n",
      "├─Sequential (4)                                        [1, 128, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    └─MBConv (0)                                       [1, 128, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 128, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 128, 32, 32]     [1, 768, 32, 32]     99,840               True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 768, 32, 32]     [1, 768, 32, 32]     8,448                True\n",
      "│    │    │    └─SEBlock (2)                            [1, 768, 32, 32]     [1, 768, 32, 32]     49,952               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 768, 32, 32]     [1, 160, 32, 32]     123,200              True\n",
      "│    └─MBConv (1)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (2)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (3)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (4)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (5)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (6)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (7)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "│    └─MBConv (8)                                       [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 160, 32, 32]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 32, 32]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 32, 32]     [1, 960, 32, 32]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 32, 32]     [1, 160, 32, 32]     153,920              True\n",
      "├─Sequential (5)                                        [1, 160, 32, 32]     [1, 256, 16, 16]     --                   True\n",
      "│    └─MBConv (0)                                       [1, 160, 32, 32]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 160, 32, 32]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 160, 32, 32]     [1, 960, 32, 32]     155,520              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 960, 32, 32]     [1, 960, 16, 16]     10,560               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 960, 16, 16]     [1, 960, 16, 16]     77,800               True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 960, 16, 16]     [1, 256, 16, 16]     246,272              True\n",
      "│    └─MBConv (1)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (2)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (3)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (4)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (5)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (6)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (7)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (8)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (9)                                       [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (10)                                      [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (11)                                      [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (12)                                      [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (13)                                      [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "│    └─MBConv (14)                                      [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    └─Sequential (block)                          [1, 256, 16, 16]     [1, 256, 16, 16]     --                   True\n",
      "│    │    │    └─Conv2dNormActivation (0)               [1, 256, 16, 16]     [1, 1536, 16, 16]    396,288              True\n",
      "│    │    │    └─Conv2dNormActivation (1)               [1, 1536, 16, 16]    [1, 1536, 16, 16]    16,896               True\n",
      "│    │    │    └─SEBlock (2)                            [1, 1536, 16, 16]    [1, 1536, 16, 16]    198,208              True\n",
      "│    │    │    └─Conv2dNormActivation (3)               [1, 1536, 16, 16]    [1, 256, 16, 16]     393,728              True\n",
      "=======================================================================================================================================\n",
      "Total params: 19,846,552\n",
      "Trainable params: 19,846,552\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 14.74\n",
      "=======================================================================================================================================\n",
      "Input size (MB): 6.29\n",
      "Forward/backward pass size (MB): 986.21\n",
      "Params size (MB): 79.39\n",
      "Estimated Total Size (MB): 1071.88\n",
      "=======================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNetV2Config(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # B4 configuration\n",
    "        width_mult=1.4, depth_mult=1.8,\n",
    "        dropout=0.4, last_channels=1280,\n",
    "\n",
    "        # Block configuration\n",
    "        kernel=3,\n",
    "\n",
    "        # Blocks\n",
    "        se_block=SEBlock,\n",
    "        conv_block=ops.Conv2dNormActivation,\n",
    "        pool_block=nn.AdaptiveAvgPool2d,\n",
    "    ):\n",
    "        self.width_mult = width_mult\n",
    "        self.depth_mult =depth_mult\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.last_channels = last_channels\n",
    "\n",
    "        # Conv type 1\n",
    "        self.kernel = kernel\n",
    "\n",
    "        # Blocks\n",
    "        self.se_block = se_block\n",
    "        self.conv_block = conv_block\n",
    "        self.pool_block = pool_block\n",
    "\n",
    "        # Block configs\n",
    "        self.block_configs = [\n",
    "            # (type, in_channels, out_channels, bottleneck, kernel, padding, stride, layers)\n",
    "            (FusedMBConv, 24, 24, 1, kernel, 'same', 1, 2),\n",
    "            (FusedMBConv, 24, 48, 4, 3, 1, 2, 4),\n",
    "            (FusedMBConv, 48, 64, 4, 3, 1, 2, 4),\n",
    "\n",
    "            (MBConv, 64, 128, 4, 3, 1, 2, 6),\n",
    "            (MBConv, 128, 160, 6, kernel, 'same', 1, 9),\n",
    "            (MBConv, 160, 256, 6, 3, 1, 2, 15),\n",
    "        ]\n",
    "\n",
    "    def adjust_channels(self, channels):\n",
    "        return self.round_to(channels*self.width_mult)\n",
    "    \n",
    "    def adjust_depth(self, num_layers):\n",
    "        return int(math.ceil(num_layers*self.depth_mult))\n",
    "    \n",
    "    @staticmethod\n",
    "    def round_to(v, multiple=8):\n",
    "        return int(multiple * round(v / multiple))\n",
    "    \n",
    "    def _block(self, args, **kwargs):\n",
    "        b_type, in_channels, out_channels, bottleneck, kernel, padding, stride, layers = args\n",
    "\n",
    "        # # 1. Update in_channels and out_channels based on the width_mult\n",
    "        # in_channels = self.adjust_channels(in_channels)\n",
    "        # out_channels = self.adjust_channels(out_channels)\n",
    "\n",
    "        # # 2. Update layers based on depth_mult\n",
    "        # layers = self.adjust_depth(layers)\n",
    "        block = nn.Sequential(\n",
    "            b_type(\n",
    "                in_channels, out_channels,\n",
    "                bottleneck=bottleneck,\n",
    "                kernel_size=kernel, stride=stride, padding=padding,\n",
    "                se_block=self.se_block,\n",
    "                conv_block=self.conv_block,\n",
    "                pool_block=self.pool_block,\n",
    "                **kwargs\n",
    "            ),\n",
    "            *map(\n",
    "                lambda _: b_type(\n",
    "                    out_channels, out_channels,\n",
    "                    bottleneck=bottleneck,\n",
    "                    kernel_size=kernel, stride=1, padding='same',\n",
    "                    se_block=self.se_block,\n",
    "                    conv_block=self.conv_block,\n",
    "                    pool_block=self.pool_block,\n",
    "                    **kwargs\n",
    "                ),\n",
    "                range(layers - 1)\n",
    "            )\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def make_blocks(self, **kwargs):\n",
    "        modules = nn.Sequential(\n",
    "            *map(\n",
    "                lambda b_config: self._block(b_config,  **kwargs),\n",
    "                self.block_configs\n",
    "            )\n",
    "        )\n",
    "        return modules\n",
    "\n",
    "eff_config = EfficientNetV2Config(\n",
    "    # se_block=SEBlock,\n",
    "    # conv_block=Conv1dNormActivation,\n",
    "    # pool_block=nn.AdaptiveAvgPool1d\n",
    "    \n",
    ")\n",
    "block_config = eff_config.block_configs[0]\n",
    "block = eff_config.make_blocks()\n",
    "\n",
    "x = torch.randn(1, 24, 256, 256)\n",
    "\n",
    "print(summary(\n",
    "    model=block, \n",
    "    input_data=x,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=4,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================\n",
      "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
      "=================================================================================================================================================\n",
      "EfficientNetV2 (EfficientNetV2)                                   [1, 4, 256, 256]     [1, 6]               --                   True\n",
      "├─Sequential (model)                                              [1, 4, 256, 256]     [1, 1280, 1, 1]      --                   True\n",
      "│    └─Conv2dNormActivation (0)                                   [1, 4, 256, 256]     [1, 24, 128, 128]    --                   True\n",
      "│    │    └─Conv2d (0)                                            [1, 4, 256, 256]     [1, 24, 128, 128]    864                  True\n",
      "│    │    └─BatchNorm2d (1)                                       [1, 24, 128, 128]    [1, 24, 128, 128]    48                   True\n",
      "│    │    └─SiLU (2)                                              [1, 24, 128, 128]    [1, 24, 128, 128]    --                   --\n",
      "│    └─Sequential (1)                                             [1, 24, 128, 128]    [1, 256, 8, 8]       --                   True\n",
      "│    │    └─Sequential (0)                                        [1, 24, 128, 128]    [1, 24, 128, 128]    10,464               True\n",
      "│    │    └─Sequential (1)                                        [1, 24, 128, 128]    [1, 48, 64, 64]      303,552              True\n",
      "│    │    └─Sequential (2)                                        [1, 48, 64, 64]      [1, 64, 32, 32]      589,184              True\n",
      "│    │    └─Sequential (3)                                        [1, 64, 32, 32]      [1, 128, 16, 16]     917,680              True\n",
      "│    │    └─Sequential (4)                                        [1, 128, 16, 16]     [1, 160, 16, 16]     3,463,840            True\n",
      "│    │    └─Sequential (5)                                        [1, 160, 16, 16]     [1, 256, 8, 8]       14,561,832           True\n",
      "│    └─Conv2dNormActivation (2)                                   [1, 256, 8, 8]       [1, 1280, 8, 8]      --                   True\n",
      "│    │    └─Conv2d (0)                                            [1, 256, 8, 8]       [1, 1280, 8, 8]      327,680              True\n",
      "│    │    └─BatchNorm2d (1)                                       [1, 1280, 8, 8]      [1, 1280, 8, 8]      2,560                True\n",
      "│    │    └─SiLU (2)                                              [1, 1280, 8, 8]      [1, 1280, 8, 8]      --                   --\n",
      "│    └─AdaptiveAvgPool2d (3)                                      [1, 1280, 8, 8]      [1, 1280, 1, 1]      --                   --\n",
      "├─Sequential (classifier)                                         [1, 1280]            [1, 6]               --                   True\n",
      "│    └─Linear (0)                                                 [1, 1280]            [1, 6]               7,686                True\n",
      "=================================================================================================================================================\n",
      "Total params: 20,185,390\n",
      "Trainable params: 20,185,390\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 3.72\n",
      "=================================================================================================================================================\n",
      "Input size (MB): 1.05\n",
      "Forward/backward pass size (MB): 254.36\n",
      "Params size (MB): 80.74\n",
      "Estimated Total Size (MB): 336.15\n",
      "=================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "class EfficientNetV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, out_channels, config,\n",
    "        activation_layer=nn.SiLU,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"EfficientNet\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            bottle_factor (int, optional): The size of bottle neck. Defaults to 4.\n",
    "            kernel_size (int, optional): The kernel for the middle convolution. Defaults to 3.\n",
    "            stride (int, optional): The stride for the middle convolution and the shortcut. Defaults to 1.\n",
    "            padding (str, optional): The padding for the middle convolution. Defaults to 'same'.\n",
    "            phi (float, optional): The compound scaling coefficient. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # in_mb_channels = config.adjust_channels(32)\n",
    "        # out_mb_channels = config.adjust_channels(320)\n",
    "        in_mb_channels = config.block_configs[0][1]\n",
    "        out_mb_channels = config.block_configs[-1][2]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            config.conv_block(\n",
    "                in_channels, in_mb_channels,\n",
    "                kernel_size=3, padding=1, stride=2,\n",
    "                activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            self.config.make_blocks(activation_layer=activation_layer, **kwargs),\n",
    "            config.conv_block(\n",
    "                out_mb_channels, config.last_channels,\n",
    "                kernel_size=1, activation_layer=activation_layer,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            config.pool_block(1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(p=config.dropout, inplace=True),\n",
    "            nn.Linear(config.last_channels, out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "in_channels, out_channels = 4, 6\n",
    "eff_config = EfficientNetV2Config()\n",
    "model = EfficientNetV2(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    eff_config,\n",
    ")\n",
    "\n",
    "x = torch.randn(1, in_channels, 256, 256)\n",
    "print(summary(\n",
    "    model=model, \n",
    "    input_data=x,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "    # depth=6\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
