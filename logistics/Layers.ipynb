{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PositionEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, initializer=\"glorot_uniform\", **kwargs ):\n",
    "        super().__init__(**kwargs)\n",
    "        self._initializer = tf.keras.initializers.get(initializer)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
    "        }\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        sequence_length = input_shape[-2]\n",
    "        width = input_shape[-1]\n",
    "\n",
    "        self._position_embeddings = self.add_weight(\n",
    "            \"position_embeddings\",\n",
    "            shape=[sequence_length, width],\n",
    "            initializer=self._initializer)\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self._position_embeddings\n",
    "\n",
    "xx = tf.zeros((2, 4, 12))\n",
    "l = PositionEmbedding()\n",
    "output = l(xx)\n",
    "\n",
    "print('PositionEmbedding Layer')\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))\n",
    "print(f'Embedding Size: {l.weights[0].shape}')\n",
    "print(f'Verify Embeddings: {tf.reduce_all(tf.math.equal(output, l.weights))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StochasticDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"It is sourced from tensorflow-models package\n",
    "\n",
    "Source: https://github.com/tensorflow/models/blob/v2.12.0/official/vision/modeling/layers/nn_layers.py#L227-L262\n",
    "\"\"\"\n",
    "class StochasticDepth(layers.Layer):\n",
    "    \"\"\"Creates a stochastic depth layer.\"\"\"\n",
    "\n",
    "    def __init__(self, stochastic_depth_drop_rate, **kwargs):\n",
    "        \"\"\"Initializes a stochastic depth layer.\n",
    "\n",
    "        Args:\n",
    "          stochastic_depth_drop_rate: A `float` of drop rate.\n",
    "          **kwargs: Additional keyword arguments to be passed.\n",
    "\n",
    "        Returns:\n",
    "          A output `tf.Tensor` of which should have the same shape as input.\n",
    "        \"\"\"\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self._drop_rate = stochastic_depth_drop_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stochastic_depth_drop_rate': self._drop_rate}\n",
    "        base_config = super(StochasticDepth, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "        if not training or self._drop_rate is None or self._drop_rate == 0:\n",
    "            return inputs\n",
    "\n",
    "        keep_prob = 1.0 - self._drop_rate\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random.uniform(\n",
    "            [batch_size] + [1] * (inputs.shape.rank - 1), dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output\n",
    "\n",
    "batch_size, num_patches, dims = 2, 256, 768\n",
    "drop_prob = .5\n",
    "\n",
    "l = StochasticDepth(drop_prob)\n",
    "xx = tf.random.normal((batch_size, num_patches, dims))\n",
    "output = l(xx, training=True)\n",
    "non_zeros = 1 - (tf.math.reduce_sum(tf.cast(output == 0, tf.int64))/(batch_size * num_patches * dims))\n",
    "\n",
    "print('StochasticDepth Layer')\n",
    "print('---------------------')\n",
    "print(f'batch_size: {batch_size}, num_patches: {num_patches}, dims: {dims}')\n",
    "print(f'\\ndrop_prob: {drop_prob}')\n",
    "print(f'\\nInput: {xx.shape} --> {output.shape}')\n",
    "print(f'\\nOutput Drop Rate: {non_zeros}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, dims):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        \n",
    "        self.key = layers.Dense(dims, use_bias=False)\n",
    "        self.query = layers.Dense(dims, use_bias=False)\n",
    "        self.value = layers.Dense(dims, use_bias=False)\n",
    "        self.dims = dims\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k, q, v = self.key(x), self.query(x), self.query(x)\n",
    "\n",
    "        kq = k @ tf.transpose(q, perm=[0, 2, 1]) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "        kq /= tf.math.sqrt(self.dims)\n",
    "\n",
    "        # Lower triangular matrix\n",
    "        causal_mask = tf.linalg.band_part(\n",
    "            tf.ones((T, T)), -1, 0\n",
    "        )\n",
    "\n",
    "        kq = tf.nn.softmax(tf.where(causal_mask > 0.0, kq, float('-inf')))\n",
    "        output = kq @ v\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, dims):\n",
    "        super().__init__()\n",
    "        self.attn_layers = [SelfAttention(dims) for i in range(num_heads)]\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.concat([\n",
    "            attn_layer(x) for attn_layer in self.attn_layers\n",
    "        ], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RotaryPositionalEncodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class RotaryPositionalEncodings(layers.Layer):\n",
    "    def __init__(self, dims, block_size, base=10000.):\n",
    "        super(RotaryPositionalEncodings, self).__init__()\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.base = base\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.theta = 1. / (self.base ** (tf.range(0, self.dims, 2, dtype=tf.float32) / self.dims))\n",
    "        self.positions = tf.range(self.block_size, dtype=tf.float32)\n",
    "        self.mtheta = self.positions[..., None]*self.theta[None, ...]\n",
    "        self.mtheta_paired = tf.concat([self.mtheta]*2, axis=-1)\n",
    "\n",
    "        self.cos_mtheta = tf.math.cos(self.mtheta_paired)\n",
    "        self.sin_mtheta = tf.math.sin(self.mtheta_paired)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "\n",
    "        x_real = x*self.cos_mtheta[None, :T, None, ...]\n",
    "        # print(f'{x_real=}\\n{self.cos_mtheta[:T]=}')\n",
    "        x_img = tf.concat(\n",
    "            [-x[..., self.dims//2:], x[..., :self.dims//2]],\n",
    "            axis=-1\n",
    "        )*self.sin_mtheta[None, :T, None, ...]\n",
    "        # print(f'{x_img=}\\n{self.sin_mtheta[:T]=}')\n",
    "        output = x_real + x_img\n",
    "        return output\n",
    "\n",
    "# B, T, H, D = 2, 8, 4, 4\n",
    "# B_i, T_i, H_i, D_i = 0, 1, 0, 0\n",
    "# x = tf.random.uniform((B, T, H, D))\n",
    "# l = RotaryPositionalEncodings(4, 64)\n",
    "# output = l(x)\n",
    "# # tf.print(f'{l.theta=} {l.theta.shape}')\n",
    "# # tf.print(f'{l.positions=} {l.positions.shape}')\n",
    "# tf.print(f'{x[B_i]=}')\n",
    "# tf.print(f'{l.cos_mtheta=}')\n",
    "# tf.print(f'{l.sin_mtheta=}')\n",
    "# tf.print(f'{output[B_i]=}\\n{output[B_i]=} {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KVCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class KVCache(object):\n",
    "    def __init__(self, cache_size, block_size, heads, head_dims):\n",
    "        super(KVCache, self).__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        cache_shape = (cache_size, block_size, heads, head_dims)\n",
    "\n",
    "        with tf.device('/device:CPU:0'):\n",
    "            self.cache_k = tf.Variable(tf.zeros(cache_shape), shape=cache_shape, trainable=False)\n",
    "            self.cache_v = tf.Variable(tf.zeros(cache_shape), shape=cache_shape, trainable=False)\n",
    "\n",
    "    def update(self, start, xk, xv):\n",
    "        shape = tf.shape(xk)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "\n",
    "        # Calculate update start and end positions\n",
    "        start = start%self.block_size\n",
    "        end = (start + T)%(self.block_size + 1)\n",
    "\n",
    "        # start < end: It is a single cache update.\n",
    "        # end > start: It is a split cache update.\n",
    "        if start < end:\n",
    "            self.cache_k[:B, start:start+T].assign(xk)\n",
    "            self.cache_v[:B, start:start+T].assign(xv)\n",
    "        else:\n",
    "            # Update cache with partial sequence that fits towards the end.\n",
    "            self.cache_k[:B, start:].assign(xk[:, :-(end+1)])\n",
    "            self.cache_v[:B, start:].assign(xv[:, :-(end+1)])\n",
    "\n",
    "            # Splillover sequence is cached towards the front of the cache.\n",
    "            self.cache_k[:B, :end+1].assign(xk[:, -(end+1):])\n",
    "            self.cache_v[:B, :end+1].assign(xv[:, -(end+1):])\n",
    "\n",
    "    def get(self, batch_size, start, seq_len):\n",
    "        # Calculate update start and end positions\n",
    "        start = start%self.block_size\n",
    "        end = (start + seq_len)%(self.block_size + 1)\n",
    "\n",
    "        # start < end: It is a single cache fetch.\n",
    "        # end > start: It is a split cache fetch.\n",
    "        if start < end:\n",
    "            keys = self.cache_k[:batch_size, :start+seq_len]\n",
    "            values = self.cache_v[:batch_size, :start+seq_len]\n",
    "        else:\n",
    "            # Fetch sequence prefix\n",
    "            keys_1 = self.cache_k[:, (end+1):]\n",
    "            values_1 = self.cache_k[:, (end+1):]\n",
    "\n",
    "            # Fetch sequence suffix\n",
    "            keys_2 = self.cache_k[:, :(end+1):]\n",
    "            values_2 = self.cache_k[:, :(end+1):]\n",
    "\n",
    "            # Compose the whole sequence\n",
    "            keys = tf.concat([keys_1, keys_2], axis=1)\n",
    "            values = tf.concat([values_1, values_2], axis=1)\n",
    "\n",
    "        return keys, values\n",
    "\n",
    "def update_and_show(cache, batch_size, start, seqlen, data_shape, msg, debug=False):\n",
    "    xk = tf.random.uniform((batch_size, seqlen, *data_shape))\n",
    "    xv = tf.random.uniform((batch_size, seqlen, *data_shape))\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            f'\\n{msg}::xk:\\n{xk}'\n",
    "        )\n",
    "\n",
    "    cache.update(start, xk, xv)\n",
    "    print(\n",
    "        f'\\n{msg}:\\n{cache.cache_k}'\n",
    "    )\n",
    "\n",
    "# cache_size, block_size, heads, head_dims = 1, 8, 1, 4\n",
    "# cache = KVCache(cache_size, block_size, heads, head_dims)\n",
    "# data_shape = (heads, head_dims)\n",
    "\n",
    "# # print(f'\\nInitial:\\n{cache.cache_k}')\n",
    "\n",
    "# # update_and_show(cache, cache_size, 2, 4, data_shape, 'InUpdate(2, 4)')\n",
    "# # update_and_show(cache, cache_size, 4, 4, data_shape, 'EndUpdate(4, 4)')\n",
    "# # update_and_show(cache, cache_size, 6, 4, data_shape, 'SpilledUpdate(6, 4)', debug=True)\n",
    "# update_and_show(cache, cache_size, 6, 8, data_shape, 'Fill(6, 8)')\n",
    "# print(\n",
    "#     # f'\\nInQuery:\\n{cache.get(cache_size, 0, 4)[0]}'\n",
    "#     # f'\\n\\nEndQuery:\\n{cache.get(cache_size, 4, 4)[0]}'\n",
    "#     f'\\n\\nSpilledQuery:\\n{cache.get(cache_size, 6, 2)[0]}'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupedQueryAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class GroupedQueryAttention(tf.keras.Model):\n",
    "    def __init__(self, block_size, heads, kv_heads, dims, cache_size):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dims = dims\n",
    "        self.head_dims = dims // self.heads\n",
    "        self.kv_heads = kv_heads or heads\n",
    "        self.block_size = block_size\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        self.wq = layers.Dense(self.dims, use_bias=False)\n",
    "        self.wk = layers.Dense(self.kv_heads * self.head_dims, use_bias=False)\n",
    "        self.wv = layers.Dense(self.kv_heads * self.head_dims, use_bias=False)\n",
    "        self.wo = layers.Dense(self.dims, use_bias=False)\n",
    "        \n",
    "        self.cache = KVCache(self.cache_size, self.block_size, self.kv_heads, self.head_dims)\n",
    "        self.rope = RotaryPositionalEncodings(self.head_dims, self.block_size)\n",
    "    \n",
    "    def call(self, x, start, use_cache):\n",
    "        # print(f'{x=}\\n{start=}')\n",
    "        shape = tf.shape(x)\n",
    "        B = shape[0]\n",
    "        T = shape[1]\n",
    "\n",
    "        # (B, T, dims)\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # (B, T, heads/kv_heads, head_dims)\n",
    "        xq = tf.reshape(q, (B, T, self.heads, self.head_dims))\n",
    "        xk = tf.reshape(k, (B, T, self.kv_heads, self.head_dims))\n",
    "        xv = tf.reshape(v, (B, T, self.kv_heads, self.head_dims))\n",
    "\n",
    "        # Apply RoPE\n",
    "        # (B, T, heads/kv_heads, head_dims)\n",
    "        xq = self.rope(xq)\n",
    "        xk = self.rope(xk)\n",
    "\n",
    "        if use_cache:\n",
    "            # Update KV cache\n",
    "            self.cache.update(start, xk, xv)\n",
    "            \n",
    "            # Get prefix context from the cache.\n",
    "            # (B, start+T, kv_heads, head_dims)\n",
    "            keys, values = self.cache.get(B, start, T)\n",
    "        else:\n",
    "            assert start == 0\n",
    "            keys, values = xk, xv\n",
    "\n",
    "        # Expand kv_heads to heads\n",
    "        # (B, start+T, heads, head_dims)\n",
    "        # print(f'{keys.shape=} {values.shape=} {keys.dtype=}')\n",
    "        keys = tf.tile(keys, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "        values = tf.tile(values, multiples=(1, 1, self.heads//self.kv_heads, 1))\n",
    "        \n",
    "        # Transpose xq, keys and values to (B, heads, T/start+T, head_dims)\n",
    "        xq = tf.transpose(xq, perm=[0, 2, 1, 3])\n",
    "        xk = tf.transpose(keys, perm=[0, 2, 1, 3])\n",
    "        xv = tf.transpose(values, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # Multiply xq and xk to compute attention matrix.\n",
    "        # (B, heads, T, head_dims) @ (B, heads, head_dims, start+T) -> (B, heads, T, start+T)\n",
    "        xa = xq @ tf.transpose(xk, perm=[0, 1, 3, 2]) / math.sqrt(self.head_dims*1.)\n",
    "\n",
    "        # Compute softmax scores.\n",
    "        if use_cache:\n",
    "            scores = tf.nn.softmax(xa)\n",
    "        else:\n",
    "            # If cache is not used, apply auto-regressive mask to block forward looking\n",
    "            tril = tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)\n",
    "            scores = tf.math.softmax(tf.where(tril > 0.0, xa, float('-inf')))\n",
    "\n",
    "        # Scale Values and compute output\n",
    "        # (B, heads, T, start+T) @ (B, heads, start+T, head_dims) -> (B, heads, T, head_dims)\n",
    "        output = scores @ xv\n",
    "        \n",
    "        # Reshape output to (B, T, dims)\n",
    "        output = tf.reshape(\n",
    "            tf.transpose(output, perm=[0, 2, 1, 3]),\n",
    "            shape=(B, T, self.dims)\n",
    "        )\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "# l = GroupedQueryAttention(\n",
    "#     block_size=16,\n",
    "#     heads=4,\n",
    "#     kv_heads=2,\n",
    "#     dims=16,\n",
    "#     cache_size=4\n",
    "# )\n",
    "\n",
    "# B, T, C = 2, 2, 16\n",
    "# start = 1\n",
    "# x = tf.reshape(tf.range(B*T*C), (B, T, C))\n",
    "# output = l(x, start)\n",
    "\n",
    "# B_i, T_i, C_i = 0, 0, 3\n",
    "# H_i, F_i = 0, C_i\n",
    "\n",
    "# print(\n",
    "#     f'{x[B_i]=}'\n",
    "#     f'\\n{l.cache.cache_k.shape=}\\n{l.cache.cache_k[B_i, :start+T]=}'\n",
    "#     f'\\n{l.cache.cache_v[B_i, :start+T]=}'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
