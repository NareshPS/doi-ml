{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PositionEmbedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, initializer=\"glorot_uniform\", **kwargs ):\n",
    "        super().__init__(**kwargs)\n",
    "        self._initializer = tf.keras.initializers.get(initializer)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"initializer\": tf.keras.initializers.serialize(self._initializer),\n",
    "        }\n",
    "        base_config = super(PositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        sequence_length = input_shape[-2]\n",
    "        width = input_shape[-1]\n",
    "\n",
    "        self._position_embeddings = self.add_weight(\n",
    "            \"position_embeddings\",\n",
    "            shape=[sequence_length, width],\n",
    "            initializer=self._initializer)\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self._position_embeddings\n",
    "\n",
    "xx = tf.zeros((2, 4, 12))\n",
    "l = PositionEmbedding()\n",
    "output = l(xx)\n",
    "\n",
    "print('PositionEmbedding Layer')\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))\n",
    "print(f'Embedding Size: {l.weights[0].shape}')\n",
    "print(f'Verify Embeddings: {tf.reduce_all(tf.math.equal(output, l.weights))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StochasticDepth Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"It is sourced from tensorflow-models package\n",
    "\n",
    "Source: https://github.com/tensorflow/models/blob/v2.12.0/official/vision/modeling/layers/nn_layers.py#L227-L262\n",
    "\"\"\"\n",
    "class StochasticDepth(layers.Layer):\n",
    "    \"\"\"Creates a stochastic depth layer.\"\"\"\n",
    "\n",
    "    def __init__(self, stochastic_depth_drop_rate, **kwargs):\n",
    "        \"\"\"Initializes a stochastic depth layer.\n",
    "\n",
    "        Args:\n",
    "          stochastic_depth_drop_rate: A `float` of drop rate.\n",
    "          **kwargs: Additional keyword arguments to be passed.\n",
    "\n",
    "        Returns:\n",
    "          A output `tf.Tensor` of which should have the same shape as input.\n",
    "        \"\"\"\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self._drop_rate = stochastic_depth_drop_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'stochastic_depth_drop_rate': self._drop_rate}\n",
    "        base_config = super(StochasticDepth, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "        if not training or self._drop_rate is None or self._drop_rate == 0:\n",
    "            return inputs\n",
    "\n",
    "        keep_prob = 1.0 - self._drop_rate\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += tf.random.uniform(\n",
    "            [batch_size] + [1] * (inputs.shape.rank - 1), dtype=inputs.dtype)\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
    "        return output\n",
    "\n",
    "batch_size, num_patches, dims = 2, 256, 768\n",
    "drop_prob = .5\n",
    "\n",
    "l = StochasticDepth(drop_prob)\n",
    "xx = tf.random.normal((batch_size, num_patches, dims))\n",
    "output = l(xx, training=True)\n",
    "non_zeros = 1 - (tf.math.reduce_sum(tf.cast(output == 0, tf.int64))/(batch_size * num_patches * dims))\n",
    "\n",
    "print('StochasticDepth Layer')\n",
    "print('---------------------')\n",
    "print(f'batch_size: {batch_size}, num_patches: {num_patches}, dims: {dims}')\n",
    "print(f'\\ndrop_prob: {drop_prob}')\n",
    "print(f'\\nInput: {xx.shape} --> {output.shape}')\n",
    "print(f'\\nOutput Drop Rate: {non_zeros}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
