{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegmenterPolynomialDecay Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmenterPolynomialDecay(optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule that uses polynomial decay schedule described in the Segmenter paper. \n",
    "\n",
    "    The learning rate is computed as follows:\n",
    "\n",
    "    ```python\n",
    "    def decayed_learning_rate(step):\n",
    "        return learning_rate * (1 - step/total_steps)**power\n",
    "    ```\n",
    "\n",
    "    Example Usage:\n",
    "\n",
    "    ```python\n",
    "    learning_rate = 0.001\n",
    "    lr_schedule = SegmenterPolynomialDecay(\n",
    "        learning_rate,\n",
    "        total_steps,\n",
    "        power=0.9\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizers.SGD(learning_rate=lr_schedule),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(data, labels, epochs=5)\n",
    "    ```\n",
    "\n",
    "    Returns:\n",
    "      A 1-arg callable learning rate schedule that takes the current optimizer\n",
    "      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n",
    "      type as `learning_rate`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        total_steps,\n",
    "        power=0.9,\n",
    "        name=None,\n",
    "    ):\n",
    "        \"\"\"Applies polynomial decay to the learning rate.\n",
    "\n",
    "        Args:\n",
    "          learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
    "            Python number.  The initial learning rate.\n",
    "          total_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "            Must be positive.  See the decay computation above.\n",
    "          power: A scalar `float32` or `float64` `Tensor` or a\n",
    "            Python number.  See the decay computation above.\n",
    "          staircase: Boolean.  If `True` decay the learning rate at discrete\n",
    "            intervals\n",
    "          name: String.  Optional name of the operation.  Defaults to\n",
    "            'ExponentialDecay'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.total_steps = total_steps\n",
    "        self.power = power\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        learning_rate = tf.convert_to_tensor(self.learning_rate, name=\"learning_rate\")\n",
    "        dtype = learning_rate.dtype\n",
    "        total_steps = tf.cast(self.total_steps, dtype)\n",
    "        power = tf.cast(self.power, dtype)\n",
    "\n",
    "        step = tf.cast(step, dtype)\n",
    "        lr = learning_rate * ((1 - (step / total_steps)) ** power)\n",
    "\n",
    "        return lr\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"power\": self.power,\n",
    "            \"name\": self.name,\n",
    "        }\n",
    "\n",
    "learning_rate, total_steps = 0.001, 20\n",
    "lr_schedule = SegmenterPolynomialDecay(learning_rate, total_steps)\n",
    "learning_rates = list(map(lambda step: f'{lr_schedule(step):.8f}', range(total_steps)))\n",
    "print(f'Learning Rates: {learning_rates}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
