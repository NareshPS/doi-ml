{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Epoch Training Using Gradient Tape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean(metrics.Metric):\n",
    "    def __init__(self, name=\"mean\", **kwargs):\n",
    "        super(Mean, self).__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight(name='total_{}'.format(name), initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name='count_{}'.format(name), initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, result):\n",
    "        self.total.assign_add(result)\n",
    "        self.count.assign_add(1)\n",
    "\n",
    "    def result(self):\n",
    "        return self.total/self.count\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0)\n",
    "\n",
    "class History(object):\n",
    "    def __init__(self):\n",
    "        self.metrics = dict(\n",
    "            learning_rate = Mean(name='learning_rate'),\n",
    "\n",
    "            loss = Mean(name='loss'),\n",
    "            val_loss = Mean(name='val_loss'),\n",
    "\n",
    "            yx_loss = Mean(name='yx_loss'),\n",
    "            val_yx_loss = Mean(name='val_yx_loss'),\n",
    "\n",
    "            hw_loss = Mean(name='hw_loss'),\n",
    "            val_hw_loss = Mean(name='val_hw_loss'),\n",
    "\n",
    "            iou = Mean(name='iou'),\n",
    "            val_iou = Mean(name='val_iou'),\n",
    "\n",
    "            positive_iou = Mean(name='positive_iou'),\n",
    "            val_positive_iou = Mean(name='val_positive_iou'),\n",
    "\n",
    "            negative_iou = Mean(name='negative_iou'),\n",
    "            val_negative_iou = Mean(name='val_negative_iou'),\n",
    "        )\n",
    "        self.history = {name: [] for name, metric in self.metrics.items()}\n",
    "    \n",
    "    @property\n",
    "    def metric_names(self):\n",
    "        return list(self.metrics.keys())\n",
    "\n",
    "    @property\n",
    "    def training_metrics_names(self):\n",
    "        return list(filter(lambda name: not name.startswith('val_'), self.metrics.keys()))\n",
    "    \n",
    "    @property\n",
    "    def training_metrics(self):\n",
    "        return [(name, self.metrics[name].result()) for name in self.training_metrics_names]\n",
    "\n",
    "    @property\n",
    "    def metric_values(self):\n",
    "        return [(name, metric.result()) for name, metric in self.metrics.items()]\n",
    "    \n",
    "    def train_step(self, yx_loss, hw_loss, iou, positive_iou, negative_iou):\n",
    "        self.metrics['loss'].update_state(yx_loss + hw_loss)\n",
    "        self.metrics['yx_loss'].update_state(yx_loss)\n",
    "        self.metrics['hw_loss'].update_state(hw_loss)\n",
    "\n",
    "        self.metrics['iou'].update_state(iou)\n",
    "        self.metrics['positive_iou'].update_state(positive_iou)\n",
    "        self.metrics['negative_iou'].update_state(negative_iou)\n",
    "\n",
    "        return self.training_metrics\n",
    "    \n",
    "    def val_step(self, yx_loss, hw_loss, iou, positive_iou, negative_iou):\n",
    "        self.metrics['val_loss'].update_state(yx_loss + hw_loss)\n",
    "        self.metrics['val_yx_loss'].update_state(yx_loss)\n",
    "        self.metrics['val_hw_loss'].update_state(hw_loss)\n",
    "\n",
    "        self.metrics['val_iou'].update_state(iou)\n",
    "        self.metrics['val_positive_iou'].update_state(positive_iou)\n",
    "        self.metrics['val_negative_iou'].update_state(negative_iou)\n",
    "\n",
    "        return self.metric_values\n",
    "    \n",
    "    def learning_rate(self, lr_value):\n",
    "        self.metrics['learning_rate'].update_state(lr_value)\n",
    "    \n",
    "    def epoch(self):\n",
    "        # Record the current epoch values before reset.\n",
    "        values = self.metric_values\n",
    "\n",
    "        for name in self.metrics.keys():\n",
    "            self.record_and_reset(name)\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    def record_and_reset(self, name):\n",
    "        self.history[name].append(self.metrics[name].result().numpy())\n",
    "        self.metrics[name].reset_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        yx_loss, hw_loss = model.loss(y, logits)\n",
    "        loss = yx_loss + hw_loss\n",
    "        iou, positive_iou, negative_iou = compute_iou_metric(y, logits)\n",
    "\n",
    "    # Compute gradients and backpropagate.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    return yx_loss, hw_loss, iou, positive_iou, negative_iou\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, x, y):\n",
    "    logits = model(x, training=False)\n",
    "    yx_loss, hw_loss = model.loss(y, logits)\n",
    "    iou, positive_iou, negative_iou = compute_iou_metric(y, logits)\n",
    "    \n",
    "    return yx_loss, hw_loss, iou, positive_iou, negative_iou\n",
    "\n",
    "def train(model, tds, vds, epochs=100):\n",
    "    # Record progress\n",
    "    ckpt = tf.train.Checkpoint(optimizer=model.optimizer, model=model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, './sequence_of_bboxes', max_to_keep=3)\n",
    "    history = History()\n",
    "    \n",
    "    # tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    # vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE) if vds else None\n",
    "    tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE).take(2)\n",
    "    vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE).take(2) if vds else None\n",
    "    # tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE).take(1)\n",
    "    # vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE).take(1) if vds else None\n",
    "\n",
    "    history.learning_rate(LEARNING_RATE)\n",
    "    history.record_and_reset('learning_rate')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Track training progress\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch + 1, epochs))\n",
    "        p_bar = utils.Progbar(STEPS_PER_EPOCH, stateful_metrics=history.metric_names)\n",
    "        steps = 0\n",
    "\n",
    "        for step, (x, y) in enumerate(iter(tds)):\n",
    "            yx_loss, hw_loss, iou, positive_iou, negative_iou = train_step(model, x, y)\n",
    "\n",
    "            p_bar.update(step + 1, values=history.train_step(yx_loss, hw_loss, iou, positive_iou, negative_iou))\n",
    "            steps += 1\n",
    "        \n",
    "        # Record learning rates\n",
    "        history.learning_rate(model.optimizer.lr((epoch + 1)*STEPS_PER_EPOCH))\n",
    "        \n",
    "        for x, y in iter(vds):\n",
    "            yx_loss, hw_loss, iou, positive_iou, negative_iou = val_step(model, x, y)\n",
    "\n",
    "            history.val_step(yx_loss, hw_loss, iou, positive_iou, negative_iou)\n",
    "        \n",
    "        # Display metrics at the end of each epoch.\n",
    "        p_bar.update(steps, values=history.epoch())\n",
    "\n",
    "        # Save Checkpoint\n",
    "        print('\\nSaved Checkpoint: {}'.format(ckpt_manager.save()))\n",
    "\n",
    "\n",
    "    return history\n",
    "\n",
    "EPOCHS = 2\n",
    "# EPOCHS = 50\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "tds = train_prep_ds.batch(BATCH_SIZE)\n",
    "# vds = val_prep_ds.batch(256).cache()\n",
    "vds = val_prep_ds.batch(2).cache()\n",
    "\n",
    "hist = train(model, tds, vds, epochs=EPOCHS)\n",
    "hist.history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Pruning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "STEPS_PER_EPOCH = 805\n",
    "PRUNING_WEIGHT_RESET_EPOCHS = 1\n",
    "PRUNING_TRAINING_EPOCHS = 1\n",
    "LOG_DIR = './sequence_of_bboxes'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        yx_loss, hw_loss = model.loss(y, logits)\n",
    "        # loss = yx_loss + hw_loss\n",
    "        iou, positive_iou, negative_iou = compute_iou_metric(y, logits)\n",
    "        loss = 10*yx_loss + (1 - positive_iou) + (2 - negative_iou) + hw_loss\n",
    "\n",
    "\n",
    "    # Compute gradients and backpropagate.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    # grads = tape.gradient(yx_loss, model.trainable_weights)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    return yx_loss, hw_loss, iou, positive_iou, negative_iou\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, x, y):\n",
    "    logits = model(x, training=False)\n",
    "    yx_loss, hw_loss = model.loss(y, logits)\n",
    "    iou, positive_iou, negative_iou = compute_iou_metric(y, logits)\n",
    "\n",
    "    return yx_loss, hw_loss, iou, positive_iou, negative_iou\n",
    "\n",
    "# def auto_train(model, tds, vds, epochs=100, callbacks=[]):\n",
    "#     # tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#     # vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE) if vds else None\n",
    "#     tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE).take(1)\n",
    "#     vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE).take(1) if vds else None\n",
    "\n",
    "#     # tf.profiler.experimental.start('logdir')\n",
    "#     history = model.fit(tds, validation_data=vds, epochs=epochs, callbacks=callbacks)\n",
    "#     # tf.profiler.experimental.stop()\n",
    "\n",
    "#     return history\n",
    "\n",
    "\n",
    "def make_prunable_model(model, params):\n",
    "    print('Pruning is enabled with parameters: {}'.format(params))\n",
    "\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    kwargs = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(**params)\n",
    "    }\n",
    "\n",
    "    # Configure the model for pruning\n",
    "    pruning_model = prune_low_magnitude(model, **kwargs)\n",
    "    pruning_model.compile(\n",
    "        optimizer=model.optimizer,\n",
    "        loss=model.loss,\n",
    "        metrics=model.metrics\n",
    "    )\n",
    "    pruning_model.summary()\n",
    "\n",
    "    return pruning_model\n",
    "\n",
    "\n",
    "km = None\n",
    "\n",
    "class TrainingHooks(object):\n",
    "    def __init__(self, params):\n",
    "        none_fn = lambda: None\n",
    "        self.on_train_begin = params.get('on_train_begin') or none_fn\n",
    "\n",
    "def train(model, tds, vds, epochs=100, pruning_params=None):\n",
    "    global km\n",
    "\n",
    "    # tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    # vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE) if vds else iter([])\n",
    "    tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE).take(4)\n",
    "    vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE).take(\n",
    "        2) if vds else iter([])\n",
    "    # tds = tds.prefetch(buffer_size=tf.data.AUTOTUNE).take(1)\n",
    "    # vds = vds.prefetch(buffer_size=tf.data.AUTOTUNE).take(1) if vds else None\n",
    "\n",
    "    # Save initial weights and reset them after 'PRUNING_WEIGHT_RESET_EPOCHS' epochs\n",
    "    model.save_weights('initial_weights.h5')\n",
    "\n",
    "    # Configure pruning and callbacks\n",
    "    if(pruning_params):\n",
    "        m = make_prunable_model(model, pruning_params)\n",
    "        step_cb = tfmot.sparsity.keras.UpdatePruningStep()\n",
    "        step_cb.set_model(m)\n",
    "    else:\n",
    "        m = model\n",
    "\n",
    "    # Record progress\n",
    "    ckpt = tf.train.Checkpoint(optimizer=m.optimizer, model=m)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, LOG_DIR, max_to_keep=3)\n",
    "    history = History()\n",
    "\n",
    "    # Initialize training hooks\n",
    "    def on_train_begin():\n",
    "        # Record the initial learning rate at epoch 0\n",
    "        history.learning_rate(LEARNING_RATE)\n",
    "        history.record_and_reset('learning_rate')\n",
    "\n",
    "        # Signal start of training to the pruning callback\n",
    "        if (step_cb):\n",
    "            step_cb.on_train_begin()\n",
    "\n",
    "    hook_params = dict(\n",
    "        on_train_begin = on_train_begin\n",
    "    )\n",
    "    hooks = TrainingHooks(hook_params)\n",
    "\n",
    "    km = m\n",
    "    hooks.on_train_begin()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Track training progress\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch + 1, epochs))\n",
    "        p_bar = utils.Progbar(\n",
    "            STEPS_PER_EPOCH, stateful_metrics=history.metric_names)\n",
    "        steps = 0\n",
    "\n",
    "        for step, (x, y) in enumerate(iter(tds)):\n",
    "            if (step_cb):\n",
    "                step_cb.on_train_batch_begin(-1)\n",
    "\n",
    "            yx_loss, hw_loss, iou, positive_iou, negative_iou = train_step(\n",
    "                m, x, y)\n",
    "            # true_boxes = hw_grid_to_yxhw(y)\n",
    "            # tf.print('Step: ', step)\n",
    "            # tf.print('true_boxes: ', true_boxes, true_boxes.shape)\n",
    "            # tf.print('hw_loss: ', hw_loss, hw_loss.shape)\n",
    "\n",
    "            p_bar.update(step + 1, values=history.train_step(yx_loss,\n",
    "                         hw_loss, iou, positive_iou, negative_iou))\n",
    "            steps += 1\n",
    "\n",
    "        # Record learning rates\n",
    "        history.learning_rate(m.optimizer.lr((epoch + 1)*STEPS_PER_EPOCH))\n",
    "\n",
    "        for x, y in iter(vds):\n",
    "            yx_loss, hw_loss, iou, positive_iou, negative_iou = val_step(\n",
    "                m, x, y)\n",
    "\n",
    "            history.val_step(yx_loss, hw_loss, iou, positive_iou, negative_iou)\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        p_bar.update(steps, values=history.epoch())\n",
    "\n",
    "        # Signal epoch end to the pruning callback\n",
    "        if (step_cb):\n",
    "            step_cb.on_epoch_end(batch=-1)\n",
    "\n",
    "        # Save Checkpoint\n",
    "        print('\\nSaved Checkpoint: {}'.format(ckpt_manager.save()))\n",
    "\n",
    "        # Reset pruning weights after every PRUNING_WEIGHT_RESET_EPOCHS epochs\n",
    "        if (step_cb and ((epoch + 1) % PRUNING_WEIGHT_RESET_EPOCHS) == 0 and epoch < PRUNING_TRAINING_EPOCHS):\n",
    "            print('Reloading initial weights after {} epochs'.format(epoch + 1))\n",
    "            model.load_weights('initial_weights.h5', by_name=True)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "tds = train_prep_ds.batch(2)\n",
    "# vds = val_prep_ds.batch(256).cache()\n",
    "# vds = val_prep_ds.batch(2).cache()\n",
    "vds = None\n",
    "\n",
    "# hist = train(model, tds, vds, epochs=1)\n",
    "\n",
    "hist = train(model, tds, vds, epochs=3, pruning_params=dict(\n",
    "    initial_sparsity=0.50,\n",
    "    final_sparsity=0.80,\n",
    "    begin_step=STEPS_PER_EPOCH,\n",
    "    end_step=STEPS_PER_EPOCH*PRUNING_TRAINING_EPOCHS\n",
    "))\n",
    "hist.history"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "472baa808a066784c660228b7522f02c55b99d16f672674ca10b75b514659298"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
