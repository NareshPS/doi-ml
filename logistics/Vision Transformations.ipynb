{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize and Scale Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "\n",
    "@tf.function\n",
    "def resize_and_scale(item):\n",
    "    image, mask = item['image'], item['annotation']\n",
    "\n",
    "    # Resize image and mask to IMG_SIZE\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    mask = tf.image.resize(mask, [IMG_SIZE, IMG_SIZE], method='nearest')\n",
    "\n",
    "    # Normalize the image\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Extract semantic mask channel\n",
    "    mask = mask[..., :1]\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Images and Masks to Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(images, masks, patch_size):\n",
    "    patch_count = (images.shape[1]//patch_size)**2\n",
    "    sizes=[1, patch_size, patch_size, 1]\n",
    "    kwargs = dict(sizes=sizes, strides=sizes, rates=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "    # Extract patches from  images\n",
    "    image_patches = tf.image.extract_patches(images, **kwargs)\n",
    "    image_patches = tf.reshape(image_patches, [-1, patch_count, patch_size, patch_size, 3])\n",
    "\n",
    "    # Extract patches from  masks\n",
    "    box_patches = tf.image.extract_patches(masks, **kwargs)\n",
    "    box_patches = tf.reshape(box_patches, [-1, patch_count, patch_size, patch_size, 2])\n",
    "\n",
    "\n",
    "    return image_patches, box_patches\n",
    "\n",
    "images, masks = tf.reshape(tf.range(16*256*256*3), (16, 256, 256, 3)), tf.reshape(tf.range(16*256*256*2), (16, 256, 256, 2))\n",
    "# images, masks = tf.reshape(tf.range(4*4*3), (1, 4, 4, 3)), tf.reshape(tf.range(4*4*2), (1, 4, 4, 2))\n",
    "image_patches, box_patches = extract_patches(images, masks, patch_size=2)\n",
    "\n",
    "print('Image: {} --> Patches: {}'.format(images.shape, image_patches.shape))\n",
    "print('Box: {} --> Patches: {}'.format(masks.shape, box_patches.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Images to Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_patch(images, patch_size):\n",
    "    grid_size = images.shape[1]//patch_size\n",
    "    patch_count = grid_size**2\n",
    "    sizes=[1, patch_size, patch_size, 1]\n",
    "    kwargs = dict(sizes=sizes, strides=sizes, rates=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "    # Extract patches from  images\n",
    "    image_patches = tf.image.extract_patches(images, **kwargs)\n",
    "    image_patches = tf.reshape(image_patches, [-1, patch_count, patch_size, patch_size, 3])\n",
    "\n",
    "    return image_patches\n",
    "\n",
    "batch_size, img_size, patch_size = 2, 16, 4\n",
    "input_shape = (batch_size, img_size, img_size, 3)\n",
    "images = tf.reshape(tf.range(reduce(operator.mul, input_shape)), input_shape)\n",
    "\n",
    "image_patches = batch_to_patch(images, patch_size=patch_size)\n",
    "print('Image: {} --> Patches: {}'.format(images.shape, image_patches.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Standardization and Horizontal Flip Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_block = tf.keras.Sequential([\n",
    "    layers.Lambda(tf.image.per_image_standardization),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "], name='augmentation_block')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Mask Augmentations Using Albumenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmenter(size):\n",
    "    fn = A.Compose([\n",
    "        A.RandomResizedCrop(width=size, height=size, scale=(0.5, 2.0)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "    return fn\n",
    "\n",
    "batch_size, img_size, classes = 2, 8, 3\n",
    "image = tf.random.normal((img_size, img_size, 3))\n",
    "mask = tf.random.uniform((img_size, img_size, 1), maxval=classes, dtype=tf.int32)\n",
    "\n",
    "transform_fn = get_augmenter(4)\n",
    "result = transform_fn(image=image.numpy(), mask=mask.numpy())\n",
    "t_image, t_mask = result['image'], result['mask']\n",
    "\n",
    "print(f'Input Image: {image.shape} --> Transformed Image: {t_image.shape}')\n",
    "print(f'Input Mask: {mask.shape} --> Transformed Mask: {t_mask.shape}')\n",
    "\n",
    "show_related_images(image, t_image, mask, t_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Mask augmentation using Albumenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "AUG_FN = get_augmenter(IMG_SIZE)\n",
    "\n",
    "def transform_fn(image, mask):\n",
    "    result = AUG_FN(image=image, mask=mask)\n",
    "    return result['image'], result['mask']\n",
    "\n",
    "@tf.function\n",
    "def rescale_and_augment(item):\n",
    "    image, mask = item['image'], item['annotation']\n",
    "\n",
    "    # Normalize the image and extract mask\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    mask = mask[..., :1]\n",
    "\n",
    "    result = tf.numpy_function(transform_fn, [image, mask], [tf.float32, tf.uint8])\n",
    "    image = result[0]\n",
    "    mask = result[1]\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "train_prep_ds = train_ds.map(rescale_and_augment, num_parallel_calls=None)\n",
    "\n",
    "def show_preprocessing_results(ds, p_ds, title='Preprocessing Results'):\n",
    "    item = next(iter(ds))\n",
    "    image, mask = item['image'], item['annotation']\n",
    "\n",
    "    p_image, p_mask = next(iter(p_ds))\n",
    "\n",
    "    print('Image: {} Mask: {} --> Image: {} Mask: {}'.format(image.shape, mask.shape, p_image.shape, p_mask.shape))\n",
    "\n",
    "    show_related_images(image, mask, p_image, p_mask, title=title)\n",
    "\n",
    "print('Training Set Preprocessing')\n",
    "print('--------------------------')\n",
    "show_preprocessing_results(train_ds, train_prep_ds, title='Training Set')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
