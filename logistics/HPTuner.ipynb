{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import importlib as imp\n",
    "\n",
    "from collections import namedtuple\n",
    "from random import sample, shuffle\n",
    "from functools import reduce\n",
    "from itertools import accumulate\n",
    "from math import floor, ceil, sqrt, log, pi\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers, utils, losses, models as mds, optimizers\n",
    "\n",
    "if imp.util.find_spec('aggdraw'): import aggdraw\n",
    "if imp.util.find_spec('tensorflow_addons'): from tensorflow_addons import layers as tfa_layers\n",
    "if imp.util.find_spec('tensorflow_models'): from official.vision.beta.ops import augment as visaugment\n",
    "if imp.util.find_spec('tensorflow_probability'): from tensorflow_probability import distributions as tfd\n",
    "if imp.util.find_spec('keras_tuner'): import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset image size\n",
    "IMG_SIZE = 264\n",
    "N_CLASSES = 102\n",
    "\n",
    "def preprocess(image, *args):\n",
    "    image = tf.image.resize_with_pad(image, IMG_SIZE, IMG_SIZE)\n",
    "    image /= 255\n",
    "    return (image, *args)\n",
    "\n",
    "train_ds, val_ds = tfds.load(\n",
    "    'oxford_flowers102',\n",
    "    split=['train', 'validation'],\n",
    "    as_supervised=True,\n",
    "    read_config=tfds.ReadConfig(try_autocache=False)\n",
    ")\n",
    "\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=0.001):\n",
    "    m = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.MaxPool2D(2),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPool2D(2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(N_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(lr)\n",
    "    m.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "    return m\n",
    "\n",
    "def create_with_params(lr):\n",
    "    return create_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_hp():\n",
    "    def build_model(hp):\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        bs = hp.Choice('bs', [8, 16, 24, 32])\n",
    "        m = create_with_params(lr=lr)\n",
    "\n",
    "        return m\n",
    "    \n",
    "    return build_model\n",
    "\n",
    "class RandomSearchWithBatch(kt.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        augs = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "        ])\n",
    "        print(trial.hyperparameters.values)\n",
    "        train_ds = args[0]\n",
    "        val_ds = kwargs.get('validation_data')\n",
    "        batch_size = trial.hyperparameters['bs']\n",
    "\n",
    "        tds = train_ds.shuffle(batch_size*20, reshuffle_each_iteration=True).batch(batch_size)\n",
    "        tds = tds.map(lambda x,y: (augs(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE).take(1)\n",
    "        vds = val_ds.batch(batch_size).cache().take(1) if val_ds else None\n",
    "\n",
    "        # Update Trial Arguments\n",
    "        args = tds, *args[1:]\n",
    "        kwargs['validation_data'] = vds\n",
    "\n",
    "        return super(RandomSearchWithBatch, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "# Tune Hyper Parameters\n",
    "tuner = RandomSearchWithBatch(build_model_with_hp(), objective='val_loss', max_trials=7)\n",
    "tuner.search(train_ds, epochs=10, validation_data=val_ds)\n",
    "\n",
    "# Select best parameters\n",
    "model = tuner.get_best_models()[0]\n",
    "BATCH_SIZE = tuner.get_best_hyperparameters()[0]['bs']\n",
    "tuner.results_summary()\n",
    "tuner.get_best_hyperparameters()[0].values"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
