{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import importlib as imp\n",
    "\n",
    "from collections import namedtuple\n",
    "from random import sample, shuffle\n",
    "from functools import reduce\n",
    "from itertools import accumulate\n",
    "from math import floor, ceil, sqrt, log, pi\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers, utils, losses, models as mds, optimizers\n",
    "\n",
    "if imp.util.find_spec('aggdraw'): import aggdraw\n",
    "if imp.util.find_spec('tensorflow_addons'): from tensorflow_addons import layers as tfa_layers\n",
    "if imp.util.find_spec('tensorflow_models'): from official.vision.beta.ops import augment as visaugment\n",
    "if imp.util.find_spec('tensorflow_probability'): from tensorflow_probability import distributions as tfd\n",
    "if imp.util.find_spec('keras_tuner'): import keras_tuner as kt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 10:08:20.808747: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/broxoli/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-03-30 10:08:20.809754: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-30 10:08:20.811286: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-19319V3): /proc/driver/nvidia/version does not exist\n",
      "2022-03-30 10:08:20.857598: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2480      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,501\n",
      "Trainable params: 2,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(20, 10)),\n",
    "    layers.LSTM(20),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_WIDTH = 64\n",
    "DROPOUT_RATE = 0.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(x, g, dims, name='attention'):\n",
    "    \"\"\"\n",
    "        x: Feature map from skip connection.\n",
    "        g: Feature map from the last layer. It has smaller spatial dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform x with strided convolution to match gating dimensions\n",
    "    stride_x = x.shape[1]//g.shape[1]\n",
    "    # print('Initial Shapes: ', x.shape, g.shape, stride_x)\n",
    "    x_transform = layers.Conv2D(dims, 3, (stride_x, stride_x), padding=\"same\")(x)\n",
    "\n",
    "    # Transform g with 1x1 convolution\n",
    "    g_transform = layers.Conv2D(dims, 1, padding=\"same\")(g)\n",
    "\n",
    "    if DEBUG:\n",
    "        print('x_transform: ', x_transform.shape, 'g_transform: ', g_transform.shape)\n",
    "\n",
    "    # Combine transformed x and g and transform them to a single channel output\n",
    "    # which will be used to scale the input x. A sigmoid function is applied\n",
    "    # to the comination to ensure scaling factors in range [0,1)\n",
    "    x_g_combined = layers.Add()([x_transform, g_transform])\n",
    "    x_g_combined = layers.Activation('relu')(x_g_combined)\n",
    "\n",
    "    if DEBUG:\n",
    "        print('x_g_combined: ', x_g_combined.shape)\n",
    "    # x_g_combined = layers.Activation('relu')(x_transform + g_transform)\n",
    "    x_g_collapsed = layers.Conv2D(1, 1, padding='same', activation=\"relu\")(x_g_combined)\n",
    "\n",
    "    # Match the computed weights to the input x.\n",
    "    attention_weights = layers.UpSampling2D((stride_x, stride_x))(x_g_collapsed)\n",
    "    \n",
    "    if DEBUG: print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "    # Scale input x with attention\n",
    "    attended_x = layers.Multiply()([attention_weights, x])\n",
    "\n",
    "    if DEBUG: print('attended_x: ', attended_x.shape)\n",
    "\n",
    "    # Apply another convolution to compute the output\n",
    "    output = layers.Conv2D(x.shape[-1], 1, padding='same')(attended_x)\n",
    "    output = layers.BatchNormalization()(output)\n",
    "\n",
    "    block = tf.keras.Model(inputs=[x, g], outputs=output, name=name)\n",
    "\n",
    "    # if DEBUG:\n",
    "    #     print('attention_block: ', output.shape)\n",
    "        \n",
    "    return block([x, g])\n",
    "\n",
    "# x = tf.random.normal((1, 32, 32, 64))\n",
    "# g = tf.random.normal((1, 16, 16, 128))\n",
    "\n",
    "# x = tf.keras.Input((32, 32, 64))\n",
    "# g = tf.keras.Input((16, 16, 128))\n",
    "# DEBUG = True\n",
    "# attention_block(x, g, 32)\n",
    "\n",
    "# for l in att.layers[2:]:\n",
    "#     print(l)\n",
    "#     tfmot.sparsity.keras.prune_low_magnitude(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(input, width_multiplier, name='block'):\n",
    "    kwargs = dict(activation=\"relu\", padding=\"same\")\n",
    "    block = tf.keras.Sequential([\n",
    "        layers.Conv2D(INITIAL_WIDTH*width_multiplier, (3, 3), **kwargs),\n",
    "        layers.Conv2D(INITIAL_WIDTH*width_multiplier, (3, 3), **kwargs),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(DROPOUT_RATE)\n",
    "    ], name=name)\n",
    "\n",
    "    return block(input)\n",
    "\n",
    "def decoder_block(input, skip_input, width_multiplier, name='block'):\n",
    "    kwargs = dict(activation=\"relu\", padding=\"same\")\n",
    "\n",
    "    # Apply attention to the skip input\n",
    "    attended_skip_input = attention_block(skip_input, input, INITIAL_WIDTH*width_multiplier, name='attention_{}'.format(name))\n",
    "\n",
    "    conv_transpose = layers.Conv2DTranspose(\n",
    "        INITIAL_WIDTH*width_multiplier, (3, 3), strides=(2, 2), padding='same')(input)\n",
    "\n",
    "    crop_size = conv_transpose.shape[1]\n",
    "\n",
    "    if DEBUG: print('crop_size: ', crop_size, 'attended_skip_input: ', attended_skip_input.shape)\n",
    "\n",
    "    conv_input = layers.Concatenate()([conv_transpose, attended_skip_input])\n",
    "\n",
    "    conv_1 = layers.Conv2D(INITIAL_WIDTH*width_multiplier,\n",
    "                           (3, 3), **kwargs)(conv_input)\n",
    "    conv_2 = layers.Conv2D(INITIAL_WIDTH*width_multiplier,\n",
    "                           (3, 3), **kwargs)(conv_1)\n",
    "    normalization = layers.BatchNormalization()(conv_2)\n",
    "    dropout = layers.Dropout(DROPOUT_RATE)(normalization)\n",
    "\n",
    "    block = tf.keras.Model(inputs=[input, attended_skip_input], outputs=dropout, name=name)\n",
    "\n",
    "    return block([input, attended_skip_input])\n",
    "\n",
    "# x = tf.keras.Input((32, 32, 64))\n",
    "# g = tf.keras.Input((16, 16, 128))\n",
    "# DEBUG = True\n",
    "# decoder_block(g, x, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENCODERS = 2\n",
    "# LEARNING_RATE = 0.0003\n",
    "LEARNING_RATE = 0.001\n",
    "LR_DECAY_EPOCHS = 10\n",
    "LR_DECAY_RATE = .5\n",
    "STEPS_PER_EPOCH = 805\n",
    "# MAX_BOXES = 500\n",
    "PRUNING_WEIGHT_RESET_EPOCHS = 1\n",
    "PRUNING_TRAINING_EPOCHS = 1\n",
    "LOG_DIR = './sequence_of_bboxes'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Metrics and Model Assembly\n",
    "\n",
    "* The resize block reformats the UNet output as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_block(input):\n",
    "    [y_input, x_input, h_input, w_input] = layers.Lambda(lambda x: tf.split(x, 4, axis=-1))(input)\n",
    "\n",
    "    initial_channel_width = y_input.shape[1]\n",
    "    kwargs = dict(padding='same')\n",
    "    y_leg = tf.keras.Sequential([\n",
    "        layers.Conv2D(initial_channel_width*2, 3, activation='relu', **kwargs),\n",
    "        layers.MaxPool2D((1, initial_channel_width*2), strides=(1, initial_channel_width*2), **kwargs),\n",
    "        layers.Conv2D(MAX_BOXES, 3, activation='sigmoid', **kwargs),\n",
    "        layers.MaxPool2D((initial_channel_width*2, 1), strides=(initial_channel_width*2, 1), **kwargs),\n",
    "        layers.Reshape([MAX_BOXES, -1]),\n",
    "    ], name='y_leg')\n",
    "\n",
    "    x_leg = tf.keras.Sequential([\n",
    "        layers.Conv2D(initial_channel_width*2, 3, activation='relu', **kwargs),\n",
    "        layers.MaxPool2D((initial_channel_width*2, 1), strides=(initial_channel_width*2, 1), **kwargs),\n",
    "        layers.Conv2D(MAX_BOXES, 3, activation='sigmoid', **kwargs),\n",
    "        layers.MaxPool2D((1, initial_channel_width*2), strides=(1, initial_channel_width*2), **kwargs),\n",
    "        layers.Reshape([MAX_BOXES, -1]),\n",
    "    ], name='x_leg')\n",
    "\n",
    "    h_leg = tf.keras.Sequential([\n",
    "        layers.Conv2D(initial_channel_width*2, 3, activation='relu', **kwargs),\n",
    "        layers.AveragePooling2D((1, initial_channel_width*2), strides=(1, initial_channel_width*2), **kwargs),\n",
    "        layers.Conv2D(MAX_BOXES, 3, activation='sigmoid', **kwargs),\n",
    "        layers.AveragePooling2D((initial_channel_width*2, 1), strides=(initial_channel_width*2, 1), **kwargs),\n",
    "        layers.Reshape([MAX_BOXES, -1]),\n",
    "    ], name='h_leg')\n",
    "\n",
    "    w_leg = tf.keras.Sequential([\n",
    "        layers.Conv2D(initial_channel_width*2, 3, activation='relu', **kwargs),\n",
    "        layers.AveragePooling2D((initial_channel_width*2, 1), strides=(initial_channel_width*2, 1), **kwargs),\n",
    "        layers.Conv2D(MAX_BOXES, 3, activation='sigmoid', **kwargs),\n",
    "        layers.AveragePooling2D((1, initial_channel_width*2), strides=(1, initial_channel_width*2), **kwargs),\n",
    "        layers.Reshape([MAX_BOXES, -1]),\n",
    "    ], name='w_leg')\n",
    "\n",
    "    output = layers.Concatenate()([\n",
    "        y_leg(y_input), \n",
    "        x_leg(x_input), \n",
    "        h_leg(h_input), \n",
    "        w_leg(w_input), \n",
    "    ])\n",
    "\n",
    "    return output\n",
    "\n",
    "def create_model():\n",
    "    inp = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    encoder_blocks = list(accumulate([inp] + list(range(1, N_ENCODERS+1)), lambda x,\n",
    "                                     idx: encoder_block(x, idx, name='encoder_{}'.format(idx))))\n",
    "\n",
    "    # Remove the initial input from the list of encoder blocks\n",
    "    encoder_blocks.pop(0)\n",
    "\n",
    "    # Create the mid block. It is kept separate from the encoder blocks\n",
    "    # because it doesn't have a corresponding decoder block.\n",
    "    mid_block = encoder_block(\n",
    "        encoder_blocks[-1], N_ENCODERS+1, name='bottom_of_u')\n",
    "\n",
    "    decoder_blocks = list(accumulate([mid_block] + list(enumerate(reversed(encoder_blocks))), lambda x, item: decoder_block(\n",
    "        x, item[1], N_ENCODERS - item[0], 'decoder_{}'.format(item[0]+1))))\n",
    "\n",
    "    # Remove the mid block from the list of decoder blocks\n",
    "    decoder_blocks.pop(0)\n",
    "\n",
    "    output = resize_block(decoder_blocks[-1])\n",
    "    m = tf.keras.Model(inputs=inp, outputs=output)\n",
    "    lr_schedule = optimizers.schedules.InverseTimeDecay(\n",
    "        LEARNING_RATE,\n",
    "        decay_steps=STEPS_PER_EPOCH*LR_DECAY_EPOCHS,\n",
    "        decay_rate=LR_DECAY_RATE,\n",
    "        staircase=False)\n",
    "    optimizer = optimizers.Adam(lr_schedule)\n",
    "    # loss = compute_hw_loss\n",
    "    # loss = 'mae'\n",
    "    # metrics = ['accuracy']\n",
    "    loss = compute_yxhw_loss\n",
    "    metrics = []\n",
    "    m.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return m\n",
    "\n",
    "lr_schedule = optimizers.schedules.InverseTimeDecay(\n",
    "        LEARNING_RATE,\n",
    "        decay_steps=41,\n",
    "        decay_rate=LR_DECAY_RATE,\n",
    "        staircase=False)\n",
    "optimizer = optimizers.Adam(lr_schedule)\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=optimizer, loss=compute_yxhw_loss)\n",
    "model.summary()\n",
    "\n",
    "utils.plot_model(model, show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Standardization Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization_block(xx, name='standardization_block'):\n",
    "    block = tf.keras.Sequential([\n",
    "        layers.Lambda(tf.image.per_image_standardization),\n",
    "        # layers.RandomFlip(\"horizontal\"),\n",
    "    ], name=name)\n",
    "\n",
    "    return block(xx)\n",
    "\n",
    "images, _ = next(iter(train_prep_ds.batch(10)))\n",
    "standardized_images = standardization_block(images)\n",
    "\n",
    "print(f'Input Shape: {images.shape} --> Output Shape {standardized_images.shape}')\n",
    "\n",
    "show_related_images(images, standardized_images, batch_to_rows=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Perceptron\n",
      "dims: 8 dropout_rate: 0.2\n",
      "Input: (2, 4, 4, 3) --> (2, 4, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "def mlp_block(xx, dims, name='mlp_block'):\n",
    "    block = tf.keras.Sequential([\n",
    "        layers.Dense(dims*4, activation='gelu', use_bias=False),\n",
    "        layers.Dense(dims, use_bias=False),\n",
    "    ], name=name)\n",
    "\n",
    "    return block(xx)\n",
    "\n",
    "dims, dropout_rate = 8, .2\n",
    "xx = tf.random.normal((2, 4, 4, 3))\n",
    "output = mlp_block(xx, dims)\n",
    "\n",
    "print('Multilayer Perceptron')\n",
    "print('dims: {} dropout_rate: {}'.format(dims, dropout_rate))\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embedding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_embedding_block(xx, dims, name='patch_embedding_block'):\n",
    "    \"\"\"\n",
    "        It embeds the input patches in dims-dimensional space.\n",
    "\n",
    "        Arguments:\n",
    "            xx: A tensor with shape:\n",
    "                (BATCH_SIZE, N_PATCHES, PATCH_SIZE*PATCH_SIZE*N_CHANNELS)\n",
    "            dims: Latent dimensions in the embedding space.\n",
    "        \n",
    "        Returns:\n",
    "            A tensor with shape:\n",
    "                (BATCH_SIZE, N_PATCHES, dims)\n",
    "    \"\"\"\n",
    "    block = tf.keras.Sequential([\n",
    "        layers.Dense(dims, use_bias=False)\n",
    "    ], name=name)\n",
    "\n",
    "    return block(xx)\n",
    "\n",
    "dims = 8\n",
    "xx = tf.random.normal((2, 4, 12))\n",
    "output = patch_embedding_block(xx, dims)\n",
    "\n",
    "print('Patch Embedding Block')\n",
    "print('dims: {}'.format(dims))\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Block\n",
      "dims: 8\n",
      "Input: (None, 4, 8) --> (None, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "def encoder_block(xx, dims, heads, dropout_rate=0.2, name='encoder_block'):\n",
    "\n",
    "    normalized_1 = layers.LayerNormalization()(xx)\n",
    "    attention_layer = layers.MultiHeadAttention(heads, dims//heads, dropout=dropout_rate, use_bias=False)\n",
    "    x = xx + attention_layer(normalized_1, normalized_1)\n",
    "    normalized_2 = layers.LayerNormalization()(x)\n",
    "    x = x + mlp_block(normalized_2, dims, name='{}_mlp'.format(name))\n",
    "\n",
    "    block = tf.keras.Model(inputs=xx, outputs=x, name=name)\n",
    "    # block.summary()\n",
    "\n",
    "    return block(xx)\n",
    "\n",
    "# dims, heads, dropout_rate = 8, 2, .2\n",
    "dims, heads, dropout_rate = 768, 12, .2\n",
    "# xx = tf.random.normal((2, 4, dims))\n",
    "# xx = tf.keras.Input((4, dims))\n",
    "xx = tf.keras.Input((256, 768))\n",
    "output = encoder_block(xx, dims, heads, dropout_rate=dropout_rate)\n",
    "\n",
    "print('Encoder Block')\n",
    "print('dims: {}'.format(dims))\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stochastic Depth Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(xx, dims, heads, drop_prob=0.1, name='encoder_block'):\n",
    "    # Save the input 'xx' to use as skip connections later on.\n",
    "    skip_connection_1 = xx\n",
    "\n",
    "    ### Attention Computations [Start] ###\n",
    "    x = layers.LayerNormalization()(xx)\n",
    "    x = layers.MultiHeadAttention(heads, dims//heads, use_bias=False)(x, x)\n",
    "    x = StochasticDepth(drop_prob, name=f'stochastic_depth_1_{name}')(x)\n",
    "    x = x + skip_connection_1\n",
    "    ### Attention Computations [End] ###\n",
    "\n",
    "    # Save current 'x' to use as skip connections later on.\n",
    "    skip_connection_2 = x\n",
    "\n",
    "    ### MLP Computations [Start] ###\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = mlp_block(x, dims, name='{}_mlp'.format(name))\n",
    "    x = StochasticDepth(drop_prob, name=f'stochastic_depth_2_{name}')(x)\n",
    "    x = x + skip_connection_2\n",
    "    ### MLP Computations [End] ###\n",
    "\n",
    "    block = tf.keras.Model(inputs=xx, outputs=x, name=name)\n",
    "    # block.summary()\n",
    "\n",
    "    return block(xx)\n",
    "\n",
    "# dims, heads, dropout_rate = 8, 2, .2\n",
    "num_patches, dims, heads, drop_prob = 256, 768, 12, .2\n",
    "xx = tf.keras.Input((num_patches, dims))\n",
    "output = encoder_block(xx, dims, heads, drop_prob=drop_prob)\n",
    "\n",
    "print('Encoder Block')\n",
    "print('dims: {}'.format(dims))\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))\n",
    "\n",
    "# model = tf.keras.Model(inputs=xx, outputs=output)\n",
    "# xx = tf.random.normal((2, num_patches, dims))\n",
    "# model(xx, training=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_decoder_block(xx, classes, grid_size, img_size, name='linear_decoder_block'):\n",
    "    # Patch level class logits\n",
    "    x = layers.Dense(classes)(xx)\n",
    "\n",
    "    # Patch sequence to H/P x W/P grid\n",
    "    x = layers.Reshape([grid_size, grid_size, classes])(x)\n",
    "    x = tf.image.resize(x, [img_size, img_size])\n",
    "    x = layers.Activation('softmax')(x)\n",
    "\n",
    "    block = tf.keras.Model(inputs=xx, outputs=x, name=name)\n",
    "    # block.summary()\n",
    "\n",
    "    return block(xx)\n",
    "\n",
    "sequence_length = 16\n",
    "classes, grid_size, img_size = 8, 4, 16\n",
    "xx = tf.keras.Input((sequence_length, dims))\n",
    "output = linear_decoder_block(xx, classes, grid_size, img_size)\n",
    "block = tf.keras.Model(inputs=xx, outputs=output)\n",
    "\n",
    "xx = tf.random.normal((2, sequence_length, dims))\n",
    "output = block(xx)\n",
    "\n",
    "print('Linear Decoder Block')\n",
    "print('sequence_length: {}'.format(sequence_length))\n",
    "print('classes: {} grid_size: {} img_size: {}'.format(classes, grid_size, img_size))\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))\n",
    "# print(output[0, 0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assembly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size, patch_size, classes, encoder_units, dims, heads, dropout_rate=0.2):\n",
    "    grid_size = img_size//patch_size\n",
    "    xx = tf.keras.Input((img_size, img_size, 3))\n",
    "    \n",
    "    # x = augmentation_block(xx)\n",
    "    x = standardization_block(xx, name='standardization_block')\n",
    "    x = layers.Lambda(lambda images: batch_to_patch(images, patch_size), name='patching')(x)\n",
    "    x = layers.Reshape([grid_size**2, -1])(x)\n",
    "    x = patch_embedding_block(x, dims, name='patch_embedding')\n",
    "    x = PositionEmbedding(name='position_embedding')(x)\n",
    "\n",
    "    encoder_fn = lambda x, i: encoder_block(x, dims, heads, name='encoder_{}'.format(i + 1))\n",
    "    x = reduce(encoder_fn, range(encoder_units), x)\n",
    "\n",
    "    x = linear_decoder_block(x, classes, grid_size, img_size)\n",
    "\n",
    "    model = tf.keras.Model(inputs=xx, outputs=x, name='PatchTransformer')\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# encoder_units, dims, heads = 12, 192, 3\n",
    "encoder_units, dims, heads = 12, 768, 12\n",
    "img_size, patch_size, classes = 256, 16, 150\n",
    "\n",
    "print('Baseline Segmenter')\n",
    "print('img_size: {}, patch_size: {} classes: {}'.format(\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    classes\n",
    "))\n",
    "print('encoder_units: {}, dims: {} heads: {}'.format(\n",
    "    encoder_units,\n",
    "    dims,\n",
    "    heads\n",
    "))\n",
    "\n",
    "model = create_model(img_size, patch_size, classes, encoder_units, dims, heads)\n",
    "\n",
    "xx = tf.keras.Input((img_size, img_size, 3))\n",
    "output = model(xx)\n",
    "\n",
    "print('Input: {} --> {}'.format(xx.shape, output.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stochastic Depth Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size, patch_size, classes, encoder_units, dims, heads, sdr=0.1):\n",
    "    grid_size = img_size//patch_size\n",
    "    xx = tf.keras.Input((img_size, img_size, 3))\n",
    "    \n",
    "    x = standardization_block(xx, name='standardization_block')\n",
    "    x = layers.Lambda(lambda images: batch_to_patch(images, patch_size), name='patching')(x)\n",
    "    x = layers.Reshape([grid_size**2, -1])(x)\n",
    "    x = patch_embedding_block(x, dims, name='patch_embedding')\n",
    "    x = PositionEmbedding(name='position_embedding')(x)\n",
    "\n",
    "    drop_prob_fn = lambda index: (sdr / (encoder_units - 1)) * index\n",
    "    name_fn = lambda index: 'encoder_{}'.format(index + 1)\n",
    "    encoder_fn = lambda x, i: encoder_block(x, dims, heads, drop_prob=drop_prob_fn(i), name=name_fn(i))\n",
    "\n",
    "    x = reduce(encoder_fn, range(encoder_units), x)\n",
    "\n",
    "    x = linear_decoder_block(x, classes, grid_size, img_size)\n",
    "\n",
    "    model = tf.keras.Model(inputs=xx, outputs=x, name='PatchTransformer')\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# encoder_units, dims, heads = 12, 192, 3\n",
    "stochastic_depth_rate = 0.1\n",
    "encoder_units, dims, heads = 12, 768, 12\n",
    "img_size, patch_size, classes = 256, 16, 150\n",
    "\n",
    "print('Baseline Segmenter')\n",
    "print('------------------')\n",
    "print('\\nimg_size: {}, patch_size: {} classes: {}'.format(\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    classes\n",
    "))\n",
    "print('\\nencoder_units: {}, dims: {} heads: {} stochastic_depth_rate: {}'.format(\n",
    "    encoder_units,\n",
    "    dims,\n",
    "    heads,\n",
    "    stochastic_depth_rate\n",
    "))\n",
    "\n",
    "model = create_model(\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    classes,\n",
    "    encoder_units,\n",
    "    dims,\n",
    "    heads,\n",
    "    sdr=stochastic_depth_rate\n",
    ")\n",
    "\n",
    "xx = tf.keras.Input((img_size, img_size, 3))\n",
    "print('')\n",
    "output = model(xx)\n",
    "\n",
    "print('\\nInput: {} --> {}'.format(xx.shape, output.shape))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "472baa808a066784c660228b7522f02c55b99d16f672674ca10b75b514659298"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
