{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install dropbox\n!pip install GPUtil\n# !pip install tensorflow_data_validation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T05:21:26.731972Z","iopub.execute_input":"2022-02-03T05:21:26.732305Z","iopub.status.idle":"2022-02-03T05:21:47.047484Z","shell.execute_reply.started":"2022-02-03T05:21:26.732216Z","shell.execute_reply":"2022-02-03T05:21:47.046326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers, optimizers, metrics, losses\nfrom tensorflow.keras.utils import plot_model\n\nfrom IPython.display import display\nfrom os import path\nfrom pathlib import Path\nfrom time import time\nfrom tqdm import tqdm\nfrom functools import reduce\nfrom shutil import make_archive, unpack_archive, rmtree\nfrom GPUtil import showUtilization as gpu_usage","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:21:47.049832Z","iopub.execute_input":"2022-02-03T05:21:47.050225Z","iopub.status.idle":"2022-02-03T05:21:54.397917Z","shell.execute_reply.started":"2022-02-03T05:21:47.050176Z","shell.execute_reply":"2022-02-03T05:21:54.396966Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"%%capture\n(train_ds, val_ds), ds_info = tfds.load(\n    'voc',\n    with_info=True,\n    split=['train', 'validation'],\n    as_supervised=False,\n    read_config=tfds.ReadConfig(try_autocache=False)\n)\n\n# tf.print(ds_info.features['objects']['bbox'].encode_example({'xmax': .1, 'ymax': .2, 'xmin': .5, 'ymin': .2}))\n# tf.print(ds_info.features['objects']['bbox'].encode_example([.2]))\n\n# train_ds = train_ds.map(lambda item: (item['description'], item['label']))\n# ds_info","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:21:54.399171Z","iopub.execute_input":"2022-02-03T05:21:54.399417Z","iopub.status.idle":"2022-02-03T05:22:43.471176Z","shell.execute_reply.started":"2022-02-03T05:21:54.399388Z","shell.execute_reply":"2022-02-03T05:22:43.470263Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\nGRID_SIZE = 7\nN_ANCHORS = 3\n# ANCHORS = [(20, 20), (100, 80), (200, 180)]\n# Anchor box width and height found in https://fairyonice.github.io/Part_1_Object_Detection_with_Yolo_for_VOC_2014_data_anchor_box_clustering.html\nANCHORS = tf.constant([\n    [0.08285376, 0.13705531],\n    [0.20850361, 0.39420716],\n    [0.80552421, 0.77665105]\n])\nANCHORS = tf.stack([ANCHORS[..., 1], ANCHORS[..., 0]], axis=-1)#*IMG_SIZE\nMAX_BOXES = 10\n\ndef prep_boxes(images, boxes, labels, box_counts):\n    x1, y1, x2, y2 = tf.split(boxes, boxes.shape[-1], axis=-1)\n    \n    # Compute centers for the boxes wrt the image\n    cx = ((x1 + x2)*IMG_SIZE)/2\n    cy = ((y1 + y2)*IMG_SIZE)/2\n    centers = tf.concat([cy, cx], axis=-1)\n    \n    # Compute sizes for the boxes wrt the image\n    sx = (x2 - x1)*IMG_SIZE\n    sy = (y2 - y1)*IMG_SIZE\n    sizes = tf.concat([sy, sx], axis=-1)\n    \n    return images, centers, sizes, labels, box_counts\n\ndef resize(image, boxes):\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method='bilinear') # For downsizing\n    image = tf.cast(image, tf.uint8)\n    \n    # TODO: Resize boxes based on image resize\n    \n    return (image, boxes)\n\ndef to_corners(boxes):\n    \"\"\"Changes boxes from (CY, CX, BH, BW) format to corner format (TOP, LEFT, BOTTOM, RIGHT).\n\n    Arguments:\n        boxes: TensorSpec([N_BOXES, 4], tf.float32)\n        The values in the boxes tensor are: (CY, CX, H, W)\n        CY, CX: The coordinates of the box center.\n        BH, BW: Box height and width\n\n    Returns:\n        boxes: [N_BOXES, 4]\n        The values in the boxes are (TOP, LEFT, BOTTOM, RIGHT) which represent\n        the coordinates of the boxes.\n    \"\"\"\n    return tf.concat(\n        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n        axis=-1,\n    )\n\ndef box_ious(box, candidates):\n    \"\"\"Computes the IOUs of the box with the candidates.\n    Adopted from: https://keras.io/examples/vision/retinanet/#preprocessing-data\n    \n    Args:\n        box: TensorSpec([4], tf.float32)\n        The values are formatted as (TOP, LEFT, BOTTOM, RIGHT)\n        \n        candidates: TensorSpec([N_CANDIDATES, 4], tf.float32)\n        The values are formatted as (CY, CX, H, W)\n        CY, CX: The coordinates of the box center.\n        BH, BW: Box height and width\n    Returns:\n        ious: TensorSpec([N_CANDIDATES], tf.float32)\n    \"\"\"\n#     tf.print('Candidates: ', candidates)\n    candidate_corners = to_corners(candidates)\n    \n    # Compute intersection area\n    tl = tf.maximum(box[None, :2], candidate_corners[:, :2]) # Top-Left Intersection\n    br = tf.minimum(box[None, 2:], candidate_corners[:, 2:]) # Bottom-Right Intersection\n    intersection = tf.math.maximum(0.0, br - tl)\n    intersection_area = intersection[:, 0]*intersection[:, 1]\n    \n    # Compute box, candidates and union areas\n    box_h, box_w = box[2]-box[0], box[3]-box[1]\n    box_area = box_h*box_w\n    candidate_areas = candidates[:, 2]*candidates[:, 3]\n    replicated_box_areas = tf.tile([box_area], [candidates.shape[0]])\n    union_area = tf.math.maximum(replicated_box_areas + candidate_areas - intersection_area, 1e-8)\n    \n    return tf.clip_by_value(intersection_area/union_area, 0.0, 1.0)\n\ndef pick_anchors(image, boxes):\n    \"\"\"\n    Args:\n        image: TensorSpec([BS, H, W, C], tf.uint8)\n        boxes: TensorSpec([BS, BOX_COUNT, 5], tf.float32)\n        The values in the box tensor are (TOP, LEFT, BOTTOM, RIGHT, CLASS_ID)\n    Returns:\n        image: TensorSpec([BS, H, W, C], tf.uint8)\n        boxes: TensorSpec([BS, BOX_COUNT, 8], tf.float32)\n        The values in the boxes tensor are (GY, GX, ANCHOR_ID, CY, CX, BH, BW, CLASS_ID).\n        GY, GX are the offsets of the grid which contains the center of the box.\n        CY, CX are the coordinates of the center of the box.\n        BH, BW are the height and the width of the box respectively.\n    \"\"\"\n    batch_size = tf.shape(image)[0]\n    \n    # Extract Box Dimensions\n    left, top, right, bottom = boxes[..., 0], boxes[..., 1], boxes[..., 2], boxes[..., 3]\n    cy, cx = (top+bottom)/2, (left+right)/2,\n    h, w = (bottom-top), (right-left)\n    gy, gx = tf.math.floor(cy*GRID_SIZE), tf.math.floor(cx*GRID_SIZE)\n    \n    def box_fn(box):\n        gcyx = (box[:2]+0.5)/GRID_SIZE # Coordinates of the grid center.\n        replicated_gcyx = tf.tile([gcyx], [len(ANCHORS), 1])\n        anchor_boxes = tf.concat([replicated_gcyx, ANCHORS], axis=-1)\n\n        scores = box_ious(box[2:], anchor_boxes)\n        anchor_id = tf.math.argmax(scores)\n        \n        return anchor_id\n    \n    def batch_fn(t):\n        return tf.map_fn(box_fn, t, fn_output_signature=tf.TensorSpec((), tf.int64))\n    \n    # Boxes with grid coordinates and assigned anchors\n    cid = boxes[..., 4]\n    corner_boxes = tf.stack([gy, gx, top, left, bottom, right], axis=-1)\n    anchors = tf.map_fn(batch_fn, corner_boxes, fn_output_signature=tf.RaggedTensorSpec([None], tf.int64))\n    anchors = tf.cast(anchors, dtype=tf.float32)\n    boxes = tf.stack([gy, gx, anchors, cy, cx, h, w, cid], axis=-1)\n    \n    return image, boxes\n\ndef dsitem_to_tuple(item):\n    image = item['image']\n    boxes = item['objects']['bbox']\n    labels = item['objects']['label']\n    \n    float_labels = tf.cast(labels[:, None], dtype=tf.float32)\n    box_attrs = tf.concat([boxes, float_labels], axis=-1)\n    \n    return image, box_attrs\n\nBATCH_SIZE = 28\ndense_to_ragged_fn = tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE)\nbox_ds = train_ds.map(dsitem_to_tuple).map(resize).shuffle(2000, reshuffle_each_iteration=True).apply(dense_to_ragged_fn).map(pick_anchors)\n# box_ds = train_ds.map(dsitem_to_tuple) #.map(rectangularize).batch(2).map(prep_boxes)\n\nitr = iter(box_ds)\nnext(itr)\nimages, boxes = next(itr)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:22:43.473819Z","iopub.execute_input":"2022-02-03T05:22:43.474338Z","iopub.status.idle":"2022-02-03T05:22:50.446031Z","shell.execute_reply.started":"2022-02-03T05:22:43.474302Z","shell.execute_reply":"2022-02-03T05:22:50.444976Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def rgb_to_hex(rgb):\n    return '#%02x%02x%02x' % rgb\n\ndef bbox_to_rect(bbox, shape, color):\n    \"\"\"Convert bounding box to matplotlib format.\"\"\"\n    h, w = shape\n    left, top, right, bottom = bbox[0]*w, bbox[1]*h, bbox[2]*w, bbox[3]*h\n    \n    rect = plt.Rectangle(\n        xy=(left, top), width=right-left, height=bottom-top,\n        fill=False, edgecolor=color, linewidth=2)\n#     print(rect)\n    return rect\n\nN_CLASSES = ds_info.features['labels'].num_classes\nLABELS = ds_info.features['labels'].names\nCLASS_COLORS = list(map(lambda v: rgb_to_hex(tuple(v.tolist())), np.random.choice(range(64, 255),size=[N_CLASSES, 3])))\n\nitr = iter(train_ds)\nitem = next(itr)\n\nfig = plt.imshow(item['image'])\n\ndef draw_boxes(image, fig):\n    box_shape = image.shape[:2]\n    objs = item['objects']\n    \n    for index, bbox in enumerate(objs['bbox']):\n        rect = bbox_to_rect(bbox, box_shape, CLASS_COLORS[objs['label'][index]])\n        fig.axes.add_patch(rect)\n        fig.axes.text(*rect.get_xy(), LABELS[objs['label'][index]], ha='left', va='top', bbox=dict(ec='none', fc=CLASS_COLORS[objs['label'][index]]))\n\ndraw_boxes(item['image'], fig)\n# item['image'].shape, item['objects']['bbox'], item\nprint(list(enumerate(ds_info.features['labels'].names)))\nitem['objects']['label'], item['image'].shape, item['objects']['bbox']","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:22:50.449507Z","iopub.execute_input":"2022-02-03T05:22:50.451237Z","iopub.status.idle":"2022-02-03T05:22:52.927606Z","shell.execute_reply.started":"2022-02-03T05:22:50.451182Z","shell.execute_reply":"2022-02-03T05:22:52.926614Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"def to_corners(boxes):\n    \"\"\"Changes boxes from (cy, cx, h, w) format to corner format.\n\n    Arguments:\n        boxes: [N_BOXES, 4] float32\n        [CY, CX, H, W]\n\n    Returns:\n        boxes: [N_BOXES, 4]\n        [TOP, LEFT, BOTTOM, RIGHT]\n    \"\"\"\n    return tf.concat(\n        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n        axis=-1,\n    )\n\ndef box_ious(box, candidates):\n    \"\"\"Computes the IOUs of the box with the candidates.\n    Adopted from: https://keras.io/examples/vision/retinanet/#preprocessing-data\n    \n    Args:\n        box: [4] float32\n        [TOP, LEFT, BOTTOM, RIGHT]\n        \n        candidates: [N_ANCHORS, 4] float32\n        [CY, CX, H, W]\n    Returns:\n        ious: [N_ANCHORS] float32\n    \"\"\"\n    candidate_corners = to_corners(candidates)\n    \n    # Compute intersection area\n    tl = tf.maximum(box[None, :2], candidate_corners[:, :2]) # Top-Left Intersection\n    br = tf.minimum(box[None, 2:], candidate_corners[:, 2:]) # Bottom-Right Intersection\n    intersection = tf.math.maximum(0.0, br - tl)\n    intersection_area = intersection[:, 0]*intersection[:, 1]\n    \n    # Compute box, candidates and union areas\n    box_h, box_w = box[2]-box[0], box[3]-box[1]\n    box_area = box_h*box_w\n    candidate_areas = candidates[:, 2]*candidates[:, 3]\n    replicated_box_areas = tf.tile([box_area], [candidates.shape[0]])\n    union_area = tf.math.maximum(replicated_box_areas + candidate_areas - intersection_area, 1e-8)\n    \n    return tf.clip_by_value(intersection_area/union_area, 0.0, 1.0)\n\nclass YoloLoss(losses.Loss):\n    def __init__(self, N_CLASSES, ANCHORS, GRID_SIZE, l_coord=5, l_noobj=.5, iou_threshold=0.5):\n        \"\"\"\n        Args:\n            N_CLASSES: The number of object classes\n            ANCHORS: A list of a pair of (height, width) of anchor boxes.\n            FT_SIZE: The size of base model output\n        \"\"\"\n        self.N_CLASSES = N_CLASSES\n        self.ANCHORS = ANCHORS\n        self.GRID_SIZE = GRID_SIZE\n        self.bce = losses.BinaryCrossentropy()\n        self.l_coord = 5\n        self.l_noobj = 0.5\n        self.iou_threshold = 0.5\n    \n    def reorder_indices_nd(self, indices_nd, shape, return_argsort=False):\n        shape = tf.cast(shape, dtype=indices_nd.dtype)\n        nd_offsets = tf.math.cumprod(shape, exclusive=True, reverse=True)\n        indices = tf.math.reduce_sum(nd_offsets*indices_nd, axis=-1)\n#         indices = tf.unique(indices)\n        argsort = tf.argsort(indices) if return_argsort else None\n        indices = tf.sort(indices)\n        indices_nd = (indices[..., None]//nd_offsets)%shape\n\n        return indices_nd, indices, argsort\n    \n    def true_boxes_to_grid(self, yt):\n        def batch_fn(boxes):\n            indices = tf.cast(boxes[..., :3], dtype=tf.int64)\n            indices, indices1d, argsort = self.reorder_indices_nd(indices, [GRID_SIZE, GRID_SIZE, N_ANCHORS], return_argsort=True)\n            boxes = tf.gather(boxes, argsort, axis=0)\n            \n            # Expand sparse class\n            sparse_class = tf.cast(boxes[..., 7], dtype=tf.int64)\n            oh_class = tf.one_hot(sparse_class, N_CLASSES)\n            \n            # Handle the case where multiple boxes are tracked by the same anchor\n            _, segments = tf.unique(indices1d)\n            indices = tf.math.segment_min(indices, segments)\n            boxes_grid = tf.math.segment_min(boxes[..., :3], segments)\n            boxes_yx = tf.math.segment_mean(boxes[..., 3:5], segments)\n            boxes_hw = tf.math.segment_max(boxes[..., 5:7], segments)\n            boxes_class = tf.math.segment_max(oh_class, segments)\n            \n            # Create a mask value to identify the true boxes in the grid\n            mask = tf.ones_like(boxes_grid[..., 0], dtype=yt.dtype)\n            \n            # Reassemble the boxes\n            boxes = tf.concat([boxes_grid, boxes_yx, boxes_hw, mask[..., None], boxes_class], axis=-1)\n\n            def to_sparse(values):\n                values = tf.reshape(values, [-1])\n                return tf.SparseTensor(indices, values, [GRID_SIZE, GRID_SIZE, N_ANCHORS])\n\n            map_fn_output_signature = tf.SparseTensorSpec([GRID_SIZE, GRID_SIZE, N_ANCHORS], dtype=tf.float32)\n            st = tf.map_fn(to_sparse, tf.transpose(boxes, [1, 0]), fn_output_signature=map_fn_output_signature)\n            grid_boxes = tf.transpose(tf.sparse.to_dense(st), [1, 2, 3, 0])\n            \n            return grid_boxes\n        \n        map_fn_output_signature = tf.TensorSpec([GRID_SIZE, GRID_SIZE, N_ANCHORS, yt.shape[-1]+N_CLASSES], tf.float32)\n        grid_boxes = tf.map_fn(batch_fn, yt, fn_output_signature=map_fn_output_signature)\n        \n        return grid_boxes\n    \n    def yolo_head(self, m_output):\n        yx, hw, confidence, classes = tf.split(m_output, [2, 2, 1, N_CLASSES], axis=-1)\n    \n        # Normalize output\n        yx = tf.nn.sigmoid(yx) # Box center relative (within) to the grid cell.\n        hw = tf.math.exp(hw) # Box sizes relative to the anchor box sizes.\n        confidence = tf.nn.sigmoid(confidence)\n        classes = tf.nn.sigmoid(classes)\n\n        return yx, hw, confidence, classes\n    \n    def box_ious(self, boxes, candidates):\n        \"\"\"Computes the IOUs of the boxes with the candidates.\n        Adopted from: https://keras.io/examples/vision/retinanet/#preprocessing-data\n\n        Args:\n            boxes: [N, 4] float32\n            [TOP, LEFT, BOTTOM, RIGHT]\n\n            candidates: [M, 4] float32\n            [CY, CX, H, W]\n        Returns:\n            ious: [N, M] float32\n        \"\"\"\n        boxes_corners = to_corners(boxes)\n        candidate_corners = to_corners(candidates)\n\n        # Compute intersection area\n        tl = tf.maximum(boxes_corners[..., None, :2], candidate_corners[..., :2]) # Top-Left Intersection\n        br = tf.minimum(boxes_corners[..., None, 2:], candidate_corners[..., 2:]) # Bottom-Right Intersection\n        intersections = tf.math.maximum(0.0, br - tl)\n        intersection_areas = intersections[..., 0]*intersections[..., 1]\n\n        # Compute box, candidates and union areas\n        box_areas = boxes[..., 2]*boxes[..., 2]\n        candidate_areas = candidates[:, 2]*candidates[:, 3]\n        union_areas = box_areas[..., None] + candidate_areas\n\n        return tf.clip_by_value(intersection_areas/union_areas, 0.0, 1.0)\n    \n    def low_confidence_prediction_mask(self, true_boxes, pred_boxes, mask):\n        squeezed_shape = mask.shape[:-1]\n        mask = tf.squeeze(mask)\n        mask.set_shape(squeezed_shape)\n        \n        true_boxes = tf.boolean_mask(true_boxes, mask) # Filter-out the unassigned anchors\n        \n        ious = self.box_ious(pred_boxes, true_boxes)\n        best_ious = tf.math.reduce_max(ious, axis=-1)\n        \n        return tf.cast(tf.expand_dims(best_ious < self.iou_threshold, axis=-1), dtype=true_boxes.dtype)\n\n    def call(self, yt, yp):\n        \"\"\"YOLO loss function\n        \n        Args:\n            yt : TensorSpec([BATCH, BOX_COUNT, 8], tf.float32)\n            The values in the boxes tensor are (GY, GX, ANCHOR_ID, CY, CX, BH, BW, CLASS_ID).\n            GY, GX are the offsets of the grid which contains the center of the box.\n            CY, CX are the coordinates of the center of the box.\n            BH, BW are the height and the width of the box respectively.\n            \n            yp: TensorSpec([BATCH, GRID_SIZE, GRID_SIZE, N_ANCHORS, N_CLASSES+5])\n            The values in the predicted bounding boxes tensor are\n            (CY, CX, BH, BW, CONFIDENCE, CLASS_0, CLASS_1,...CLASS_(N_CLASSES-1))\n            CY, CX are the box center coordinates within the grid cell.\n            BH, BW are the box dimensions wrt the anchor box.\n            confidence represents the certainity of box prediction.\n            class_0,..class_(n_classes-1) are the predictions for each class.\n        Returns:\n            loss: TensorSpec([BATCH], tf.float32)\n        \"\"\"\n        # Transform ragged boxes to grid\n        yt = self.true_boxes_to_grid(yt)\n        \n        # Extract true dimensions of the boxes\n        grid, anchors, true_yx, true_hw, mask, true_classes = tf.split(yt, [2, 1, 2, 2, 1, N_CLASSES], axis=-1)\n        boolean_mask = tf.cast(mask, dtype=tf.bool)\n        \n        # Transform true box values to enable comparison with the model output\n        rel_true_yx = true_yx*GRID_SIZE - grid # Grid cell relative coordinates\n        raw_true_hw = tf.math.log(true_hw/ANCHORS) # Box dimensions relative to the anchor\n        raw_true_hw = tf.where(boolean_mask, raw_true_hw, tf.zeros_like(raw_true_hw))\n        \n        # Extract predictions\n        logits_pred_hw = yp[..., 2:4] # Residual dimensions\n        rel_pred_yx, rel_pred_hw, confidence, pred_classes = self.yolo_head(yp)\n        pred_yx, pred_hw = (rel_pred_yx + grid)/GRID_SIZE, rel_pred_hw*ANCHORS\n        pred_boxes = tf.concat([pred_yx, pred_hw], axis=-1)\n        \n        # Compute an ignore_mask to ignore predictions with high overlap with the unassigned anchor boxes\n        low_confidence_mask = self.low_confidence_prediction_mask(yt[..., 3:7], pred_boxes, boolean_mask)\n        \n        # Compute Losses\n        yx_loss = self.l_coord*mask*self.bce(rel_true_yx, rel_pred_yx) # Loss of grid cell relative coordinates\n        hw_loss = mask*((logits_pred_hw - raw_true_hw)**2)\n        class_loss = mask*self.bce(true_classes, pred_classes) + self.l_noobj*(1-mask)*self.bce(true_classes, pred_classes)\n        confidence_loss = mask*self.bce(mask, confidence) + low_confidence_mask*(1-mask)*mask*self.bce(mask, confidence)\n        \n        # Total Losses\n        yx_loss = tf.reduce_sum(yx_loss)\n        hw_loss = tf.reduce_sum(hw_loss)\n        class_loss = tf.reduce_sum(class_loss)\n        confidence_loss = tf.reduce_sum(confidence_loss)\n        \n        # Compute total loss\n        loss = (yx_loss + hw_loss + class_loss + confidence_loss) / tf.cast(tf.shape(yt)[0], tf.float32)\n        \n        # Console logging\n#         tf.print(\n#             ' yx: ', yx_loss,\n#             'hw: ', hw_loss,\n#             'cl: ', class_loss,\n#             'conf: ', confidence_loss\n#         )\n        \n        return loss\n\n\n# loss = YoloLoss(N_CLASSES, ANCHORS, GRID_SIZE)\n# sample_image = images[0]\n# preds = model(sample_image[None, ...])\n# sample_boxes = tf.constant([[3, 3, 2, 1, 1, 3, 2, 5], [3, 3, 2, 0, 0, 2, 2, 5]], dtype=tf.float32)\n# preds = model(images)\n# loss_yx = loss.call(boxes, preds)\n# loss_yx = loss.call(sample_boxes[None, ...], preds)\n\n# loss_yx","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:24:48.165461Z","iopub.execute_input":"2022-02-03T05:24:48.165861Z","iopub.status.idle":"2022-02-03T05:24:48.204649Z","shell.execute_reply.started":"2022-02-03T05:24:48.165817Z","shell.execute_reply":"2022-02-03T05:24:48.203715Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Yolo Model","metadata":{}},{"cell_type":"code","source":"def get_conv_block(num_channels, shape=(3,3), padding='same', **kwargs):\n    return [\n        layers.Conv2D(num_channels, shape, padding=padding, **kwargs),\n        layers.BatchNormalization(),\n        layers.Activation('relu')\n    ]\n\ndef get_conv_dsc_block():\n    pass\n\ndef get_conv_builder(layer_type='standard'):\n    if (layer_type == 'standard'):\n        return get_conv_block\n    elif (layer_type == 'dsc'):\n        return get_conv_dsc_block\n    else:\n        raise Error('Invalid layer type: ', layer_type)\n\ndef create_model(conv_type='standard'):\n    conv_builder = get_conv_builder(conv_type)\n    image_input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    \n    features = tf.keras.Sequential([\n        image_input,\n        *conv_builder(32, strides=2),\n        *conv_builder(64, strides=1),\n        *conv_builder(128, strides=2),\n        *conv_builder(128, strides=1),\n        *conv_builder(256, strides=2),\n        *conv_builder(256, strides=1),\n        *conv_builder(512, strides=2),\n        *conv_builder(512, strides=1),\n        *conv_builder(512, strides=1),\n        *conv_builder(512, strides=1),\n        *conv_builder(512, strides=1),\n        *conv_builder(512, strides=1),\n        *conv_builder(1024, strides=2),\n        layers.Conv2D(N_ANCHORS*(5 + N_CLASSES), 1)\n#         layers.AveragePooling2D(pool_size=(7, 7)),\n#         layers.Dense(1000, activation='softmax')\n    ])\n    \n    # Raw Predictions\n    output = layers.Reshape(target_shape=[GRID_SIZE, GRID_SIZE, N_ANCHORS, 5 + N_CLASSES])(features.outputs[0])\n    \n#     # Non-maximal suppression\n#     output = non_max_supression(boxes)\n    \n    # Model Creation\n    model = tf.keras.Model(inputs=features.inputs, outputs=[output])\n    loss = YoloLoss(N_CLASSES, ANCHORS, GRID_SIZE)\n    \n    model.compile(optimizer='adam', loss=loss.call)\n    \n    return model\n\nmodel = create_model()\nmodel.summary()\n\n# plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:24:52.547888Z","iopub.execute_input":"2022-02-03T05:24:52.548714Z","iopub.status.idle":"2022-02-03T05:24:52.953110Z","shell.execute_reply.started":"2022-02-03T05:24:52.548669Z","shell.execute_reply":"2022-02-03T05:24:52.952236Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"history = model.fit(box_ds, epochs=20)\nmodel.save_weights('yolo_model/weights')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T05:24:56.535251Z","iopub.execute_input":"2022-02-03T05:24:56.535518Z","iopub.status.idle":"2022-02-03T05:32:54.564523Z","shell.execute_reply.started":"2022-02-03T05:24:56.535490Z","shell.execute_reply":"2022-02-03T05:32:54.563496Z"},"trusted":true},"execution_count":11,"outputs":[]}]}